{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liked-washington",
   "metadata": {},
   "source": [
    "# Hashtag Grouping on Twitter\n",
    "\n",
    "Input: \n",
    "    Hastag scrap result from cekmedsos_database.vw_ttidata\n",
    "\n",
    "Output: \n",
    "    Mapping tables to group each hashtag entry with similarity\n",
    "    \n",
    "\n",
    "What we need to do....\n",
    "1. Load the table entry from mySQL into python\n",
    "2. Read the entry\n",
    "\n",
    "Pre-requisite\n",
    "pip install wheel\n",
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-hayes",
   "metadata": {},
   "source": [
    "### 1. Create Connection to mySQL\n",
    "\n",
    "ref to this page:\n",
    "    [How to Use Python with mySQL in Jupyter](https://medium.com/@tattwei46/how-to-use-python-with-mysql-79304bee8753)\n",
    "    \n",
    "first, we need to install mysql connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "danish-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "\n",
    "# db parameters\n",
    "_conn_host='202.157.185.40'\n",
    "_conn_user='cekmedsos_db'\n",
    "_conn_password='282E~f0si'\n",
    "_conn_database='cekmedsos_database'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-palestinian",
   "metadata": {},
   "source": [
    "### Input as proces started\n",
    "Record header and parameter information\n",
    "\n",
    " * Kesepakatan status di kolom screen_analisis_ai.status\n",
    " * 1 --> baru diinput\n",
    " * 2 --> lagi dikerjakan\n",
    " * 3 --> proses berhasil\n",
    " * 4 --> proses gagal\n",
    " *\n",
    " \n",
    " * Kesepakatan jenis analisa AI\n",
    " * 1 --> Analisa Cluster\n",
    " * 2 --> Analisa image clustering\n",
    " * 3 --> Analisa sentiment\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fresh-video",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Zero jobs, quitting now\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aaec73581a98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdatabase_keyword_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hastag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0msimilarity_treshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parameter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mi_process_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
=======
      "35697\n",
      "0.88\n",
      "743\n"
>>>>>>> 41331fb052c236d9c7736d7cb6dacf7c29e543a5
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# get available jobs from database server, first come first serve\n",
    "sql = \"select id, hastag, `parameter` \\\n",
    "from screen_analisis_ai \\\n",
    "where active = 1 \\\n",
    "and status = 1 \\\n",
    "and jenis_analisa = 1 \\\n",
    "order by created asc, id asc limit 1\"\n",
    "\n",
    "row_count = cursor.execute(sql)\n",
    "\n",
    "if(row_count == 0):\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "database_keyword_id = result[0]['hastag']\n",
    "similarity_treshold = result[0]['parameter']\n",
    "i_process_id = result[0]['id']\n",
    "screen_name = ''\n",
    "\n",
    "print(database_keyword_id)\n",
    "print(similarity_treshold)\n",
    "print(i_process_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-microwave",
   "metadata": {},
   "source": [
    "### Marking this process as running\n",
    "so that another process will not take this one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chief-resource",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "739\n"
=======
      "743\n"
>>>>>>> 41331fb052c236d9c7736d7cb6dacf7c29e543a5
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Prepare SQL Statement\n",
    "print(i_process_id)\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = %s\"\n",
    "\n",
    "# execute\n",
    "cursor.execute(sql, i_process_id)\n",
    "\n",
    "# commit record\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-louisiana",
   "metadata": {},
   "source": [
    "### Runnning Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spectacular-manner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "739\n"
=======
      "743\n"
>>>>>>> 41331fb052c236d9c7736d7cb6dacf7c29e543a5
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Header Record\n",
    "sql = \"insert into ret_analysis_header (job_id, datetime_start, user_id) values (%s, %s, %s)\"\n",
    "# Execute the query\n",
    "print(i_process_id)\n",
    "cursor.execute(sql, (str(i_process_id), datetime.now(), 1 ))\n",
    "\n",
    "#\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "cursor.execute(sql, (i_process_id, 1, 'Similarity Treshold', similarity_treshold))\n",
    "cursor.execute(sql, (i_process_id, 1, 'DB_ID', database_keyword_id))\n",
    "\n",
    "# Commit Record\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-lighting",
   "metadata": {},
   "source": [
    "#### Starting process, \n",
    "Run query against RDBMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weekly-jonathan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "select id, tweet, tweet_date_time from ret_tweet where db_id = \"28330\"\n"
=======
      "select id, tweet, tweet_date_time from ret_tweet where db_id = \"35697\"\n"
>>>>>>> 41331fb052c236d9c7736d7cb6dacf7c29e543a5
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
<<<<<<< HEAD
       "      <td>1495911234730074112</td>\n",
       "      <td>la jadi ktp ckny bpjs????</td>\n",
       "      <td>2022-02-22 07:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1495911276098486272</td>\n",
       "      <td>@FFOODFESS Sekalian make bpjs</td>\n",
       "      <td>2022-02-22 07:00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1495911463399346176</td>\n",
       "      <td>@HarisRuslyMoti @jokowi Harusnya pajak orang/p...</td>\n",
       "      <td>2022-02-22 07:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1495911510517948417</td>\n",
       "      <td>@MadeSupada @_ekokuntadhi Kalau ada RS yg asal...</td>\n",
       "      <td>2022-02-22 07:01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1495911663576707072</td>\n",
       "      <td>@papa_loren BPJS gimana?</td>\n",
       "      <td>2022-02-22 07:01:47</td>\n",
=======
       "      <td>1535219740720914433</td>\n",
       "      <td>I see what are trying to do with Liz Cheney &amp;a...</td>\n",
       "      <td>2022-06-10 18:18:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1535219744156004352</td>\n",
       "      <td>@youremyjoys babi</td>\n",
       "      <td>2022-06-10 18:18:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1535219760316551169</td>\n",
       "      <td>@alwyfrozy @detikcom Tetap saja itu mencoreng ...</td>\n",
       "      <td>2022-06-10 18:18:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1535219766385852416</td>\n",
       "      <td>tô sendo má né ????</td>\n",
       "      <td>2022-06-10 18:18:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1535219772258123776</td>\n",
       "      <td>Aneh banget.... yang penting kan si penjual ud...</td>\n",
       "      <td>2022-06-10 18:18:11</td>\n",
>>>>>>> 41331fb052c236d9c7736d7cb6dacf7c29e543a5
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
<<<<<<< HEAD
       "0  1495911234730074112                          la jadi ktp ckny bpjs????   \n",
       "1  1495911276098486272                      @FFOODFESS Sekalian make bpjs   \n",
       "2  1495911463399346176  @HarisRuslyMoti @jokowi Harusnya pajak orang/p...   \n",
       "3  1495911510517948417  @MadeSupada @_ekokuntadhi Kalau ada RS yg asal...   \n",
       "4  1495911663576707072                           @papa_loren BPJS gimana?   \n",
       "\n",
       "      tweet_date_time  \n",
       "0 2022-02-22 07:00:05  \n",
       "1 2022-02-22 07:00:15  \n",
       "2 2022-02-22 07:01:00  \n",
       "3 2022-02-22 07:01:11  \n",
       "4 2022-02-22 07:01:47  "
=======
       "0  1535219740720914433  I see what are trying to do with Liz Cheney &a...   \n",
       "1  1535219744156004352                                  @youremyjoys babi   \n",
       "2  1535219760316551169  @alwyfrozy @detikcom Tetap saja itu mencoreng ...   \n",
       "3  1535219766385852416                                tô sendo má né ????   \n",
       "4  1535219772258123776  Aneh banget.... yang penting kan si penjual ud...   \n",
       "\n",
       "      tweet_date_time  \n",
       "0 2022-06-10 18:18:03  \n",
       "1 2022-06-10 18:18:04  \n",
       "2 2022-06-10 18:18:08  \n",
       "3 2022-06-10 18:18:09  \n",
       "4 2022-06-10 18:18:11  "
>>>>>>> 41331fb052c236d9c7736d7cb6dacf7c29e543a5
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Query to get tweet data, apply analitics to this dataset\n",
    "#\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "s_query_string = 'select id, tweet, tweet_date_time from ret_tweet where '\n",
    "# s_query_string = 'select id, tweet, tweet_date_time from vw_ret_tweet_clean where '\n",
    "\n",
    "if (screen_name != ''):\n",
    "    # print('use screen name')\n",
    "    s_query_string = s_query_string + 'screen_name = \"' + screen_name + '\" and db_id = \"' + str(database_keyword_id) + '\"'\n",
    "else:\n",
    "    # print('no use')\n",
    "    s_query_string = s_query_string + 'db_id = \"' + database_keyword_id.replace('\"','') + '\"'\n",
    "    \n",
    "print(s_query_string)\n",
    "df = pd.read_sql(s_query_string, con=connection)\n",
    "\n",
    "# Close Connection\n",
    "connection.close()\n",
    "\n",
    "# see result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-waste",
   "metadata": {},
   "source": [
    "### 2. Try to pre-process all the text\n",
    "\n",
    "target-> tokenizing into another dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chemical-bulgarian",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a4d2304f429f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Apply to data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_preproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
=======
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1535219740720914433</td>\n",
       "      <td>i see what are trying to do with liz cheney am...</td>\n",
       "      <td>2022-06-10 18:18:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1535219744156004352</td>\n",
       "      <td>youremyjoys babi</td>\n",
       "      <td>2022-06-10 18:18:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1535219760316551169</td>\n",
       "      <td>alwyfrozy detikcom tetap saja itu mencoreng na...</td>\n",
       "      <td>2022-06-10 18:18:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1535219766385852416</td>\n",
       "      <td>tô sendo má né</td>\n",
       "      <td>2022-06-10 18:18:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1535219772258123776</td>\n",
       "      <td>aneh banget yang penting kan si penjual udah t...</td>\n",
       "      <td>2022-06-10 18:18:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1535219740720914433  i see what are trying to do with liz cheney am...   \n",
       "1  1535219744156004352                                   youremyjoys babi   \n",
       "2  1535219760316551169  alwyfrozy detikcom tetap saja itu mencoreng na...   \n",
       "3  1535219766385852416                                     tô sendo má né   \n",
       "4  1535219772258123776  aneh banget yang penting kan si penjual udah t...   \n",
       "\n",
       "      tweet_date_time  \n",
       "0 2022-06-10 18:18:03  \n",
       "1 2022-06-10 18:18:04  \n",
       "2 2022-06-10 18:18:08  \n",
       "3 2022-06-10 18:18:09  \n",
       "4 2022-06-10 18:18:11  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 41331fb052c236d9c7736d7cb6dacf7c29e543a5
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    \n",
    "    return strOut\n",
    "# end Text Preprocessing\n",
    "\n",
    "\n",
    "# Apply to data frame\n",
    "df['tweet'] = df['tweet'].apply(text_preproc)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-power",
   "metadata": {},
   "source": [
    "### Finish preprocessing, Tokenized\n",
    "Next step, is to output to new column for tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "classical-position",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
<<<<<<< HEAD
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2bce49766fa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# apply tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# apply stopword removal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
=======
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1535219740720914433</td>\n",
       "      <td>i see what are trying to do with liz cheney am...</td>\n",
       "      <td>2022-06-10 18:18:03</td>\n",
       "      <td>[i, see, what, are, trying, to, do, with, liz,...</td>\n",
       "      <td>{on, what, still, let, amp, respectfully, see,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1535219744156004352</td>\n",
       "      <td>youremyjoys babi</td>\n",
       "      <td>2022-06-10 18:18:04</td>\n",
       "      <td>[youremyjoys, babi]</td>\n",
       "      <td>{youremyjoys, babi}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1535219760316551169</td>\n",
       "      <td>alwyfrozy detikcom tetap saja itu mencoreng na...</td>\n",
       "      <td>2022-06-10 18:18:08</td>\n",
       "      <td>[alwyfrozy, detikcom, tetap, saja, itu, mencor...</td>\n",
       "      <td>{kecuali, berasal, berbau, nya, mencoreng, nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1535219766385852416</td>\n",
       "      <td>tô sendo má né</td>\n",
       "      <td>2022-06-10 18:18:09</td>\n",
       "      <td>[tô, sendo, má, né]</td>\n",
       "      <td>{tô, né, sendo, má}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1535219772258123776</td>\n",
       "      <td>aneh banget yang penting kan si penjual udah t...</td>\n",
       "      <td>2022-06-10 18:18:11</td>\n",
       "      <td>[aneh, banget, yang, penting, kan, si, penjual...</td>\n",
       "      <td>{banget, bilang, terangan, penjual, ga, halal,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1535219740720914433  i see what are trying to do with liz cheney am...   \n",
       "1  1535219744156004352                                   youremyjoys babi   \n",
       "2  1535219760316551169  alwyfrozy detikcom tetap saja itu mencoreng na...   \n",
       "3  1535219766385852416                                     tô sendo má né   \n",
       "4  1535219772258123776  aneh banget yang penting kan si penjual udah t...   \n",
       "\n",
       "      tweet_date_time                                    tokenized_tweet  \\\n",
       "0 2022-06-10 18:18:03  [i, see, what, are, trying, to, do, with, liz,...   \n",
       "1 2022-06-10 18:18:04                                [youremyjoys, babi]   \n",
       "2 2022-06-10 18:18:08  [alwyfrozy, detikcom, tetap, saja, itu, mencor...   \n",
       "3 2022-06-10 18:18:09                                [tô, sendo, má, né]   \n",
       "4 2022-06-10 18:18:11  [aneh, banget, yang, penting, kan, si, penjual...   \n",
       "\n",
       "                                                  sw  \n",
       "0  {on, what, still, let, amp, respectfully, see,...  \n",
       "1                                {youremyjoys, babi}  \n",
       "2  {kecuali, berasal, berbau, nya, mencoreng, nam...  \n",
       "3                                {tô, né, sendo, má}  \n",
       "4  {banget, bilang, terangan, penjual, ga, halal,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 41331fb052c236d9c7736d7cb6dacf7c29e543a5
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Load stopwords\n",
    "sw = stopwords.words('indonesian')\n",
    "\n",
    "# apply tokenize\n",
    "df['tokenized_tweet'] = df.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
    "\n",
    "# apply stopword removal\n",
    "df['sw'] = df.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-commerce",
   "metadata": {},
   "source": [
    "### Define function to calculate similarity\n",
    "Function return similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "close-sessions",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(X_set, Y_set):\n",
    "# Program to measure the similarity between  \n",
    "# two sentences using cosine similarity. \n",
    "\n",
    "    l1 =[];l2 =[]\n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector\n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i]\n",
    "    try:\n",
<<<<<<< HEAD
    "        cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "    except:\n",
    "        # print('zero div on X_set')\n",
    "        return 0\n",
    "    \n",
=======
    "        cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    except:\n",
    "        # print('zero div on X_set')\n",
    "        return 0;\n",
    "        \n",
>>>>>>> 41331fb052c236d9c7736d7cb6dacf7c29e543a5
    "    return cosine\n",
    "\n",
    "def largest_in_col(arr,nCol):\n",
    "    # \n",
    "    # Find largest value of col nCol on 2D arr\n",
    "    #\n",
    "    \n",
    "    # init value\n",
    "    max_val = arr[0][nCol]\n",
    "    # also, remember index\n",
    "    row_index = 0\n",
    "    \n",
    "    for x in range(0, len(arr)):\n",
    "        if arr[x][nCol] > max_val:\n",
    "            max_val = arr[x][nCol]\n",
    "            row_index = x\n",
    "        \n",
    "    return max_val,row_index\n",
    "\n",
    "def smallest_in_col(arr,nCol):\n",
    "    # \n",
    "    # Find smallest value of col nCol on 2D arr\n",
    "    #\n",
    "    \n",
    "    # init value\n",
    "    min_val = arr[0][nCol]\n",
    "    # also, remember index\n",
    "    row_index = 0\n",
    "    \n",
    "    for x in range(0, len(arr)):\n",
    "        if arr[x][nCol] < min_val:\n",
    "            min_val = arr[x][nCol]\n",
    "            row_index = x\n",
    "        \n",
    "    return min_val,row_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-appearance",
   "metadata": {},
   "source": [
    "### Try to using function\n",
    "using some of array cells, create n matrices\n",
    "\n",
    "1. take one tweet, compare to all data set\n",
    "2. flag 1 if similar\n",
    "3. take next tweet, if similar from prev tweet, skip\n",
    "4. if not similar, add counter, then proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unavailable-clearing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "  1%|          | 101404/9743905.0 [00:05<08:57, 17956.15it/s]\n"
=======
      "  2%|▏         | 1758986/87496606.0 [00:41<33:38, 42476.21it/s] \n"
>>>>>>> 41331fb052c236d9c7736d7cb6dacf7c29e543a5
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[0;32m<ipython-input-14-389c861569a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0ms_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_to_compare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# print(tweet_to_compare)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;31m# print(df['sw'][i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-0231a92412f9>\u001b[0m in \u001b[0;36mcalculate_similarity\u001b[0;34m(X_set, Y_set)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# cosine formula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcosine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
=======
      "\u001b[0;32m<ipython-input-9-4af4d878374c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0ms_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_to_compare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms_score\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mcosine_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi_current_cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c8393fff28b1>\u001b[0m in \u001b[0;36mcalculate_similarity\u001b[0;34m(X_set, Y_set)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# cosine formula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcosine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
>>>>>>> 41331fb052c236d9c7736d7cb6dacf7c29e543a5
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import array as arr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set to max CPU Cores\n",
    "# multitasking.set_max_threads(multitasking.config[\"CPU_CORES\"] * 5)\n",
    "\n",
    "st = similarity_treshold\n",
    "cluster_no = 1\n",
    "s_score = 0\n",
    "s_score_current = 0\n",
    "i_current_cluster = 0\n",
    "\n",
    "#create zero element array\n",
    "#col 0 => index base tweet\n",
    "#col 1 => cluster number\n",
    "#col 2 => similarity score\n",
    "base_tweet = []\n",
    "\n",
    "#proceed to compare to all tweet\n",
    "cosine_matrix = np.zeros(( len(df), len(df) ), dtype=np.dtype('f4'))\n",
    "\n",
    "# flag the mt\n",
    "with tqdm(total=( (len(df)*len(df)/2))-(len(df)/2))  as pbar:\n",
    "    for j in range(0, len(df)):\n",
    "        tweet_to_compare = df['sw'][j]\n",
    "\n",
    "        #check, is this second tweet?\n",
    "        if(j == 0):\n",
    "            #first tweet, add as cluster no #1\n",
    "            base_tweet.append([j,1,1.0])\n",
    "            i_current_cluster = base_tweet[0][1]\n",
    "\n",
    "        elif(j == 1):\n",
    "            #compare to prev tweet\n",
    "            s_score = calculate_similarity(tweet_to_compare, df['sw'][ base_tweet[0][0] ])\n",
    "\n",
    "            if(s_score < st):\n",
    "                #not similar\n",
    "                base_tweet.append([j,2,1])\n",
    "                i_current_cluster = base_tweet[j][1]\n",
    "\n",
    "        else:\n",
    "            #other else tweet\n",
    "            for x in range(0,len(base_tweet)):\n",
    "                #compare every element\n",
    "                s_score = calculate_similarity(tweet_to_compare, df['sw'][ base_tweet[x][0] ])\n",
    "                base_tweet[x][2] = s_score\n",
    "\n",
    "            if(largest_in_col(base_tweet,2)[0] < st):\n",
    "                #no similar, add as one new cluster\n",
    "                i_current_cluster = i_current_cluster + 1\n",
    "                base_tweet.append([j,i_current_cluster,largest_in_col(base_tweet,2)[0]])\n",
    "            else:\n",
    "                #determine cluster# from biggest similarity\n",
    "                i_current_cluster = base_tweet[(largest_in_col(base_tweet,2)[1])][1]\n",
    "\n",
    "        #proceed to compare to all tweet\n",
    "        for i in range(0,len(df)):\n",
    "            # update progress\n",
    "            if (j<i):\n",
    "                pbar.update(1)\n",
    "                s_score = calculate_similarity(tweet_to_compare, df['sw'][i])\n",
    "                # print(tweet_to_compare)\n",
    "                # print(df['sw'][i])\n",
    "                if (s_score >= st):\n",
    "                    cosine_matrix[i,j] = i_current_cluster\n",
    "\n",
    "        \n",
    "pbar.close()\n",
    "print(cosine_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-transparency",
   "metadata": {},
   "source": [
    "### Writing result to file\n",
    "Using NPZ format for efficiency, and try to load them after save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-mobile",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save numpy array as npz file\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "\n",
    "# save to npy file\n",
    "# savez_compressed('./output/df_380.npz',df)\n",
    "# savez_compressed('./output/data_380.npz', cosine_matrix)\n",
    "# savez_compressed('./output/data_380_base.npz', base_tweet)\n",
    "\n",
    "print(len(cosine_matrix))\n",
    "print('Save data complete .... ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-tennessee",
   "metadata": {},
   "source": [
    "### How to save to rdbms?\n",
    "save the header data, meta data of the process\n",
    "[ret_analysis_header]\n",
    "- JobID\n",
    "- user initiated\n",
    "- Time Started\n",
    "- Time End\n",
    "\n",
    "Process Parameter\n",
    "[ret_analysis_parameter]\n",
    "- JobID\n",
    "- Param Name\n",
    "- Param Value\n",
    "    - ST Value\n",
    "    - DB ID\n",
    "    - Screen Name\n",
    "    - How Many Tweet analyzed\n",
    "\n",
    "save detail cluster information\n",
    "[ret_base_tweet]\n",
    "- JobID\n",
    "- Tweet Base# --> tweet ID\n",
    "- Cluster#\n",
    "\n",
    "save detail calculation result, structure\n",
    "[ret_cluster_result]\n",
    "- JobID\n",
    "- TweetID\n",
    "- Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Base Tweet Record\n",
    "sql = \"insert into ret_base_tweet (job_id, tweet_id, cluster_id) values (%s, %s, %s)\"\n",
    "\n",
    "## inserting base taweet\n",
    "for i in range(0,len(base_tweet)):\n",
    "    # Execute the query\n",
    "    cursor.execute(sql, (i_process_id, df['id'][i], base_tweet[i][1]))\n",
    "    \n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('finished inserting base tweet record')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-kennedy",
   "metadata": {},
   "source": [
    "### Record cluster result\n",
    "into table ret_cluster_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "\n",
    "savetxt('data.csv', cosine_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-nylon",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"insert into ret_cluster_result (job_id, tweet_id, cluster_no) values (%s, %s, %s)\"\n",
    "\n",
    "print(len(cosine_matrix))\n",
    "print(i_process_id)\n",
    "\n",
    "# initiate cluster number\n",
    "i_cluster_no_save = 0\n",
    "temp_val = 0\n",
    "                 \n",
    "## inserting tweet cluster\n",
    "for i in range(0, len(cosine_matrix)):\n",
    "    \n",
    "    # find value in this particular row\n",
    "    for j in range(0,len(cosine_matrix[i])):\n",
    "        \n",
    "        # print(cosine_matrix[i][j])\n",
    "        temp_val = cosine_matrix[j][i]\n",
    "        \n",
    "        if(temp_val != 0):\n",
    "            i_cluster_no_save = temp_val\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(sql, (i_process_id, df['id'][i], i_cluster_no_save))\n",
    "    \n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('finished inserting cluster data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-foundation",
   "metadata": {},
   "source": [
    "### Record finish time\n",
    "update table ret_analysis_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-florence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "cursor.execute(sql, (i_process_id, 1, '#Tweet Processed',len(df)))\n",
    "\n",
    "#\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"update ret_analysis_header set datetime_finish = %s where job_id = %s\"\n",
    "# Executing query\n",
    "cursor.execute(sql, (datetime.now(),i_process_id) )\n",
    "\n",
    "print(i_process_id)\n",
    "\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('job finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-brown",
   "metadata": {},
   "source": [
    "### Marking as finished the job\n",
    "on this particular i_process_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# inserting jobs into table mv result\n",
    "sql = \"call spInsertResultToMV(%s);\" \n",
    "# Executing query\n",
    "cursor.execute(sql,i_process_id)\n",
    "\n",
    "sql = \"update screen_analisis_ai set status = 3, end_process = now(), duration = TIMESTAMPDIFF(second,start_process, end_process) where id = %s\"\n",
    "# Executing query\n",
    "cursor.execute(sql,i_process_id)\n",
    "\n",
    "# commit changes\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('inserting result finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-tooth",
   "metadata": {},
   "source": [
    "### Done Here ...\n",
    "1. Create data structure to record below things\n",
    "    Job Header\n",
    "    - JobID\n",
    "    - Similarity Treshold\n",
    "    - Dataset Parameter\n",
    "        - By User\n",
    "        - By Keyword\n",
    "    - Running time start\n",
    "    - Running time end\n",
    "    \n",
    "    Dataset Result\n",
    "        - base_tweet\n",
    "        - cosine_matrix\n",
    "        \n",
    "2. Push the result data into RDBMS Server, in this case is mySQL\n",
    "    - upload csv file to mySQL\n",
    "    - import to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait 10 sec before release\n",
    "time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
