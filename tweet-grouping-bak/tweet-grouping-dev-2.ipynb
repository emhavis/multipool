{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liked-washington",
   "metadata": {},
   "source": [
    "# Hashtag Grouping on Twitter\n",
    "\n",
    "Input: \n",
    "    Hastag scrap result from cekmedsos_database.vw_ttidata\n",
    "\n",
    "Output: \n",
    "    Mapping tables to group each hashtag entry with similarity\n",
    "    \n",
    "\n",
    "What we need to do....\n",
    "1. Load the table entry from mySQL into python\n",
    "2. Read the entry\n",
    "\n",
    "Pre-requisite\n",
    "pip install wheel\n",
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-hayes",
   "metadata": {},
   "source": [
    "### 1. Create Connection to mySQL\n",
    "\n",
    "ref to this page:\n",
    "    [How to Use Python with mySQL in Jupyter](https://medium.com/@tattwei46/how-to-use-python-with-mysql-79304bee8753)\n",
    "    \n",
    "first, we need to install mysql connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "musical-ticket",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-palestinian",
   "metadata": {},
   "source": [
    "### Input as proces started\n",
    "Record header and parameter information\n",
    "\n",
    " * Kesepakatan status di kolom screen_analisis_ai.status\n",
    " * 1 --> baru diinput\n",
    " * 2 --> lagi dikerjakan\n",
    " * 3 --> proses berhasil\n",
    " * 4 --> proses gagal\n",
    " *\n",
    " \n",
    " * Kesepakatan jenis analisa AI\n",
    " * 1 --> Analisa Cluster\n",
    " * 2 --> Analisa sentiment\n",
    " * 3 --> Analisa image clustering\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-microwave",
   "metadata": {},
   "source": [
    "### Marking this process as running\n",
    "so that another process will not take this one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-louisiana",
   "metadata": {},
   "source": [
    "### Runnning Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-lighting",
   "metadata": {},
   "source": [
    "#### Starting process, \n",
    "Run query against RDBMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weekly-jonathan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select id, tweet, tweet_date_time from ret_tweet where db_id = \"3507\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1414011830247194625</td>\n",
       "      <td>Apa yang sedikit tetapi mencukupi lebih baik d...</td>\n",
       "      <td>2021-07-11 07:01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1414012129028411395</td>\n",
       "      <td>Allah, tidak ada Tuhan melainkan Dia, Yang Hid...</td>\n",
       "      <td>2021-07-11 07:02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1414012255167913989</td>\n",
       "      <td>Jika engkau berniat ingin bersedekah, atau mel...</td>\n",
       "      <td>2021-07-11 07:02:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1414012548635004928</td>\n",
       "      <td>Dan sebutlah (nama) Tuhanmu sebanyak-banyaknya...</td>\n",
       "      <td>2021-07-11 07:03:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1414012577642811393</td>\n",
       "      <td>Puasa itu tak menyakiti namun menyehatkan... m...</td>\n",
       "      <td>2021-07-11 07:04:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1414011830247194625  Apa yang sedikit tetapi mencukupi lebih baik d...   \n",
       "1  1414012129028411395  Allah, tidak ada Tuhan melainkan Dia, Yang Hid...   \n",
       "2  1414012255167913989  Jika engkau berniat ingin bersedekah, atau mel...   \n",
       "3  1414012548635004928  Dan sebutlah (nama) Tuhanmu sebanyak-banyaknya...   \n",
       "4  1414012577642811393  Puasa itu tak menyakiti namun menyehatkan... m...   \n",
       "\n",
       "      tweet_date_time  \n",
       "0 2021-07-11 07:01:06  \n",
       "1 2021-07-11 07:02:17  \n",
       "2 2021-07-11 07:02:47  \n",
       "3 2021-07-11 07:03:57  \n",
       "4 2021-07-11 07:04:04  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Query to get tweet data, apply analitics to this dataset\n",
    "#\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host='202.157.176.225',\n",
    "                             user='cekmedsos_db',\n",
    "                             password='kuku838485*#',\n",
    "                             database='cekmedsos_database',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "s_query_string = 'select id, tweet, tweet_date_time from ret_tweet where '\n",
    "# s_query_string = 'select id, tweet, tweet_date_time from vw_ret_tweet_clean where '\n",
    "\n",
    "if (screen_name != ''):\n",
    "    # print('use screen name')\n",
    "    s_query_string = s_query_string + 'screen_name = \"' + screen_name + '\" and db_id = \"' + str(database_keyword_id) + '\"'\n",
    "else:\n",
    "    # print('no use')\n",
    "    s_query_string = s_query_string + 'db_id = \"' + database_keyword_id.replace('\"','') + '\"'\n",
    "    \n",
    "print(s_query_string)\n",
    "df = pd.read_sql(s_query_string, con=connection)\n",
    "\n",
    "# Close Connection\n",
    "connection.close()\n",
    "\n",
    "# see result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-waste",
   "metadata": {},
   "source": [
    "### 2. Try to pre-process all the text\n",
    "\n",
    "target-> tokenizing into another dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chemical-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1414011830247194625</td>\n",
       "      <td>apa yang sedikit tetapi mencukupi lebih baik d...</td>\n",
       "      <td>2021-07-11 07:01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1414012129028411395</td>\n",
       "      <td>allah tidak ada tuhan melainkan dia yang hidup...</td>\n",
       "      <td>2021-07-11 07:02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1414012255167913989</td>\n",
       "      <td>jika engkau berniat ingin bersedekah atau mela...</td>\n",
       "      <td>2021-07-11 07:02:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1414012548635004928</td>\n",
       "      <td>dan sebutlah nama tuhanmu sebanyakbanyaknya se...</td>\n",
       "      <td>2021-07-11 07:03:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1414012577642811393</td>\n",
       "      <td>puasa itu tak menyakiti namun menyehatkan mena...</td>\n",
       "      <td>2021-07-11 07:04:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1414011830247194625  apa yang sedikit tetapi mencukupi lebih baik d...   \n",
       "1  1414012129028411395  allah tidak ada tuhan melainkan dia yang hidup...   \n",
       "2  1414012255167913989  jika engkau berniat ingin bersedekah atau mela...   \n",
       "3  1414012548635004928  dan sebutlah nama tuhanmu sebanyakbanyaknya se...   \n",
       "4  1414012577642811393  puasa itu tak menyakiti namun menyehatkan mena...   \n",
       "\n",
       "      tweet_date_time  \n",
       "0 2021-07-11 07:01:06  \n",
       "1 2021-07-11 07:02:17  \n",
       "2 2021-07-11 07:02:47  \n",
       "3 2021-07-11 07:03:57  \n",
       "4 2021-07-11 07:04:04  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    \n",
    "    return strOut\n",
    "# end Text Preprocessing\n",
    "\n",
    "\n",
    "# Apply to data frame\n",
    "df['tweet'] = df['tweet'].apply(text_preproc)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-power",
   "metadata": {},
   "source": [
    "### Finish preprocessing, Tokenized\n",
    "Next step, is to output to new column for tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "classical-position",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1414011830247194625</td>\n",
       "      <td>apa yang sedikit tetapi mencukupi lebih baik d...</td>\n",
       "      <td>2021-07-11 07:01:06</td>\n",
       "      <td>[apa, yang, sedikit, tetapi, mencukupi, lebih,...</td>\n",
       "      <td>{mencukupi, dawud, eyde, hrabu, ✅, sambutkejay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1414012129028411395</td>\n",
       "      <td>allah tidak ada tuhan melainkan dia yang hidup...</td>\n",
       "      <td>2021-07-11 07:02:17</td>\n",
       "      <td>[allah, tidak, ada, tuhan, melainkan, dia, yan...</td>\n",
       "      <td>{allah, mengantuk, tidur, ⏱️, terusmenerus, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1414012255167913989</td>\n",
       "      <td>jika engkau berniat ingin bersedekah atau mela...</td>\n",
       "      <td>2021-07-11 07:02:47</td>\n",
       "      <td>[jika, engkau, berniat, ingin, bersedekah, ata...</td>\n",
       "      <td>{atstsaury, menghalangi, amal, laksanakan, suf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1414012548635004928</td>\n",
       "      <td>dan sebutlah nama tuhanmu sebanyakbanyaknya se...</td>\n",
       "      <td>2021-07-11 07:03:57</td>\n",
       "      <td>[dan, sebutlah, nama, tuhanmu, sebanyakbanyakn...</td>\n",
       "      <td>{petang, ☝️☝️, pagi, tuhanmu, ikz, bertasbihla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1414012577642811393</td>\n",
       "      <td>puasa itu tak menyakiti namun menyehatkan mena...</td>\n",
       "      <td>2021-07-11 07:04:04</td>\n",
       "      <td>[puasa, itu, tak, menyakiti, namun, menyehatka...</td>\n",
       "      <td>{☝️☝️, menyakiti, selamat, lapar, puasa, menah...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1414011830247194625  apa yang sedikit tetapi mencukupi lebih baik d...   \n",
       "1  1414012129028411395  allah tidak ada tuhan melainkan dia yang hidup...   \n",
       "2  1414012255167913989  jika engkau berniat ingin bersedekah atau mela...   \n",
       "3  1414012548635004928  dan sebutlah nama tuhanmu sebanyakbanyaknya se...   \n",
       "4  1414012577642811393  puasa itu tak menyakiti namun menyehatkan mena...   \n",
       "\n",
       "      tweet_date_time                                    tokenized_tweet  \\\n",
       "0 2021-07-11 07:01:06  [apa, yang, sedikit, tetapi, mencukupi, lebih,...   \n",
       "1 2021-07-11 07:02:17  [allah, tidak, ada, tuhan, melainkan, dia, yan...   \n",
       "2 2021-07-11 07:02:47  [jika, engkau, berniat, ingin, bersedekah, ata...   \n",
       "3 2021-07-11 07:03:57  [dan, sebutlah, nama, tuhanmu, sebanyakbanyakn...   \n",
       "4 2021-07-11 07:04:04  [puasa, itu, tak, menyakiti, namun, menyehatka...   \n",
       "\n",
       "                                                  sw  \n",
       "0  {mencukupi, dawud, eyde, hrabu, ✅, sambutkejay...  \n",
       "1  {allah, mengantuk, tidur, ⏱️, terusmenerus, sa...  \n",
       "2  {atstsaury, menghalangi, amal, laksanakan, suf...  \n",
       "3  {petang, ☝️☝️, pagi, tuhanmu, ikz, bertasbihla...  \n",
       "4  {☝️☝️, menyakiti, selamat, lapar, puasa, menah...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Load stopwords\n",
    "sw = stopwords.words('indonesian')\n",
    "\n",
    "# apply tokenize\n",
    "df['tokenized_tweet'] = df.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
    "\n",
    "# apply stopword removal\n",
    "df['sw'] = df.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-commerce",
   "metadata": {},
   "source": [
    "### Define function to calculate similarity\n",
    "Function return similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "close-sessions",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(X_set, Y_set):\n",
    "# Program to measure the similarity between  \n",
    "# two sentences using cosine similarity. \n",
    "\n",
    "    l1 =[];l2 =[]\n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector\n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    \n",
    "    return cosine\n",
    "\n",
    "def largest_in_col(arr,nCol):\n",
    "    # \n",
    "    # Find largest value of col nCol on 2D arr\n",
    "    #\n",
    "    \n",
    "    # init value\n",
    "    max_val = arr[0][nCol]\n",
    "    # also, remember index\n",
    "    row_index = 0\n",
    "    \n",
    "    for x in range(0, len(arr)):\n",
    "        if arr[x][nCol] > max_val:\n",
    "            max_val = arr[x][nCol]\n",
    "            row_index = x\n",
    "        \n",
    "    return max_val,row_index\n",
    "\n",
    "def smallest_in_col(arr,nCol):\n",
    "    # \n",
    "    # Find smallest value of col nCol on 2D arr\n",
    "    #\n",
    "    \n",
    "    # init value\n",
    "    min_val = arr[0][nCol]\n",
    "    # also, remember index\n",
    "    row_index = 0\n",
    "    \n",
    "    for x in range(0, len(arr)):\n",
    "        if arr[x][nCol] < min_val:\n",
    "            min_val = arr[x][nCol]\n",
    "            row_index = x\n",
    "        \n",
    "    return min_val,row_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-appearance",
   "metadata": {},
   "source": [
    "### Try to using function\n",
    "using some of array cells, create n matrices\n",
    "\n",
    "1. take one tweet, compare to all data set\n",
    "2. flag 1 if similar\n",
    "3. take next tweet, if similar from prev tweet, skip\n",
    "4. if not similar, add counter, then proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unavailable-clearing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28920/28920.0 [00:02<00:00, 11285.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import array as arr\n",
    "# import multitasking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set to max CPU Cores\n",
    "# multitasking.set_max_threads(multitasking.config[\"CPU_CORES\"] * 5)\n",
    "\n",
    "st = similarity_treshold\n",
    "cluster_no = 1\n",
    "s_score = 0\n",
    "s_score_current = 0\n",
    "i_current_cluster = 0\n",
    "\n",
    "#create zero element array\n",
    "#col 0 => index base tweet\n",
    "#col 1 => cluster number\n",
    "#col 2 => similarity score\n",
    "base_tweet = []\n",
    "\n",
    "#proceed to compare to all tweet\n",
    "cosine_matrix = np.zeros(( len(df), len(df) ), dtype=np.dtype('f4'))\n",
    "\n",
    "# flag the mt\n",
    "with tqdm(total=( (len(df)*len(df)/2))-(len(df)/2))  as pbar:\n",
    "    for j in range(0, len(df)):\n",
    "        tweet_to_compare = df['sw'][j]\n",
    "\n",
    "        #check, is this second tweet?\n",
    "        if(j == 0):\n",
    "            #first tweet, add as cluster no #1\n",
    "            base_tweet.append([j,1,1.0])\n",
    "            i_current_cluster = base_tweet[0][1]\n",
    "\n",
    "        elif(j == 1):\n",
    "            #compare to prev tweet\n",
    "            s_score = calculate_similarity(tweet_to_compare, df['sw'][ base_tweet[0][0] ])\n",
    "\n",
    "            if(s_score < st):\n",
    "                #not similar\n",
    "                base_tweet.append([j,2,1])\n",
    "                i_current_cluster = base_tweet[j][1]\n",
    "\n",
    "        else:\n",
    "            #other else tweet\n",
    "            for x in range(0,len(base_tweet)):\n",
    "                #compare every element\n",
    "                s_score = calculate_similarity(tweet_to_compare, df['sw'][ base_tweet[x][0] ])\n",
    "                base_tweet[x][2] = s_score\n",
    "\n",
    "            if(largest_in_col(base_tweet,2)[0] < st):\n",
    "                #no similar, add as one new cluster\n",
    "                i_current_cluster = i_current_cluster + 1\n",
    "                base_tweet.append([j,i_current_cluster,largest_in_col(base_tweet,2)[0]])\n",
    "            else:\n",
    "                #determine cluster# from biggest similarity\n",
    "                i_current_cluster = base_tweet[(largest_in_col(base_tweet,2)[1])][1]\n",
    "\n",
    "        #proceed to compare to all tweet\n",
    "        for i in range(0,len(df)):\n",
    "            # update progress\n",
    "            if (j<i):\n",
    "                pbar.update(1)\n",
    "                s_score = calculate_similarity(tweet_to_compare, df['sw'][i])\n",
    "                if (s_score >= st):\n",
    "                    cosine_matrix[i,j] = i_current_cluster\n",
    "\n",
    "        \n",
    "pbar.close()\n",
    "print(cosine_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-transparency",
   "metadata": {},
   "source": [
    "### Writing result to file\n",
    "Using NPZ format for efficiency, and try to load them after save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "valid-mobile",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n",
      "Save data complete .... \n"
     ]
    }
   ],
   "source": [
    "# save numpy array as npz file\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "\n",
    "# save to npy file\n",
    "# savez_compressed('./output/df_380.npz',df)\n",
    "# savez_compressed('./output/data_380.npz', cosine_matrix)\n",
    "# savez_compressed('./output/data_380_base.npz', base_tweet)\n",
    "\n",
    "# print(len(cosine_matrix))\n",
    "# print('Save data complete .... ')\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "\n",
    "savetxt('data.csv', cosine_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-tennessee",
   "metadata": {},
   "source": [
    "### How to save to rdbms?\n",
    "save the header data, meta data of the process\n",
    "[ret_analysis_header]\n",
    "- JobID\n",
    "- user initiated\n",
    "- Time Started\n",
    "- Time End\n",
    "\n",
    "Process Parameter\n",
    "[ret_analysis_parameter]\n",
    "- JobID\n",
    "- Param Name\n",
    "- Param Value\n",
    "    - ST Value\n",
    "    - DB ID\n",
    "    - Screen Name\n",
    "    - How Many Tweet analyzed\n",
    "\n",
    "save detail cluster information\n",
    "[ret_base_tweet]\n",
    "- JobID\n",
    "- Tweet Base# --> tweet ID\n",
    "- Cluster#\n",
    "\n",
    "save detail calculation result, structure\n",
    "[ret_cluster_result]\n",
    "- JobID\n",
    "- TweetID\n",
    "- Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-kennedy",
   "metadata": {},
   "source": [
    "### Record cluster result\n",
    "into table ret_cluster_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mineral-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "\n",
    "savetxt('data.csv', cosine_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-foundation",
   "metadata": {},
   "source": [
    "### Record finish time\n",
    "update table ret_analysis_header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-brown",
   "metadata": {},
   "source": [
    "### Marking as finished the job\n",
    "on this particular i_process_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-tooth",
   "metadata": {},
   "source": [
    "### Done Here ...\n",
    "1. Create data structure to record below things\n",
    "    Job Header\n",
    "    - JobID\n",
    "    - Similarity Treshold\n",
    "    - Dataset Parameter\n",
    "        - By User\n",
    "        - By Keyword\n",
    "    - Running time start\n",
    "    - Running time end\n",
    "    \n",
    "    Dataset Result\n",
    "        - base_tweet\n",
    "        - cosine_matrix\n",
    "        \n",
    "2. Push the result data into RDBMS Server, in this case is mySQL\n",
    "    - upload csv file to mySQL\n",
    "    - import to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "saving-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait 10 sec before release\n",
    "time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
