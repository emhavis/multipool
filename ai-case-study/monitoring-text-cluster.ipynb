{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0bfc389a-1053-4245-95e5-d9667d757976",
   "metadata": {},
   "source": [
    "Steps Explanation:\n",
    "Data Loading and Cleaning: Load the JSON data and clean the text data.\n",
    "Feature Extraction: Convert the cleaned text into numerical features using TF-IDF.\n",
    "Optimal Cluster Determination: Use the Elbow Method and Silhouette Score to determine the optimal number of clusters. The optimal number of clusters is determined based on the highest silhouette score.\n",
    "Clustering: Perform clustering using K-Means with the optimal number of clusters.\n",
    "Print Cluster Content: Print the content of each cluster, including the number of members in each cluster.\n",
    "PCA Scatter Plot: Visualize the clusters using a 2D scatter plot with PCA.\n",
    "This script will dynamically determine the optimal number of clusters, perform clustering, and print the content of each cluster along with the number of members in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c02b20a-efa1-46f8-aec9-27c5f0289e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import json\n",
    "\n",
    "import data_connector\n",
    "\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation and numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Function to perform clustering and return the DataFrame with cluster numbers\n",
    "def cluster_and_return(df, max_clusters=15):\n",
    "    # Clean the text data\n",
    "    df['cleaned_text'] = df['text_content'].apply(clean_text)\n",
    "\n",
    "    # Feature extraction using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "    # Determine the optimal number of clusters using Elbow Method and Silhouette Score\n",
    "    def determine_optimal_clusters(X, max_clusters):\n",
    "        wcss = []\n",
    "        silhouette_scores = []\n",
    "        for i in range(2, max_clusters):  # start from 2 clusters\n",
    "            kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "            kmeans.fit(X)\n",
    "            wcss.append(kmeans.inertia_)\n",
    "            if i > 1:\n",
    "                silhouette_avg = silhouette_score(X, kmeans.labels_)\n",
    "                silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "        # Determine optimal number of clusters based on highest silhouette score\n",
    "        optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because silhouette_scores starts from 2 clusters\n",
    "        return optimal_clusters\n",
    "\n",
    "    optimal_clusters = determine_optimal_clusters(X, max_clusters)\n",
    "    print(f'Optimal number of clusters: {optimal_clusters}')\n",
    "\n",
    "    # Calculate average similarity score for each cluster\n",
    "    cosine_sim_matrix = 1 - pairwise_distances(X, metric='cosine')\n",
    "    cluster_scores = []\n",
    "\n",
    "    for cluster in range(optimal_clusters):\n",
    "        indices = np.where(kmeans.labels_ == cluster)[0]\n",
    "        if len(indices) > 1:\n",
    "            cluster_sim = cosine_sim_matrix[np.ix_(indices, indices)]\n",
    "            avg_sim = cluster_sim.mean()\n",
    "        else:\n",
    "            avg_sim = 1.0  # If there's only one member in the cluster, similarity is 1\n",
    "        cluster_scores.extend([avg_sim] * len(indices))\n",
    "\n",
    "    df['cluster_score'] = cluster_scores\n",
    "\n",
    "    # Perform clustering with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    df['cluster_no'] = kmeans.labels_\n",
    "\n",
    "    # Drop the temporary cleaned_text column\n",
    "    df = df.drop(columns=['cleaned_text'])\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8abbbd-42b5-4a9f-bd76-13dfabaf72b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero jobs, quitting now\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/ml_project/jup_env/lib/python3.10/site-packages/pandas/core/indexes/range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     quit()\n\u001b[1;32m     26\u001b[0m similarity_treshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n\u001b[0;32m---> 27\u001b[0m i_process_id \u001b[38;5;241m=\u001b[39m \u001b[43mdf_job\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjobsid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     28\u001b[0m screen_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     29\u001b[0m database_keyword_id \u001b[38;5;241m=\u001b[39m df_job[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_monitoring_media_social\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/ml_project/jup_env/lib/python3.10/site-packages/pandas/core/series.py:1112\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/ml_project/jup_env/lib/python3.10/site-packages/pandas/core/series.py:1228\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1228\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/ml_project/jup_env/lib/python3.10/site-packages/pandas/core/indexes/range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "strSQL = \"\"\"\n",
    "select \ta.id as jobsid,\n",
    "        a.*,\n",
    "\t\tc.id as key_monitoring_media_social, \n",
    "\t\td.id as key_monitoring_media_online,\n",
    "\t\tc.*\n",
    "from \tscreen_analisis_ai a \n",
    "\t\tinner join monitoring_search b\n",
    "\t\t\ton cast(a.monitoring_id as varchar) = cast(b.id as varchar)\n",
    "\t\tinner join monitoring_media_social c\n",
    "\t\t\ton b.id = c.monitoring_search_id \n",
    "\t\tleft outer join  monitoring_media_online d\n",
    "\t\t\ton d.monitoring_search_id = c.monitoring_search_id \n",
    "where a.jenis_analisa = '10'\n",
    "and a.status = 1\n",
    "order by a.created desc \n",
    "limit 1\n",
    "\"\"\"\n",
    "\n",
    "df_job = data_connector.execute_query_psql(strSQL)\n",
    "if len(df_job) == 0:\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "    \n",
    "similarity_treshold = 0.9\n",
    "i_process_id = df_job['jobsid'][0]\n",
    "screen_name = ''\n",
    "database_keyword_id = df_job['key_monitoring_media_social'][0]\n",
    "social_media_monitoring_id = df_job['key_monitoring_media_social'][0]\n",
    "media_online_monitoring_id = df_job['key_monitoring_media_online'][0]\n",
    "\n",
    "# print(database_keyword_id)\n",
    "print(similarity_treshold)\n",
    "print(i_process_id)\n",
    "print(social_media_monitoring_id)\n",
    "print(media_online_monitoring_id)\n",
    "\n",
    "\n",
    "# Prepare SQL Statement\n",
    "print(i_process_id)\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = %s\"\n",
    "sql = sql.replace('%s', str(i_process_id))\n",
    "\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "print('update ' + str(row_count) + ' rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfedb8-84de-477d-9515-32c7c0d22652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marking jobs\n",
    "# Prepare SQL Statement\n",
    "print(i_process_id)\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = %s\"\n",
    "sql = sql.replace('%s', str(i_process_id))\n",
    "\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "print('update ' + str(row_count) + ' rows')\n",
    "\n",
    "#\n",
    "# Create Header Record\n",
    "sql = \"insert into ret_analysis_header (job_id, datetime_start, user_id) values (\" + str(i_process_id) + \", now(), '1')\"\n",
    "# Execute the query\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "#\n",
    "# Create Parameter Record\n",
    "sql = \"\"\"\n",
    "insert into ret_analysis_parameter \n",
    "(job_id, param_id, param_name, param_value) \n",
    "values (%s, %s, %s, %s)\n",
    "\"\"\" % (str(i_process_id), '1', \"'Similarity Treshold'\", str(similarity_treshold))\n",
    "# Execute the query\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "\n",
    "sql = \"\"\"\n",
    "insert into ret_analysis_parameter \n",
    "(job_id, param_id, param_name, param_value) \n",
    "values (%s, %s, %s, '%s')\n",
    "\"\"\" % (str(i_process_id), '1', \"'DB_ID'\", str(database_keyword_id))\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035ac14-6798-4d25-a5d7-58c0468e8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_result(df_result, platform_id):\n",
    "    # id_key and text_content fields.\n",
    "    # create statement template\n",
    "    sql = \"\"\"\n",
    "    insert into ret_cluster_result_monitor (ref_id, cluster_no, platform_id, job_id) \n",
    "    values ('%s', %s, %s, %s)\n",
    "    \"\"\"\n",
    "    for index, row in save_df.iterrows():    \n",
    "        if row['cluster_number'] == '':\n",
    "            s_cluster_number = \"NULL\"\n",
    "        else:\n",
    "            s_cluster_number = row['cluster_number']\n",
    "            \n",
    "        sql = sql % (row['ref_id'],row['cluster_no'], platform_id, i_process_id)\n",
    "    \n",
    "    print(sql)\n",
    "    # data_connector.execute_query_psql(sql)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71062b4-4d69-4c14-9c96-8bd094a3f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 = tiktok\n",
    "# 20 = youtube\n",
    "# 30 = instagram_post\n",
    "# 40 = facebook_post\n",
    "# 50 = google_result\n",
    "\n",
    "# Processing jobs for each platform\n",
    "iRowCount = 0\n",
    "\n",
    "if df_job['is_tiktok'][0]:\n",
    "    sql = '''\n",
    "    select id as ref_id, \"desc\" as text_content \n",
    "    from tiktok \n",
    "    where monitoring_id = '%s' -- 10\n",
    "    '''\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df = cluster_and_return(df)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_youtube'][0]:\n",
    "    sql = '''\n",
    "    select \tid as ref_id, title as text_content\n",
    "    from youtube \n",
    "    where monitoring_id = '%s' -- 20\n",
    "    '''\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df = cluster_and_return(df)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_instagram'][0]:\n",
    "    sql = '''\n",
    "    select id as ref_id, content as text_content\n",
    "    from instagram_post \n",
    "    where monitoring_id = '%s' -- 30\n",
    "    '''\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df = cluster_and_return(df)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_facebook'][0]:\n",
    "    sql = '''\n",
    "    select id as ref_id, \"description\" as text_content\n",
    "    from facebook_post \n",
    "    where monitoring_id = '%s' -- 40\n",
    "    and length(trim(\"description\")) > 0\n",
    "    '''\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df = cluster_and_return(df)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_google'][0]:\n",
    "    sql = '''\n",
    "    select \tid as ref_id, \"description\" as text_content\n",
    "    from google_result \n",
    "    where monitoring_id = '%s' -- 50\n",
    "    '''\n",
    "    df_google = data_connector.execute_query_psql(sql % (media_online_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df = cluster_and_return(df)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_twitter'][0]:\n",
    "    sql = '''\n",
    "    select id as ref_id, tweet as text_content\n",
    "    from twitter_tweets \n",
    "    where monitoring_id = '%s' -- 10\n",
    "    '''\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df = cluster_and_return(df)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "print(\"Total Processed Data: \" + str(iRowCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057c10f-04d2-4b78-8926-83e8db77e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finishing Jobs\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "data_connector.execute_query_psql(sql % (i_process_id, 1, \"'#Content Processed'\",iRowCount))\n",
    "\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"update ret_analysis_header set datetime_finish = NOW() where job_id = %s\"\n",
    "# Executing query\n",
    "data_connector.execute_query_psql(sql % (i_process_id) )\n",
    "\n",
    "sql = \"update screen_analisis_ai set status = 3, duration = EXTRACT(EPOCH FROM (now() - start_process)), end_process = NOW() where id = %s\"\n",
    "data_connector.execute_query_psql(sql % (i_process_id))\n",
    "\n",
    "print('inserting result finished')\n",
    "\n",
    "# Wait 10 sec before release\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c5b19-bc3e-47e8-abc1-8944d479ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform clustering and return the updated DataFrame\n",
    "# result_df = cluster_and_return(df)\n",
    "# print(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
