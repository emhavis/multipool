{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77be3745-de0f-4f27-a7ce-f0b3839d647f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "1. get sample image\n",
    "2. download image\n",
    "3. perform imnage grouping\n",
    "4. inspect result\n",
    "\n",
    "Next items to do, prepare and package as jobs\n",
    "1. create auto-query\n",
    "2. create sql to store result\n",
    "3. export to executable py\n",
    "\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse)\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');\n",
    "\n",
    "# prepare data for saving to rdbms\n",
    "# print(clus_new)\n",
    "# i = 0 \n",
    "result_cluster = []\n",
    "\n",
    "for item in clus_new:\n",
    "    # print(clus_new[item])\n",
    "    print(len(clus_new[item]))\n",
    "    for file in clus_new[item]:\n",
    "        # print(file, item)\n",
    "        result_cluster.append(file)\n",
    "    # i = i + 1\n",
    "\n",
    "# view_cluster_2(0)\n",
    "len(clus_new)\n",
    "len(result_cluster)\n",
    "\n",
    "\n",
    " * Kesepakatan status di kolom screen_analisis_ai.status\n",
    " * 1 --> baru diinput\n",
    " * 2 --> lagi dikerjakan\n",
    " * 3 --> proses berhasil\n",
    " * 4 --> proses gagal\n",
    " *\n",
    " \n",
    " * Kesepakatan jenis analisa AI\n",
    " * 1 --> Analisa Cluster\n",
    " * 2 --> Analisa image clustering\n",
    " * 3 --> Analisa sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0951b4-bdc4-4c7a-9846-016d02539805",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for loading/processing the images  \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_img \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m img_to_array \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg16\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_input \n",
      "File \u001b[0;32m~/cekmedsos/lib/python3.10/site-packages/keras/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[0;32m~/cekmedsos/lib/python3.10/site-packages/keras/__internal__/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[0;32m~/cekmedsos/lib/python3.10/site-packages/keras/__internal__/backend/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "File \u001b[0;32m~/cekmedsos/lib/python3.10/site-packages/keras/src/__init__.py:21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n",
      "File \u001b[0;32m~/cekmedsos/lib/python3.10/site-packages/keras/src/applications/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtBase\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtLarge\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[0;32m~/cekmedsos/lib/python3.10/site-packages/keras/src/applications/convnext.py:26\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"ConvNeXt models for Keras.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mReferences:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m  (CVPR 2022)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# for loading/processing the images  \n",
    "from keras.preprocessing.image import load_img \n",
    "from keras.preprocessing.image import img_to_array \n",
    "from keras.applications.vgg16 import preprocess_input \n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bdb754-c427-4732-8940-d82b64678866",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize_and_upload_image(url, save_folder):\n",
    "    try:\n",
    "        # Fetch the image from the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Open the image from the response content\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "\n",
    "        # Check the image size in bytes\n",
    "        image_size = len(response.content)\n",
    "\n",
    "        # Set the maximum allowed size in bytes (200KB)\n",
    "        max_size = 200 * 1024\n",
    "\n",
    "        # Extract the base path from the URL for saving the images\n",
    "        parsed_url = urlparse(url)\n",
    "        base_path = os.path.basename(parsed_url.path)\n",
    "\n",
    "        # Determine the paths for saving the images\n",
    "        original_image_path = os.path.join(save_folder, 'original_' + base_path)\n",
    "        resized_image_path = os.path.join(save_folder, 'resized_' + base_path)\n",
    "\n",
    "        # If the image size exceeds the maximum allowed size, resize it\n",
    "        if image_size > max_size:\n",
    "            print('more than 200kb')\n",
    "            print(original_image_path)\n",
    "            print(resized_image_path)\n",
    "            \n",
    "            # Calculate the scaling factor to reduce the image size\n",
    "            scale_factor = (max_size / image_size) ** 0.5\n",
    "            new_width = int(image.width * scale_factor)\n",
    "            new_height = int(image.height * scale_factor)\n",
    "\n",
    "            # Resize the image\n",
    "            resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "\n",
    "            # Save the resized image to the local folder\n",
    "            resized_image.save(resized_image_path, format=\"JPEG\")\n",
    "\n",
    "            # Reupload the resized image to the original location\n",
    "            with open(resized_image_path, 'rb') as resized_file:\n",
    "                reupload_url = url  # Use the original URL for reupload\n",
    "                print(url)\n",
    "                files = {'file': ('resized_' + base_path, resized_file)}\n",
    "                reupload_response = requests.post(reupload_url, files=files)\n",
    "                print(reupload_response)\n",
    "\n",
    "            # Delete the original image in the local folder\n",
    "            os.remove(original_image_path)\n",
    "\n",
    "        else:\n",
    "            # Image is smaller than 200KB, no need to resize or reupload\n",
    "            resized_image_path = original_image_path\n",
    "\n",
    "        return original_image_path, resized_image_path\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors such as invalid URLs or image processing issues\n",
    "        return str(e)\n",
    "\n",
    "# Example usage:\n",
    "# imageURL = \"https://example.com/path/to/your/image.jpg\"\n",
    "# save_folder = \"path/to/your/local/folder\"\n",
    "# original_image_path, resized_image_path = resize_and_upload_image(imageURL, save_folder)\n",
    "# Both original_image_path and resized_image_path will contain the file paths in the local folder.\n",
    "\n",
    "def download_image(url, folder_path):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Extract the filename from the URL\n",
    "            filename = os.path.join(folder_path, os.path.basename(url))\n",
    "\n",
    "            # Save the image to the specified folder location\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            file_size_bytes = os.path.getsize(filename)\n",
    "            file_size_kb = file_size_bytes / 1024\n",
    "\n",
    "            if file_size_kb >= 200:\n",
    "                print(\"Resizing...\")\n",
    "                resize_and_upload_image(filename,url)\n",
    "            # print(f\"Image downloaded and saved as {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download image. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "def resize_and_upload_image(file_path, upload_url, target_size_kb=200):\n",
    "    try:\n",
    "        # Open the image from the file\n",
    "        with Image.open(file_path) as image:\n",
    "            # Calculate the target size in bytes\n",
    "            target_size_bytes = target_size_kb * 1024\n",
    "\n",
    "            # Initialize the quality variable\n",
    "            quality = 95\n",
    "\n",
    "            while os.path.getsize(file_path) > target_size_bytes:\n",
    "                # Resize the image while keeping the quality constant\n",
    "                width, height = image.size\n",
    "                new_width = int(width * 0.9)\n",
    "                new_height = int(height * 0.9)\n",
    "\n",
    "                # Ensure the dimensions are at least 1\n",
    "                new_width = max(1, new_width)\n",
    "                new_height = max(1, new_height)\n",
    "\n",
    "                image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "                # Save the resized image to the same file\n",
    "                image.save(file_path, \"JPEG\", quality=quality)\n",
    "\n",
    "            if os.path.getsize(file_path) <= target_size_bytes:\n",
    "                print(f\"Image resized and overwritten at: {file_path}\")\n",
    "\n",
    "                # Upload the resized image to the given URL\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    response = requests.post(upload_url, files={'file': (os.path.basename(file_path), file)})\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"Image uploaded to {upload_url}\")\n",
    "                else:\n",
    "                    print(f\"Upload failed with status code {response.status_code}\")\n",
    "            else:\n",
    "                print(f\"Image not resized, not uploaded.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "## query functions\n",
    "def execute_mysql_query(query):\n",
    "    try:\n",
    "        # Define the connection parameters inside the function\n",
    "        host = \"202.157.185.40\"\n",
    "        port = 3306  # Replace with your MySQL port number\n",
    "        database = \"cekmedsos_database\"\n",
    "        user = \"cekmedsos_db\"\n",
    "        password = \"282E~f0si\"\n",
    "\n",
    "        # Create a SQLAlchemy engine using the provided connection parameters\n",
    "        connection_url = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{database}\"\n",
    "        engine = create_engine(connection_url)\n",
    "\n",
    "        # Establish a connection\n",
    "        connection = engine.connect()\n",
    "\n",
    "        # Execute the MySQL query and fetch the results into a DataFrame\n",
    "        result_df = pd.read_sql(query, connection)\n",
    "\n",
    "        # Close the database connection\n",
    "        connection.close()\n",
    "\n",
    "        print(\"Query executed successfully.\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def clear_image():\n",
    "    folder_path = '/home/qudoco/jupyter/ai-project/ai-case-study/img'\n",
    "    # Create a Path object for the folder\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    # Iterate through the files in the folder and delete them\n",
    "    for file in folder.iterdir():\n",
    "        try:\n",
    "            if file.is_file():\n",
    "                file.unlink()\n",
    "                # print(f\"Deleted {file}\")\n",
    "            else:\n",
    "                print(f\"{file} is not a file.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {str(e)}\")\n",
    "\n",
    "# extracting feature from image files\n",
    "def extract_features(file, model):\n",
    "    # load the image as a 224x224 array\n",
    "    img = load_img(file, target_size=(224,224))\n",
    "    # convert from 'PIL.Image.Image' to numpy array\n",
    "    img = np.array(img) \n",
    "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n",
    "    reshaped_img = img.reshape(1,224,224,3) \n",
    "    # prepare image for model\n",
    "    imgx = preprocess_input(reshaped_img)\n",
    "    # get the feature vector\n",
    "    features = model.predict(imgx, use_multiprocessing=True)\n",
    "    return features\n",
    "\n",
    "# function that lets you view a cluster (based on identifier)        \n",
    "def view_cluster(cluster):\n",
    "    plt.figure(figsize = (25,25));\n",
    "    # gets the list of filenames for a cluster\n",
    "    files = groups[cluster]\n",
    "    # only allow up to 30 images to be shown at a time\n",
    "    if len(files) > 30:\n",
    "        print(f\"Clipping cluster size from {len(files)} to 30\")\n",
    "        files = files[:29]\n",
    "    # plot each image in the cluster\n",
    "    for index, file in enumerate(files):\n",
    "        plt.subplot(10,10,index+1);\n",
    "        img = load_img(file)\n",
    "        img = np.array(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7431f-bef6-42e7-a49e-27ebcf66c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Get jobsid from queue table\n",
    "# \n",
    "iJobID = 0\n",
    "sql = \"select id, hastag, `parameter` \\\n",
    "from screen_analisis_ai \\\n",
    "where active = 1 \\\n",
    "and status = 1 \\\n",
    "and jenis_analisa = 2 \\\n",
    "order by created asc, id asc limit 1\"\n",
    "# sql = \"select * from screen_analisis_ai saa where id = 1913\"\n",
    "\n",
    "df_res = execute_mysql_query(sql)\n",
    "\n",
    "# check job availability\n",
    "if(len(df_res)) == 0:\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "\n",
    "df_res.head()\n",
    "iJobID = df_res['hastag'][0]\n",
    "const_job_id = df_res['id'][0]\n",
    "const_parameter = df_res['parameter'][0]\n",
    "\n",
    "print('iJobID: ' + str(iJobID))\n",
    "print('const_job_id: ' + str(const_job_id))\n",
    "print('const_parameter: ' + str(const_parameter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad97233a-c5eb-404b-9e92-f883fe3f4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "def execute_sqlalchemy_transaction(transaction_query):\n",
    "    # Database connection parameters\n",
    "    db_url = 'mysql://cekmedsos_db:282E~f0si@202.157.185.40/cekmedsos_database'\n",
    "        # Define the connection parameters inside the function\n",
    "    \n",
    "    try:\n",
    "        # Create a SQLAlchemy engine and session\n",
    "        engine = create_engine(db_url)\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "\n",
    "        # Begin a transaction\n",
    "        session.begin()\n",
    "\n",
    "        try:\n",
    "            # Execute the transaction query\n",
    "            query_text = text(transaction_query)\n",
    "            session.execute(query_text)\n",
    "\n",
    "            # Commit the transaction if the query succeeded\n",
    "            session.commit()\n",
    "            #print(\"Transaction completed successfully.\")\n",
    "        except SQLAlchemyError as e:\n",
    "            # Rollback the transaction on error\n",
    "            session.rollback()\n",
    "            print(f\"Transaction error: {str(e)}\")\n",
    "        finally:\n",
    "            # Close the session\n",
    "            session.close()\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error connecting to the database: {str(e)}\")\n",
    "\n",
    "# # Example usage:\n",
    "# transaction_query = \"\"\"\n",
    "#     UPDATE screen_analisis_ai\n",
    "#     SET status = 2,\n",
    "#         last_status_update = now(),\n",
    "#         start_process = now()\n",
    "#     WHERE id = '1841';\n",
    "# \"\"\"\n",
    "\n",
    "# execute_sqlalchemy_transaction(transaction_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f216f10-c9cf-45c4-8a41-3ab3245d8c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## report to dbms that we are working on this row\n",
    "sql = \"\"\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = '\" + str(const_job_id) + \"';\"\n",
    "print(sql)\n",
    "# execute\n",
    "execute_sqlalchemy_transaction(sql)\n",
    "#\n",
    "# Create Header Record\n",
    "sql = \"insert into ret_analysis_header (job_id, datetime_start, user_id) values (%s, now(), %s)\" % (const_job_id,\"1\")\n",
    "# Execute the query\n",
    "execute_sqlalchemy_transaction(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14cc5e6-bd16-4a40-b585-478eeea0e8c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get images from server\n",
    "# Example usage:\n",
    "query = \"select a.id, a.db_id, c.tweet_id, c.filename \\\n",
    "from ret_available_db a inner join ret_tweet b on a.db_id = b.db_id \\\n",
    "inner join media_files c on a.db_id = c.db_id and b.id = c.tweet_id \\\n",
    "where \ta.db_id = \" + str(iJobID)\n",
    "\n",
    "result_df = execute_mysql_query(query)\n",
    "\n",
    "## downloading image set\n",
    "img_prefix_http = \"https://cekmedsos.com/uploads/twimg/\"\n",
    "folder_path = \"/home/qudoco/jupyter/ai-project/ai-case-study/img\"    # Replace with the desired folder path\n",
    "\n",
    "# download_image(url, folder_path)\n",
    "# clear up folder first\n",
    "clear_image()\n",
    "\n",
    "i = 0\n",
    "for index, row in result_df.iterrows():\n",
    "    # print(result_df.at[index,'filename'])\n",
    "    url = img_prefix_http + result_df.at[index,'filename']\n",
    "    print(url)\n",
    "    download_image(url, folder_path)\n",
    "    i = i +1\n",
    "\n",
    "print(\"finished downloading \" + str(i) + \" image set\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c4e8c-9403-4fc5-b1c1-562e1f0cf97a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "path = \"/home/qudoco/jupyter/ai-project/ai-case-study/img\"\n",
    "# change the working directory to the path where the images are located\n",
    "os.chdir(path)\n",
    "\n",
    "# this list holds all the image filename\n",
    "flowers = []\n",
    "\n",
    "# creates a ScandirIterator aliased as files\n",
    "with os.scandir(path) as files:\n",
    "  # loops through each file in the directory\n",
    "    for file in files:\n",
    "        if file.name.endswith('.jpg'):\n",
    "          # adds only the image files to the flowers list\n",
    "            flowers.append(file.name)\n",
    "        if file.name.endswith('.png'):\n",
    "            # adds only the image files to the flowers list\n",
    "            flowers.append(file.name)\n",
    "            \n",
    "model = VGG16()\n",
    "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
    "   \n",
    "data = {}\n",
    "p = \"/home/qudoco/jupyter/ai-project/ai-case-study/img/fvec\"\n",
    "\n",
    "# lop through each image in the dataset\n",
    "for flower in flowers:\n",
    "    # try to extract the features and update the dictionary\n",
    "    try:\n",
    "        feat = extract_features(flower,model)\n",
    "        data[flower] = feat\n",
    "    # if something fails, save the extracted features as a pickle file (optional)\n",
    "    except:\n",
    "        with open(p,'wb') as file:\n",
    "            pickle.dump(data,file)\n",
    "          \n",
    "# get a list of the filenames\n",
    "filenames = np.array(list(data.keys()))\n",
    "\n",
    "# get a list of just the features\n",
    "feat = np.array(list(data.values()))\n",
    "\n",
    "# reshape so that there are 210 samples of 4096 vectors\n",
    "feat = feat.reshape(-1,4096)\n",
    "\n",
    "# get the unique labels (from the flower_labels.csv)\n",
    "df = pd.read_csv('/home/qudoco/jupyter/ai-project/ai-case-study/flower_labels.csv')\n",
    "label = df['label'].tolist()\n",
    "unique_labels = list(set(label))\n",
    "\n",
    "# reduce the amount of dimensions in the feature vector\n",
    "pca = PCA(n_components=100, random_state=22)\n",
    "pca.fit(feat)\n",
    "x = pca.transform(feat)\n",
    "\n",
    "# cluster feature vectors\n",
    "kmeans = KMeans(n_clusters=len(unique_labels), random_state=22)\n",
    "kmeans.fit(x)\n",
    "\n",
    "# holds the cluster id and the images { id: [images] }\n",
    "groups = {}\n",
    "for file, cluster in zip(filenames,kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(file)\n",
    "    else:\n",
    "        groups[cluster].append(file)\n",
    "\n",
    "# this is just incase you want to see which value for k might be the best \n",
    "sse = []\n",
    "list_k = list(range(3, 50))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, random_state=22)\n",
    "    km.fit(x)\n",
    "    \n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# function to calculate clusters\n",
    "def cluster(filePaths, features, threshold=0.9):\n",
    "    features = features.reshape(-1,4096)\n",
    "    simMatrix = cosine_similarity(features)\n",
    "    clusters = {}\n",
    "    for i in range(len(features)):\n",
    "        dupIdx = list(np.where(simMatrix[i] > threshold)[0])\n",
    "        # The similarity matrix will include comparisons of items with themselves, which will always \n",
    "        # result in a similarity of 1.0 (100%) and is redundant, so we ignore those\n",
    "        if len(dupIdx) > 1:\n",
    "            curCluster, clusterMatch = list(dupIdx), None\n",
    "            # The first time an image is found to be in any given cluster, we log the entire cluster, \n",
    "            # so subsequent checks of other images from the same cluster would result in duplicated clusters.\n",
    "            # Check for that here\n",
    "            for cIdx in clusters:\n",
    "                if curCluster[0] in clusters[cIdx]:\n",
    "                    clusterMatch = cIdx\n",
    "                    break\n",
    "            # If the current cluster didn't match any existing ones, create/log it\n",
    "            if clusterMatch == None: clusters[len(clusters)] = curCluster\n",
    "    # Resolve file indices back to file paths\n",
    "    for cIdx in clusters: clusters[cIdx] = [filePaths[x] for x in clusters[cIdx]]\n",
    "    return clusters\n",
    "\n",
    "# another method of clustering based on CSI\n",
    "clus_new = cluster(filenames, feat, threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431c067-bb02-4785-9f0a-511ceacbb4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for saving to rdbms\n",
    "save_df = result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19662fd0-5dd8-466f-a505-a81568f2a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df[[\"file_folder\",\"filename_actual\"]] = save_df['filename'].str.split('/',n=1, expand=True)\n",
    "save_df['cluster_number'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81fc242-b9f9-4d7e-a4e2-7e4605826aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_cluster = []\n",
    "\n",
    "for item in clus_new:\n",
    "    # print(clus_new[item])\n",
    "    # print(len(clus_new[item]))\n",
    "    for file in clus_new[item]:\n",
    "        # print(item)\n",
    "        print(file)\n",
    "        filter_condition = save_df['filename_actual'] == file\n",
    "        save_df.loc[filter_condition,'cluster_number'] = item\n",
    "        result = save_df[filter_condition]\n",
    "        # result['cluster_number'] = item\n",
    "        # print(save_df.loc[filter_condition,'cluster_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d585c9-ce51-461d-86c7-c9bc2c45fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving result to table\n",
    "# jobid, tweet_id, cluster_no\n",
    "\n",
    "s_cluster_number = \"\"\n",
    "\n",
    "for index, row in save_df.iterrows():\n",
    "    if row['cluster_number'] == '':\n",
    "        s_cluster_number = \"NULL\"\n",
    "    else:\n",
    "        s_cluster_number = row['cluster_number']\n",
    "        \n",
    "    sql = \"INSERT into ret_cluster_result (job_id, tweet_id, cluster_no) values (%s, %s, %s)\" % (str(const_job_id), row['tweet_id'], s_cluster_number)\n",
    "    print(sql)\n",
    "    execute_sqlalchemy_transaction(sql)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f02c84-7b1a-44c5-9b1f-40a952c24729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing .... report back job status into rdbms\n",
    "sql = \"update screen_analisis_ai set end_process = now(), status = 3, processby_id = 1 where id = %s\" % (str(const_job_id))\n",
    "execute_sqlalchemy_transaction(sql)\n",
    "\n",
    "sql = \"update screen_analisis_ai set duration = TIMEDIFF(end_process, start_process) where id = \" + str(const_job_id)\n",
    "execute_sqlalchemy_transaction(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e4f75-0cb1-403d-bce5-355edb5f9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait 10 seconds before finished\n",
    "import time\n",
    "time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
