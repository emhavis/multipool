{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc88b042-9d24-48fc-94a3-6009b4951e7b",
   "metadata": {},
   "source": [
    "# Multipool Google Sentiment Analysis\n",
    "\n",
    "this function will get data from google_result table\n",
    "and calculate the sentiment analysis. Result is store in google_result_sentiment\n",
    "\n",
    "crontab command\n",
    "* * * * * /home/haviz/multipool/ai-case-study/run-ai-sa-google.sh >> /home/haviz/multipool/ai-sa-google.log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0afb3-cd43-479c-a645-5eb16d589ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pymysql.cursors\n",
    "import sqlalchemy as sa\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from sqlalchemy import create_engine, Column, Integer, String, MetaData, Table\n",
    "from sqlalchemy.sql import text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64d1af-b458-4c9e-9762-1d3d780b616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define maximum allowed connections\n",
    "MAX_CONNECTIONS = 10\n",
    "\n",
    "# Track the number of active connections\n",
    "active_connections = 0\n",
    "\n",
    "def execute_query(query, params=None):\n",
    "    global active_connections\n",
    "\n",
    "    # Ensure active connections do not exceed the maximum limit\n",
    "    if active_connections >= MAX_CONNECTIONS:\n",
    "        print(\"Maximum connection limit reached. Cannot execute query.\")\n",
    "        return None\n",
    "\n",
    "    # Set your PostgreSQL connection parameters\n",
    "    db_params = {\n",
    "        'host': '98.98.117.105',\n",
    "        'port': '5432',\n",
    "        'database': 'medols',\n",
    "        'user': 'postgres',\n",
    "        'password': 'FEWcTB3JIX5gK4T06c1MdkM9N2S8w9pb',\n",
    "    }\n",
    "\n",
    "    # Create a SQLAlchemy engine\n",
    "    engine = create_engine(f\"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}\")\n",
    "\n",
    "    # Create a session\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    try:\n",
    "        # Increment active connections count\n",
    "        active_connections += 1\n",
    "\n",
    "        # Execute the query with optional parameters\n",
    "        result = session.execute(text(query), params)\n",
    "\n",
    "        # Check if the query is a SELECT query\n",
    "        is_select_query = result.returns_rows\n",
    "\n",
    "        if is_select_query:\n",
    "            # Fetch the data and return as a Pandas DataFrame\n",
    "            columns = result.keys()\n",
    "            fetched_data = result.fetchall()\n",
    "            df = pd.DataFrame(fetched_data, columns=columns)\n",
    "            return df\n",
    "        else:\n",
    "            # Get the number of rows affected for non-SELECT queries\n",
    "            rows_affected = result.rowcount\n",
    "\n",
    "            # Commit the changes to the database for non-SELECT queries\n",
    "            session.commit()\n",
    "            return rows_affected\n",
    "    except SQLAlchemyError as e:\n",
    "        # Rollback changes if there's an error\n",
    "        session.rollback()\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Decrement active connections count\n",
    "        active_connections -= 1\n",
    "\n",
    "        # Close the session\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf0378-4165-46bd-abc9-05c3a9fe18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def remove_mentions(tweet):\n",
    "    # Define regular expression pattern to match mentions\n",
    "    pattern = r'@\\w+'\n",
    "    \n",
    "    # Replace mentions with an empty string\n",
    "    cleaned_tweet = re.sub(pattern, '', tweet)\n",
    "    \n",
    "    return cleaned_tweet\n",
    "\n",
    "def stemming(comment):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    do = []\n",
    "    for w in comment:\n",
    "        dt = stemmer.stem(w)\n",
    "        do.append(dt)\n",
    "    d_clean = []\n",
    "    d_clean = \" \".join(do)\n",
    "    return d_clean\n",
    "    \n",
    "# function case folding\n",
    "def casefolding(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = comment.strip(\" \")\n",
    "    comment = re.sub(r'[?|$|.|!_:\")(-+,]','',comment)\n",
    "    return comment\n",
    "\n",
    "def clean_up_tag(comment):\n",
    "    x_ret = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",comment).split())\n",
    "    return x_ret\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    \n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    \n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    \n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    \n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    return strOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a292a-3e97-4161-bcaf-1887c838ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_by_df(source_ds):\n",
    "    from tqdm import tqdm\n",
    "    pd.options.mode.chained_assignment = None \n",
    "\n",
    "    # we calculate sentiment for 'title' and 'description' column\n",
    "\n",
    "    # remove first\n",
    "    source_ds['title'] = source_ds['title'].apply(clean_up_tag)\n",
    "    source_ds['description'] = source_ds['description'].apply(clean_up_tag)\n",
    "\n",
    "\n",
    "    # skip stemming\n",
    "    source_ds['stemmed'] = source_ds['title']\n",
    "    source_ds['stemmed'] = source_ds['stemmed'].apply(casefolding)\n",
    "    source_ds['stemmed'] = source_ds['stemmed'].apply(text_preproc)\n",
    "\n",
    "    sw = stopwords.words('indonesian')\n",
    "    # tokenized\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: nltk.word_tokenize(row['stemmed']), axis=1)\n",
    "\n",
    "    # apply stopword removal\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "\n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "\n",
    "    # Create a new list and insert each element from the original list\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    list_text = source_ds['stemmed'].tolist()\n",
    "    new_list = []\n",
    "    for i in tqdm(range( len(list_text) )):\n",
    "        res = pipe(list_text[i])\n",
    "        #print(res)\n",
    "        new_list.append(res)\n",
    "\n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "    # create a list of our conditions\n",
    "    for i in range(0, len(new_list)):\n",
    "        # print(new_list[i][0]['label'])\n",
    "        if new_list[i][0]['label'] == 'Positive':\n",
    "            new_list[i][0]['Value'] = 1\n",
    "        elif new_list[i][0]['label'] == 'Negative':\n",
    "            new_list[i][0]['Value'] = -1\n",
    "        elif new_list[i][0]['label'] == 'Neutral':\n",
    "            new_list[i][0]['Value'] = 0\n",
    "\n",
    "    for index, row in source_ds.iterrows():\n",
    "        source_ds.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "\n",
    "    for index, row in source_ds.iterrows():\n",
    "        source_ds.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "        source_ds.at[index,'Score'] = new_list[index][0]['score']\n",
    "        #source_ds.at[index,'Score'] = new_list[index][0]['Value']\n",
    "        \n",
    "    return source_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa271b-38b8-4b07-bbf8-a47a457be5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging to a file\n",
    "import logging\n",
    "logging.basicConfig(filename='error.log', level=logging.ERROR)\n",
    "\n",
    "def calculate_sentiment(row, col_description):\n",
    "    try:\n",
    "        # cleansing\n",
    "        strSource = row[col_description]\n",
    "\n",
    "        # cleansing\n",
    "        strSource = clean_up_tag(strSource)\n",
    "        strSource = casefolding(strSource)\n",
    "        strSource = text_preproc(strSource)\n",
    "        strSource = remove_mentions(strSource)\n",
    "\n",
    "        # tokenized\n",
    "        sToken = nltk.word_tokenize(strSource)\n",
    "\n",
    "        # remove stopwords\n",
    "        sw = stopwords.words('indonesian')\n",
    "\n",
    "        filtered_words = [word for word in sToken if word.lower() not in sw]\n",
    "        filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "        # Notes on return sentiment values\n",
    "        # 1 >> positif >> stay\n",
    "        # 2 >> negatif >> convert to -1\n",
    "        # 0 >> netral >> stay\n",
    "        result = pipe(strSource)\n",
    "\n",
    "        if result[0]['label'] == 'Negative':\n",
    "            sentimentClass = -1\n",
    "        elif result[0]['label'] == 'Positive':\n",
    "            sentimentClass = 1\n",
    "        else:\n",
    "            sentimentClass = 0\n",
    "\n",
    "        score = result[0]['score']\n",
    "\n",
    "        # put to result    \n",
    "        return pd.Series({'sentiment_category': sentimentClass, 'score': score})\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Log the exception to the error.log file\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        # Return default values or None for the columns\n",
    "        return pd.Series({'sentiment_category': None, 'score': None})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325ad3f-7285-4e5a-bed9-8942d2df8c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Calculate sentiment from table twitter_tweets and record to rdbms\n",
    "# \n",
    "def record_tweet_sentiment(row):\n",
    "    ssql = \"\"\"\n",
    "    INSERT into twitter_tweets_result_sentiment \n",
    "    (sentiment_category, score, twitter_tweets_id)\n",
    "    VALUES (%s, %s, %s);\n",
    "    \"\"\" %(row['sentiment_category'],row['score'],row['id'])\n",
    "    #\n",
    "    # print(ssql)\n",
    "    execute_query(ssql)\n",
    "\n",
    "select_query = \"\"\"\n",
    "SELECT t.id, t.tweet\n",
    "FROM twitter_tweets t\n",
    "LEFT JOIN twitter_tweets_result_sentiment r ON t.id = r.twitter_tweets_id\n",
    "WHERE r.twitter_tweets_id IS NULL\n",
    "ORDER BY t.id desc limit 5000\n",
    "\"\"\"\n",
    "\n",
    "source_ds = execute_query(select_query)\n",
    "\n",
    "if len(source_ds) != 0:\n",
    "    tqdm.pandas(desc=\"Calculating SA\", total=len(source_ds))\n",
    "    source_ds[['sentiment_category','score']] = source_ds.progress_apply(calculate_sentiment, args=('tweet',), axis=1)\n",
    "\n",
    "    tqdm.pandas(desc=\"Recording SA\", total=len(source_ds))\n",
    "    source_ds.progress_apply(record_tweet_sentiment, axis=1)    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b5528-6ca4-411d-9b57-069e3a6d3a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# calculate sentiment result for google_result\n",
    "# \n",
    "\n",
    "def record_google_sentiment(row):\n",
    "    ssql = \"\"\"\n",
    "    INSERT INTO public.google_result_sentiment \n",
    "    (id, sentiment_category, score, google_result_id) \n",
    "    VALUES(nextval('google_result_sentiment_id_seq'::regclass), %s, %s, %s);\n",
    "    \"\"\" %(row['sentiment_category'],row['score'],row['google_result_id'])\n",
    "    #\n",
    "    # print(ssql)\n",
    "    execute_query(ssql)\n",
    "\n",
    "select_query = \"select * \\\n",
    "from google_result gr \\\n",
    "where google_result_id not in (select google_result_id from google_result_sentiment) \\\n",
    "order by google_result_id desc limit 100\"\n",
    "\n",
    "source_ds2 = execute_query(select_query)\n",
    "\n",
    "if len(source_ds2) != 0:\n",
    "    source_ds2[['sentiment_category','score']] = source_ds2.apply(calculate_sentiment, args=('title',), axis=1)\n",
    "    source_ds2.apply(record_google_sentiment, axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
