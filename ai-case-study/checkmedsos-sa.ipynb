{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12060b76-15c3-48d2-9120-29c8826ee849",
   "metadata": {},
   "source": [
    "# Huggingface Model for Sentiment Analysis\n",
    "Source Data: checkmedsos_db, db_id = 87247\n",
    "\n",
    "1. create calculation example\n",
    "2. post to mysql\n",
    "3. inspect the result\n",
    "\n",
    " * Kesepakatan status di kolom screen_analisis_ai.status\n",
    " * 1 --> baru diinput\n",
    " * 2 --> lagi dikerjakan\n",
    " * 3 --> proses berhasil\n",
    " * 4 --> proses gagal\n",
    " *\n",
    " \n",
    " * Kesepakatan jenis analisa AI\n",
    " * 1 --> Analisa Cluster\n",
    " * 2 --> Analisa image clustering\n",
    " * 3 --> Analisa sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057ca66-229f-4278-a165-ab16afd85dac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pymysql.cursors\n",
    "import sqlalchemy as sa\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa\")\n",
    "\n",
    "# database connection properties\n",
    "connection_url = \"mysql://cekmedsos_db:282E~f0si@202.157.185.40/cekmedsos_database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb152b-5d2d-4836-9138-7b4f6710a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def execute_sqlalchemy_transaction(transaction_query):\n",
    "    # Database connection parameters\n",
    "    db_url = 'mysql://cekmedsos_db:282E~f0si@202.157.185.40/cekmedsos_database'\n",
    "        # Define the connection parameters inside the function\n",
    "    \n",
    "    try:\n",
    "        # Create a SQLAlchemy engine and session\n",
    "        engine = create_engine(db_url)\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "\n",
    "        # Begin a transaction\n",
    "        session.begin()\n",
    "\n",
    "        try:\n",
    "            # Execute the transaction query\n",
    "            query_text = text(transaction_query)\n",
    "            session.execute(query_text)\n",
    "\n",
    "            # Commit the transaction if the query succeeded\n",
    "            session.commit()\n",
    "            #print(\"Transaction completed successfully.\")\n",
    "        except SQLAlchemyError as e:\n",
    "            # Rollback the transaction on error\n",
    "            session.rollback()\n",
    "            print(f\"Transaction error: {str(e)}\")\n",
    "        finally:\n",
    "            # Close the session\n",
    "            session.close()\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error connecting to the database: {str(e)}\")\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "def stemming(comment):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    do = []\n",
    "    for w in comment:\n",
    "        dt = stemmer.stem(w)\n",
    "        do.append(dt)\n",
    "    d_clean = []\n",
    "    d_clean = \" \".join(do)\n",
    "    return d_clean\n",
    "    \n",
    "# function case folding\n",
    "def casefolding(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = comment.strip(\" \")\n",
    "    comment = re.sub(r'[?|$|.|!_:\")(-+,]','',comment)\n",
    "    return comment\n",
    "\n",
    "def clean_up_tag(comment):\n",
    "    x_ret = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",comment).split())\n",
    "    return x_ret\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    \n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    \n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    \n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    \n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    return strOut\n",
    "\n",
    "print('functions is build')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7bea6f-b4e1-40b1-aea0-b78dab6af8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, MetaData, Table\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy.util import deprecations\n",
    "deprecations.SILENCE_UBER_WARNING = True\n",
    "\n",
    "def execute_query_psql(query, params=None):\n",
    "    # Set your PostgreSQL connection parameters\n",
    "    db_params = {\n",
    "        'host': '98.98.117.105',\n",
    "        'port': '5432',\n",
    "        'database': 'medols',\n",
    "        'user': 'postgres',\n",
    "        'password': 'FEWcTB3JIX5gK4T06c1MdkM9N2S8w9pb',\n",
    "    }\n",
    "\n",
    "    # Create a SQLAlchemy engine\n",
    "    engine = create_engine(f\"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}\")\n",
    "\n",
    "    # Create a metadata object\n",
    "    metadata = MetaData()\n",
    "\n",
    "    # Example: Define your table\n",
    "    your_table = Table('your_table', metadata,\n",
    "                       Column('id', Integer, primary_key=True),\n",
    "                       Column('column1', String),\n",
    "                       Column('column2', String)\n",
    "                       )\n",
    "\n",
    "    # Create a session\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    try:\n",
    "        # Execute the query with optional parameters\n",
    "        result = session.execute(text(query), params)\n",
    "\n",
    "        # Check if the query is a SELECT query\n",
    "        is_select_query = result.returns_rows\n",
    "\n",
    "        if is_select_query:\n",
    "            # Fetch the data and return as a Pandas DataFrame\n",
    "            columns = result.keys()\n",
    "            fetched_data = result.fetchall()\n",
    "            df = pd.DataFrame(fetched_data, columns=columns)\n",
    "            # print(\"Fetched Data as DataFrame:\")\n",
    "            # print(df)\n",
    "            return df\n",
    "        else:\n",
    "            # Get the number of rows affected for non-SELECT queries\n",
    "            rows_affected = result.rowcount\n",
    "\n",
    "            # Commit the changes to the database for non-SELECT queries\n",
    "            session.commit()\n",
    "\n",
    "            print(f\"Query executed successfully. {rows_affected} rows affected.\")\n",
    "            return rows_affected\n",
    "    except Exception as e:\n",
    "        # Rollback changes if there's an error\n",
    "        session.rollback()\n",
    "        print(f\"Error executing query: {e}\")\n",
    "    finally:\n",
    "        # Close the session\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71fce38-f1f6-48dd-bf4b-2ddfb3a8d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query from db\n",
    "query = \"select id, hastag, parameter \\\n",
    "from screen_analisis_ai \\\n",
    "where active = 1 \\\n",
    "and status = 1 \\\n",
    "and jenis_analisa = '3' \\\n",
    "order by created asc, id asc limit 1\"\n",
    "df_res = execute_query_psql(query)\n",
    "\n",
    "# check job availability\n",
    "if len(df_res) == 0:\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "    \n",
    "df_res.head()\n",
    "iJobID = df_res['hastag'][0]\n",
    "const_job_id = df_res['id'][0]\n",
    "const_parameter = df_res['parameter'][0]\n",
    "\n",
    "print('iJobID: ' + str(iJobID))\n",
    "print('const_job_id: ' + str(const_job_id))\n",
    "print('const_parameter: ' + str(const_parameter))\n",
    "# record as processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb428ac-9a42-4d70-9444-5694ea7d3593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_by_db_id(db_id):\n",
    "    from tqdm import tqdm\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    \n",
    "    # Example usage:\n",
    "    query = \"select * from ret_tweet where db_id = '\" + str(db_id) + \"'\"\n",
    "    df = execute_query_psql(query)\n",
    "    \n",
    "    df.head()\n",
    "    \n",
    "    # remove first\n",
    "    df['tweet'] = df['tweet'].apply(clean_up_tag)\n",
    "    \n",
    "    # skip stemming\n",
    "    df['stemmed'] = df['tweet']\n",
    "    df['stemmed'] = df['stemmed'].apply(casefolding)\n",
    "    df['stemmed'] = df['stemmed'].apply(text_preproc)\n",
    "    \n",
    "    sw = stopwords.words('indonesian')\n",
    "    \n",
    "    #tokenized\n",
    "    df['tokenized_tweet'] = df.apply(lambda row: nltk.word_tokenize(row['stemmed']), axis=1)\n",
    "    \n",
    "    # apply stopword removal\n",
    "    df['tokenized_tweet'] = df.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "    \n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "    \n",
    "    # Create a new list and insert each element from the original list\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    list_text = df['stemmed'].tolist()\n",
    "    new_list = []\n",
    "    #for i in tqdm(range(10)):\n",
    "    for i in tqdm(range( len(list_text) )):\n",
    "        # print(list_text[i])\n",
    "        res = pipe(list_text[i])\n",
    "        # new_list.append({res[0]['label']})\n",
    "        # new_list.append(new_list.append({res[0]['label']},{res[0]['score']})\n",
    "        new_list.append(res)\n",
    "    \n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "    # create a list of our conditions\n",
    "    for i in range(0, len(new_list)):\n",
    "        # print(new_list[i][0]['label'])\n",
    "        if new_list[i][0]['label'] == 'Positive':\n",
    "            new_list[i][0]['Value'] = 1\n",
    "        elif new_list[i][0]['label'] == 'Negative':\n",
    "            new_list[i][0]['Value'] = -1\n",
    "        elif new_list[i][0]['label'] == 'Neutral':\n",
    "            new_list[i][0]['Value'] = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        df.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        df.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "        df.at[index,'Score'] = new_list[index][0]['score']\n",
    "        df.at[index,'Score'] = new_list[index][0]['Value']\n",
    "        \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e0f48-fdf4-4a58-a969-29a46a604c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## report to dbms that we are working on this row\n",
    "sql = \"\"\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = '\" + str(const_job_id) + \"';\"\n",
    "print(sql)\n",
    "# execute\n",
    "execute_query_psql(sql)\n",
    "#\n",
    "# Create Header Record\n",
    "sql = \"insert into ret_analysis_header (job_id, datetime_start, user_id) values (%s, now(), %s)\" % (const_job_id,\"1\")\n",
    "# Execute the query\n",
    "execute_query_psql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169547e-795a-4ccf-bc0a-b3dc1a583f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "df_result = process_by_db_id(iJobID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6797f-cffb-461f-8df8-6b827a4ee6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "#df.to_csv('sa-result.csv', index=False)\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9034f-2ac7-414e-adda-2303ed3638d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record result\n",
    "sentiment_class = 0\n",
    "\n",
    "for index, row in df_result.iterrows():\n",
    "    if row['Prediction'] == 'Positive':\n",
    "        sentiment_class = 1\n",
    "    elif row['Prediction'] == 'Negative':\n",
    "        sentiment_class = -1\n",
    "    else:\n",
    "        sentiment_class = 0\n",
    "        \n",
    "    sql = \"INSERT INTO ret_sentiment_result (job_id, tweet_id, sentiment_class) VALUES(%s, %s , %s);\" % (str(const_job_id), row['id_str'], str(sentiment_class))\n",
    "    #print(sql)\n",
    "    execute_query_psql(sql)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d06ee-0a42-4d83-8b79-41b581f010d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record process finish\n",
    "# closing .... report back job status into rdbms\n",
    "sql = \"update screen_analisis_ai set end_process = now(), status = 3, processby_id = 1 where id = %s\" % (str(const_job_id))\n",
    "execute_query_psql(sql)\n",
    "\n",
    "sql = \"update screen_analisis_ai set duration = EXTRACT(EPOCH FROM (end_process - start_process)) where id = \" + str(const_job_id)\n",
    "execute_query_psql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec4220-d840-4788-bd30-61020ef3f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait 10 seconds before finished\n",
    "import time\n",
    "time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
