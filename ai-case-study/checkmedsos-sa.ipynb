{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12060b76-15c3-48d2-9120-29c8826ee849",
   "metadata": {},
   "source": [
    "# Huggingface Model for Sentiment Analysis\n",
    "## Source Data: checkmedsos_db, db_id = 87247\n",
    "\n",
    "1. create calculation example\n",
    "2. post to mysql\n",
    "3. inspect the result\n",
    "\n",
    " * Kesepakatan status di kolom screen_analisis_ai.status\n",
    " * 1 --> baru diinput\n",
    " * 2 --> lagi dikerjakan\n",
    " * 3 --> proses berhasil\n",
    " * 4 --> proses gagal\n",
    " *\n",
    " \n",
    " * Kesepakatan jenis analisa AI\n",
    " * 1 --> Analisa Cluster\n",
    " * 2 --> Analisa image clustering\n",
    " * 3 --> Analisa sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1057ca66-229f-4278-a165-ab16afd85dac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pymysql.cursors\n",
    "import sqlalchemy as sa\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa\")\n",
    "\n",
    "# database connection properties\n",
    "connection_url = \"mysql://cekmedsos_db:282E~f0si@202.157.185.40/cekmedsos_database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acbb152b-5d2d-4836-9138-7b4f6710a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions is build\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def execute_sqlalchemy_transaction(transaction_query):\n",
    "    # Database connection parameters\n",
    "    db_url = 'mysql://cekmedsos_db:282E~f0si@202.157.185.40/cekmedsos_database'\n",
    "        # Define the connection parameters inside the function\n",
    "    \n",
    "    try:\n",
    "        # Create a SQLAlchemy engine and session\n",
    "        engine = create_engine(db_url)\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "\n",
    "        # Begin a transaction\n",
    "        session.begin()\n",
    "\n",
    "        try:\n",
    "            # Execute the transaction query\n",
    "            query_text = text(transaction_query)\n",
    "            session.execute(query_text)\n",
    "\n",
    "            # Commit the transaction if the query succeeded\n",
    "            session.commit()\n",
    "            #print(\"Transaction completed successfully.\")\n",
    "        except SQLAlchemyError as e:\n",
    "            # Rollback the transaction on error\n",
    "            session.rollback()\n",
    "            print(f\"Transaction error: {str(e)}\")\n",
    "        finally:\n",
    "            # Close the session\n",
    "            session.close()\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error connecting to the database: {str(e)}\")\n",
    "\n",
    "def execute_mysql_query(query, connection_url):\n",
    "    try:\n",
    "        # Create a SQLAlchemy engine using the provided connection URL\n",
    "        engine = create_engine(connection_url)\n",
    "\n",
    "        # Execute the SQL query and fetch the results into a DataFrame\n",
    "        result_df = pd.read_sql(query, engine)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "def stemming(comment):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    do = []\n",
    "    for w in comment:\n",
    "        dt = stemmer.stem(w)\n",
    "        do.append(dt)\n",
    "    d_clean = []\n",
    "    d_clean = \" \".join(do)\n",
    "    return d_clean\n",
    "    \n",
    "# function case folding\n",
    "def casefolding(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = comment.strip(\" \")\n",
    "    comment = re.sub(r'[?|$|.|!_:\")(-+,]','',comment)\n",
    "    return comment\n",
    "\n",
    "def clean_up_tag(comment):\n",
    "    x_ret = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",comment).split())\n",
    "    return x_ret\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    \n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    \n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    \n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    \n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    return strOut\n",
    "\n",
    "print('functions is build')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da53f626-9323-41a8-b975-436cf6084cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, MetaData, Table\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "def execute_query(query):\n",
    "    # Connection properties\n",
    "    db_url = 'mysql+mysqlconnector://cekmedsos_db:282E~f0si@202.157.185.40/cekmedsos_database'\n",
    "\n",
    "    # Creating an engine\n",
    "    engine = create_engine(db_url, echo=False)\n",
    "\n",
    "    # Define a metadata object\n",
    "    metadata = MetaData()\n",
    "\n",
    "    # Create a session\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    try:\n",
    "        # Execute the query\n",
    "        result = session.execute(query)\n",
    "\n",
    "        # Commit changes for INSERT, UPDATE, DELETE queries\n",
    "        if query.strip().lower().startswith((\"insert\", \"update\", \"delete\")):\n",
    "            session.commit()\n",
    "\n",
    "        # Fetch data for SELECT queries\n",
    "        if query.strip().lower().startswith(\"select\"):\n",
    "            # Convert the result to a Pandas DataFrame\n",
    "            df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "            return df\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        session.rollback()\n",
    "    finally:\n",
    "        # Close the session\n",
    "        session.close()\n",
    "\n",
    "# Example usage:\n",
    "# query = \"SELECT * FROM your_table\"\n",
    "# result_df = execute_query(query)\n",
    "# print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a71fce38-f1f6-48dd-bf4b-2ddfb3a8d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iJobID: 86903\n",
      "const_job_id: 2963\n",
      "const_parameter: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haviz/cekmedsos/lib64/python3.6/site-packages/ipykernel_launcher.py:22: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n"
     ]
    }
   ],
   "source": [
    "# query from db\n",
    "query = \"select id, hastag, `parameter` \\\n",
    "from screen_analisis_ai \\\n",
    "where active = 1 \\\n",
    "and status = 1 \\\n",
    "and jenis_analisa = 3 \\\n",
    "order by created asc, id asc limit 1\"\n",
    "df_res = execute_query(query)\n",
    "\n",
    "# check job availability\n",
    "if len(df_res) == 0:\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "    \n",
    "df_res.head()\n",
    "iJobID = df_res['hastag'][0]\n",
    "const_job_id = df_res['id'][0]\n",
    "const_parameter = df_res['parameter'][0]\n",
    "\n",
    "print('iJobID: ' + str(iJobID))\n",
    "print('const_job_id: ' + str(const_job_id))\n",
    "print('const_parameter: ' + str(const_parameter))\n",
    "# record as processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb428ac-9a42-4d70-9444-5694ea7d3593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_by_db_id(db_id):\n",
    "    from tqdm import tqdm\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    \n",
    "    # Example usage:\n",
    "    query = \"select * from ret_tweet where db_id = '\" + str(db_id) + \"'\"\n",
    "    df = execute_query(query)\n",
    "    \n",
    "    df.head()\n",
    "    \n",
    "    # remove first\n",
    "    df['tweet'] = df['tweet'].apply(clean_up_tag)\n",
    "    \n",
    "    # skip stemming\n",
    "    df['stemmed'] = df['tweet']\n",
    "    df['stemmed'] = df['stemmed'].apply(casefolding)\n",
    "    df['stemmed'] = df['stemmed'].apply(text_preproc)\n",
    "    \n",
    "    sw = stopwords.words('indonesian')\n",
    "    \n",
    "    #tokenized\n",
    "    df['tokenized_tweet'] = df.apply(lambda row: nltk.word_tokenize(row['stemmed']), axis=1)\n",
    "    \n",
    "    # apply stopword removal\n",
    "    df['tokenized_tweet'] = df.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "    \n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "    \n",
    "    # Create a new list and insert each element from the original list\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    list_text = df['stemmed'].tolist()\n",
    "    new_list = []\n",
    "    #for i in tqdm(range(10)):\n",
    "    for i in tqdm(range( len(list_text) )):\n",
    "        # print(list_text[i])\n",
    "        res = pipe(list_text[i])\n",
    "        #new_list.append({res[0]['label']})\n",
    "        # new_list.append(new_list.append({res[0]['label']},{res[0]['score']})\n",
    "        new_list.append(res)\n",
    "    \n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "    # create a list of our conditions\n",
    "    for i in range(0, len(new_list)):\n",
    "        # print(new_list[i][0]['label'])\n",
    "        if new_list[i][0]['label'] == 'Positive':\n",
    "            new_list[i][0]['Value'] = 1\n",
    "        elif new_list[i][0]['label'] == 'Negative':\n",
    "            new_list[i][0]['Value'] = -1\n",
    "        elif new_list[i][0]['label'] == 'Neutral':\n",
    "            new_list[i][0]['Value'] = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        df.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        df.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "        df.at[index,'Score'] = new_list[index][0]['score']\n",
    "        df.at[index,'Score'] = new_list[index][0]['Value']\n",
    "        \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3e0f48-fdf4-4a58-a969-29a46a604c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = '2963';\n"
     ]
    }
   ],
   "source": [
    "## report to dbms that we are working on this row\n",
    "sql = \"\"\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = '\" + str(const_job_id) + \"';\"\n",
    "print(sql)\n",
    "# execute\n",
    "execute_query(sql)\n",
    "#\n",
    "# Create Header Record\n",
    "sql = \"insert into ret_analysis_header (job_id, datetime_start, user_id) values (%s, now(), %s)\" % (const_job_id,\"1\")\n",
    "# Execute the query\n",
    "execute_query(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f169547e-795a-4ccf-bc0a-b3dc1a583f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2009/2009 [01:34<00:00, 21.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "df_result = process_by_db_id(iJobID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07b6797f-cffb-461f-8df8-6b827a4ee6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "#df.to_csv('sa-result.csv', index=False)\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7c9034f-2ac7-414e-adda-2303ed3638d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record result\n",
    "sentiment_class = 0\n",
    "\n",
    "for index, row in df_result.iterrows():\n",
    "    if row['Prediction'] == 'Positive':\n",
    "        sentiment_class = 1\n",
    "    elif row['Prediction'] == 'Negative':\n",
    "        sentiment_class = -1\n",
    "    else:\n",
    "        sentiment_class = 0\n",
    "        \n",
    "    sql = \"INSERT INTO cekmedsos_database.ret_sentiment_result (job_id, tweet_id, sentiment_class) VALUES(%s, %s , %s);\" % (str(const_job_id), row['id_str'], str(sentiment_class))\n",
    "    #print(sql)\n",
    "    execute_query(sql)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "262d06ee-0a42-4d83-8b79-41b581f010d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record process finish\n",
    "# closing .... report back job status into rdbms\n",
    "sql = \"update screen_analisis_ai set end_process = now(), status = 3, processby_id = 1 where id = %s\" % (str(const_job_id))\n",
    "execute_query(sql)\n",
    "\n",
    "sql = \"update screen_analisis_ai set duration = TIMEDIFF(end_process, start_process) where id = \" + str(const_job_id)\n",
    "execute_query(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfec4220-d840-4788-bd30-61020ef3f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait 10 seconds before finished\n",
    "import time\n",
    "time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
