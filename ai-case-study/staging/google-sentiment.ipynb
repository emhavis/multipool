{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc88b042-9d24-48fc-94a3-6009b4951e7b",
   "metadata": {},
   "source": [
    "# Multipool Google Sentiment Analysis\n",
    "\n",
    "this function will get data from google_result table\n",
    "and calculate the sentiment analysis. Result is store in google_result_sentiment\n",
    "\n",
    "crontab command\n",
    "* * * * * /home/haviz/multipool/ai-case-study/run-ai-sa-google.sh >> /home/haviz/multipool/ai-sa-google.log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d0afb3-cd43-479c-a645-5eb16d589ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 13:05:22.078846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-10 13:05:22.096995: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-10 13:05:22.102621: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-10 13:05:22.114962: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-10 13:05:22.939229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pymysql.cursors\n",
    "import sqlalchemy as sa\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from sqlalchemy import create_engine, Column, Integer, String, MetaData, Table\n",
    "from sqlalchemy.sql import text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(device)\n",
    "\n",
    "# Initialize the pipeline with the dynamic device setting\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "pipe = pipeline(\"text-classification\", model=\"ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec64d1af-b458-4c9e-9762-1d3d780b616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define maximum allowed connections\n",
    "MAX_CONNECTIONS = 10\n",
    "\n",
    "# Track the number of active connections\n",
    "active_connections = 0\n",
    "\n",
    "def execute_query(query, params=None):\n",
    "    global active_connections\n",
    "\n",
    "    # Ensure active connections do not exceed the maximum limit\n",
    "    if active_connections >= MAX_CONNECTIONS:\n",
    "        print(\"Maximum connection limit reached. Cannot execute query.\")\n",
    "        return None\n",
    "\n",
    "    # Set your PostgreSQL connection parameters\n",
    "    db_params = {\n",
    "        'host': '98.98.117.105',\n",
    "        'port': '5432',\n",
    "        'database': 'medols',\n",
    "        'user': 'postgres',\n",
    "        'password': 'FEWcTB3JIX5gK4T06c1MdkM9N2S8w9pb',\n",
    "    }\n",
    "\n",
    "    # Create a SQLAlchemy engine\n",
    "    engine = create_engine(f\"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}\")\n",
    "\n",
    "    # Create a session\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    try:\n",
    "        # Increment active connections count\n",
    "        active_connections += 1\n",
    "\n",
    "        # Execute the query with optional parameters\n",
    "        result = session.execute(text(query), params)\n",
    "\n",
    "        # Check if the query is a SELECT query\n",
    "        is_select_query = result.returns_rows\n",
    "\n",
    "        if is_select_query:\n",
    "            # Fetch the data and return as a Pandas DataFrame\n",
    "            columns = result.keys()\n",
    "            fetched_data = result.fetchall()\n",
    "            df = pd.DataFrame(fetched_data, columns=columns)\n",
    "            return df\n",
    "        else:\n",
    "            # Get the number of rows affected for non-SELECT queries\n",
    "            rows_affected = result.rowcount\n",
    "\n",
    "            # Commit the changes to the database for non-SELECT queries\n",
    "            session.commit()\n",
    "            return rows_affected\n",
    "    except SQLAlchemyError as e:\n",
    "        # Rollback changes if there's an error\n",
    "        session.rollback()\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Decrement active connections count\n",
    "        active_connections -= 1\n",
    "\n",
    "        # Close the session\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97cf0378-4165-46bd-abc9-05c3a9fe18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def remove_mentions(tweet):\n",
    "    # Define regular expression pattern to match mentions\n",
    "    pattern = r'@\\w+'\n",
    "    \n",
    "    # Replace mentions with an empty string\n",
    "    cleaned_tweet = re.sub(pattern, '', tweet)\n",
    "    \n",
    "    return cleaned_tweet\n",
    "\n",
    "def stemming(comment):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    do = []\n",
    "    for w in comment:\n",
    "        dt = stemmer.stem(w)\n",
    "        do.append(dt)\n",
    "    d_clean = []\n",
    "    d_clean = \" \".join(do)\n",
    "    return d_clean\n",
    "    \n",
    "# function case folding\n",
    "def casefolding(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = comment.strip(\" \")\n",
    "    comment = re.sub(r'[?|$|.|!_:\")(-+,]','',comment)\n",
    "    return comment\n",
    "\n",
    "def clean_up_tag(comment):\n",
    "    x_ret = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",comment).split())\n",
    "    return x_ret\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    \n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    \n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    \n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    \n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    return strOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438a292a-3e97-4161-bcaf-1887c838ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_by_df(source_ds):\n",
    "    from tqdm import tqdm\n",
    "    pd.options.mode.chained_assignment = None \n",
    "\n",
    "    # we calculate sentiment for 'title' and 'description' column\n",
    "\n",
    "    # remove first\n",
    "    source_ds['title'] = source_ds['title'].apply(clean_up_tag)\n",
    "    source_ds['description'] = source_ds['description'].apply(clean_up_tag)\n",
    "\n",
    "\n",
    "    # skip stemming\n",
    "    source_ds['stemmed'] = source_ds['title']\n",
    "    source_ds['stemmed'] = source_ds['stemmed'].apply(casefolding)\n",
    "    source_ds['stemmed'] = source_ds['stemmed'].apply(text_preproc)\n",
    "\n",
    "    sw = stopwords.words('indonesian')\n",
    "    # tokenized\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: nltk.word_tokenize(row['stemmed']), axis=1)\n",
    "\n",
    "    # apply stopword removal\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "\n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "\n",
    "    # Create a new list and insert each element from the original list\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    list_text = source_ds['stemmed'].tolist()\n",
    "    new_list = []\n",
    "    for i in tqdm(range( len(list_text) )):\n",
    "        res = pipe(list_text[i])\n",
    "        #print(res)\n",
    "        new_list.append(res)\n",
    "\n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "    # create a list of our conditions\n",
    "    for i in range(0, len(new_list)):\n",
    "        # print(new_list[i][0]['label'])\n",
    "        if new_list[i][0]['label'] == 'Positive':\n",
    "            new_list[i][0]['Value'] = 1\n",
    "        elif new_list[i][0]['label'] == 'Negative':\n",
    "            new_list[i][0]['Value'] = -1\n",
    "        elif new_list[i][0]['label'] == 'Neutral':\n",
    "            new_list[i][0]['Value'] = 0\n",
    "\n",
    "    for index, row in source_ds.iterrows():\n",
    "        source_ds.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "\n",
    "    for index, row in source_ds.iterrows():\n",
    "        source_ds.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "        source_ds.at[index,'Score'] = new_list[index][0]['score']\n",
    "        #source_ds.at[index,'Score'] = new_list[index][0]['Value']\n",
    "        \n",
    "    return source_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fa271b-38b8-4b07-bbf8-a47a457be5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging to a file\n",
    "import logging\n",
    "logging.basicConfig(filename='error.log', level=logging.ERROR)\n",
    "\n",
    "def calculate_sentiment(row, col_description):\n",
    "    try:\n",
    "        # cleansing\n",
    "        strSource = row[col_description]\n",
    "\n",
    "        # cleansing\n",
    "        strSource = clean_up_tag(strSource)\n",
    "        strSource = casefolding(strSource)\n",
    "        strSource = text_preproc(strSource)\n",
    "        strSource = remove_mentions(strSource)\n",
    "\n",
    "        # tokenized\n",
    "        sToken = nltk.word_tokenize(strSource)\n",
    "\n",
    "        # remove stopwords\n",
    "        sw = stopwords.words('indonesian')\n",
    "\n",
    "        filtered_words = [word for word in sToken if word.lower() not in sw]\n",
    "        filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "        # Notes on return sentiment values\n",
    "        # 1 >> positif >> stay\n",
    "        # 2 >> negatif >> convert to -1\n",
    "        # 0 >> netral >> stay\n",
    "        result = pipe(strSource)\n",
    "\n",
    "        if result[0]['label'] == 'Negative':\n",
    "            sentimentClass = -1\n",
    "        elif result[0]['label'] == 'Positive':\n",
    "            sentimentClass = 1\n",
    "        else:\n",
    "            sentimentClass = 0\n",
    "\n",
    "        score = result[0]['score']\n",
    "\n",
    "        # put to result    \n",
    "        return pd.Series({'sentiment_category': sentimentClass, 'score': score})\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Log the exception to the error.log file\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        # Return default values or None for the columns\n",
    "        return pd.Series({'sentiment_category': None, 'score': None})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325b5528-6ca4-411d-9b57-069e3a6d3a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# calculate sentiment result for google_result\n",
    "# \n",
    "\n",
    "def record_google_sentiment(row):\n",
    "    ssql = \"\"\"\n",
    "    INSERT INTO public.google_result_sentiment \n",
    "    (id, sentiment_category, score, google_result_id) \n",
    "    VALUES(nextval('google_result_sentiment_id_seq'::regclass), %s, %s, %s);\n",
    "    \"\"\" %(row['sentiment_category'],row['score'],row['google_result_id'])\n",
    "    #\n",
    "    # print(ssql)\n",
    "    execute_query(ssql)\n",
    "\n",
    "select_query = \"\"\"\n",
    "select *\n",
    "from google_result gr\n",
    "where google_result_id not in (select google_result_id from google_result_sentiment)\n",
    "order by google_result_id desc limit 100\n",
    "\"\"\"\n",
    "\n",
    "source_ds2 = execute_query(select_query)\n",
    "\n",
    "if len(source_ds2) != 0:\n",
    "    source_ds2[['sentiment_category','score']] = source_ds2.apply(calculate_sentiment, args=('title',), axis=1)\n",
    "    source_ds2.apply(record_google_sentiment, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88782dd5-abab-4231-8933-5916d470aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import psycopg2\n",
    "\n",
    "def record_instagram_sentiment_batch(records, connection):\n",
    "    ssql = \"\"\"\n",
    "    INSERT INTO instagram_post_sentiment (sentiment_category, score, instagram_post_id)\n",
    "    VALUES %s\n",
    "    ON CONFLICT (instagram_post_id)\n",
    "    DO UPDATE SET\n",
    "    sentiment_category = EXCLUDED.sentiment_category,\n",
    "    score = EXCLUDED.score\n",
    "    \"\"\"\n",
    "    \n",
    "    with connection.cursor() as cursor:\n",
    "        values_str = ','.join(cursor.mogrify(\"(%s,%s,%s)\", (record['sentiment_category'], record['score'], record['id'])).decode('utf-8') for record in records)\n",
    "        cursor.execute(ssql % values_str)\n",
    "    connection.commit()\n",
    "\n",
    "def execute_query(query, connection):\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        return [dict(zip(columns, row)) for row in result]\n",
    "\n",
    "def process_sentiments(batch_size):\n",
    "    # Database connection\n",
    "    connection = psycopg2.connect(\n",
    "        host='98.98.117.105',\n",
    "        user='postgres',\n",
    "        password='FEWcTB3JIX5gK4T06c1MdkM9N2S8w9pb',\n",
    "        database='medols'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        select_query = \"\"\"\n",
    "        SELECT t.id, t.\"content\" \n",
    "        FROM instagram_post t\n",
    "        LEFT JOIN instagram_post_sentiment r ON t.id = r.instagram_post_id \n",
    "        WHERE r.instagram_post_id IS NULL\n",
    "        ORDER BY t.id DESC LIMIT 10000  -- Adjust limit as needed\n",
    "        \"\"\"\n",
    "        \n",
    "        source_ds2 = execute_query(select_query, connection)\n",
    "        \n",
    "        if len(source_ds2) != 0:\n",
    "            for record in tqdm(source_ds2, desc=\"Processing sentiment analysis\"):\n",
    "                sentiment_category, score = calculate_sentiment(record['content'],\"content\")\n",
    "                record['sentiment_category'] = sentiment_category\n",
    "                record['score'] = score\n",
    "                \n",
    "            # Remove duplicates within the batch\n",
    "            unique_records = {record['id']: record for record in source_ds2}.values()\n",
    "\n",
    "            # Process in batches\n",
    "            unique_records = list(unique_records)\n",
    "            for i in range(0, len(unique_records), batch_size):\n",
    "                batch = unique_records[i:i + batch_size]\n",
    "                record_instagram_sentiment_batch(batch, connection)\n",
    "            \n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Process sentiments with the specified batch size\n",
    "process_sentiments(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8430d14-1179-4e4d-af02-9a83e918bab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentiment analysis: 100%|█████████████████████████████████| 50000/50000 [00:13<00:00, 3735.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Calculating twitter tweets sentiment result\n",
    "#\n",
    "\n",
    "from tqdm import tqdm\n",
    "import psycopg2\n",
    "\n",
    "def record_instagram_sentiment_batch(records, connection):\n",
    "    ssql = \"\"\"\n",
    "    INSERT INTO twitter_tweets_result_sentiment (sentiment_category, score, twitter_tweets_id)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "    \n",
    "    with connection.cursor() as cursor:\n",
    "        values_str = ','.join(cursor.mogrify(\"(%s,%s,%s)\", (record['sentiment_category'], record['score'], record['id'])).decode('utf-8') for record in records)\n",
    "        cursor.execute(ssql % values_str)\n",
    "    connection.commit()\n",
    "\n",
    "def execute_query(query, connection):\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        return [dict(zip(columns, row)) for row in result]\n",
    "\n",
    "def process_sentiments(batch_size):\n",
    "    # Database connection\n",
    "    connection = psycopg2.connect(\n",
    "        host='98.98.117.105',\n",
    "        user='postgres',\n",
    "        password='FEWcTB3JIX5gK4T06c1MdkM9N2S8w9pb',\n",
    "        database='medols'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        select_query = \"\"\"\n",
    "        SELECT t.id, t.tweet as content\n",
    "        FROM twitter_tweets t\n",
    "        LEFT JOIN twitter_tweets_result_sentiment r ON t.id = r.twitter_tweets_id\n",
    "        WHERE r.twitter_tweets_id IS NULL\n",
    "        ORDER BY t.id desc limit 50000\n",
    "        \"\"\"\n",
    "        \n",
    "        source_ds2 = execute_query(select_query, connection)\n",
    "        \n",
    "        if len(source_ds2) != 0:\n",
    "            for record in tqdm(source_ds2, desc=\"Processing sentiment analysis\"):\n",
    "                sentiment_category, score = calculate_sentiment(record['content'],\"content\")\n",
    "                record['sentiment_category'] = sentiment_category\n",
    "                record['score'] = score\n",
    "                \n",
    "            # Remove duplicates within the batch\n",
    "            unique_records = {record['id']: record for record in source_ds2}.values()\n",
    "\n",
    "            # Process in batches\n",
    "            unique_records = list(unique_records)\n",
    "            for i in range(0, len(unique_records), batch_size):\n",
    "                batch = unique_records[i:i + batch_size]\n",
    "                record_instagram_sentiment_batch(batch, connection)\n",
    "            \n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Process sentiments with the specified batch size\n",
    "process_sentiments(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79230a32-eb1f-443d-9fd0-890f1560da73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
