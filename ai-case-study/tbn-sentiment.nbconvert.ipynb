{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc88b042-9d24-48fc-94a3-6009b4951e7b",
   "metadata": {},
   "source": [
    "# Multipool Tribrata News and Instagram Post Sentiment Analysis\n",
    "\n",
    "this function will get data from google_result table\n",
    "and calculate the sentiment analysis. Result is store in google_result_sentiment\n",
    "\n",
    "crontab command\n",
    "* * * * * /home/haviz/multipool/ai-case-study/run-ai-sa-tbn.sh >> /home/haviz/multipool/ai-sa-tbn.log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d0afb3-cd43-479c-a645-5eb16d589ba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T14:18:03.602743Z",
     "iopub.status.busy": "2024-06-19T14:18:03.602487Z",
     "iopub.status.idle": "2024-06-19T14:18:08.668153Z",
     "shell.execute_reply": "2024-06-19T14:18:08.667433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pymysql.cursors\n",
    "import sqlalchemy as sa\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from sqlalchemy import create_engine, Column, Integer, String, MetaData, Table\n",
    "from sqlalchemy.sql import text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU is available')\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print('GPU is NOT available')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"ayameRushia/bert-base-indonesian-1.5G-sentiment-analysis-smsa\", batch_size=8, device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec64d1af-b458-4c9e-9762-1d3d780b616f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T14:18:08.671551Z",
     "iopub.status.busy": "2024-06-19T14:18:08.670849Z",
     "iopub.status.idle": "2024-06-19T14:18:08.677494Z",
     "shell.execute_reply": "2024-06-19T14:18:08.676848Z"
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.util import deprecations\n",
    "deprecations.SILENCE_UBER_WARNING = True\n",
    "\n",
    "def execute_query(query, params=None):\n",
    "    # Set your PostgreSQL connection parameters\n",
    "    db_params = {\n",
    "        'host': '98.98.117.105',\n",
    "        'port': '5432',\n",
    "        'database': 'medols',\n",
    "        'user': 'postgres',\n",
    "        'password': 'FEWcTB3JIX5gK4T06c1MdkM9N2S8w9pb',\n",
    "    }\n",
    "\n",
    "    # Create a SQLAlchemy engine\n",
    "    engine = create_engine(f\"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}\")\n",
    "\n",
    "    # Create a session\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    try:\n",
    "        # Execute the query with optional parameters\n",
    "        result = session.execute(text(query), params)\n",
    "\n",
    "        # Check if the query is a SELECT query\n",
    "        is_select_query = result.returns_rows\n",
    "\n",
    "        if is_select_query:\n",
    "            # Fetch the data and return as a Pandas DataFrame\n",
    "            columns = result.keys()\n",
    "            fetched_data = result.fetchall()\n",
    "            df = pd.DataFrame(fetched_data, columns=columns)\n",
    "            # print(\"Fetched Data as DataFrame:\")\n",
    "            # print(df)\n",
    "            return df\n",
    "        else:\n",
    "            # Get the number of rows affected for non-SELECT queries\n",
    "            rows_affected = result.rowcount\n",
    "\n",
    "            # Commit the changes to the database for non-SELECT queries\n",
    "            session.commit()\n",
    "\n",
    "            # print(f\"Query executed successfully. {rows_affected} rows affected.\")\n",
    "            return rows_affected\n",
    "    except Exception as e:\n",
    "        # Rollback changes if there's an error\n",
    "        session.rollback()\n",
    "        print(f\"Error executing query: {e}\")\n",
    "    finally:\n",
    "        # Close the session\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97cf0378-4165-46bd-abc9-05c3a9fe18ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T14:18:08.680238Z",
     "iopub.status.busy": "2024-06-19T14:18:08.680045Z",
     "iopub.status.idle": "2024-06-19T14:18:08.685681Z",
     "shell.execute_reply": "2024-06-19T14:18:08.685031Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def stemming(comment):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    do = []\n",
    "    for w in comment:\n",
    "        dt = stemmer.stem(w)\n",
    "        do.append(dt)\n",
    "    d_clean = []\n",
    "    d_clean = \" \".join(do)\n",
    "    return d_clean\n",
    "    \n",
    "# function case folding\n",
    "def casefolding(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = comment.strip(\" \")\n",
    "    comment = re.sub(r'[?|$|.|!_:\")(-+,]','',comment)\n",
    "    return comment\n",
    "\n",
    "def clean_up_tag(comment):\n",
    "    x_ret = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",comment).split())\n",
    "    return x_ret\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    \n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    \n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    \n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    \n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    return strOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438a292a-3e97-4161-bcaf-1887c838ae1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T14:18:08.688678Z",
     "iopub.status.busy": "2024-06-19T14:18:08.688130Z",
     "iopub.status.idle": "2024-06-19T14:18:08.696493Z",
     "shell.execute_reply": "2024-06-19T14:18:08.695861Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_by_df(source_ds):\n",
    "    from tqdm import tqdm\n",
    "    pd.options.mode.chained_assignment = None \n",
    "\n",
    "    # we calculate sentiment for 'title' and 'description' column\n",
    "\n",
    "    # remove first\n",
    "    source_ds['title'] = source_ds['title'].apply(clean_up_tag)\n",
    "    source_ds['description'] = source_ds['content'].apply(clean_up_tag)\n",
    "\n",
    "\n",
    "    # skip stemming\n",
    "    source_ds['stemmed'] = source_ds['title']\n",
    "    source_ds['stemmed'] = source_ds['stemmed'].apply(casefolding)\n",
    "    source_ds['stemmed'] = source_ds['stemmed'].apply(text_preproc)\n",
    "\n",
    "    sw = stopwords.words('indonesian')\n",
    "    #tokenized\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: nltk.word_tokenize(row['stemmed']), axis=1)\n",
    "\n",
    "    # apply stopword removal\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "    sw = stopwords.words('indonesian')\n",
    "\n",
    "    #tokenized\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: nltk.word_tokenize(row['stemmed']), axis=1)\n",
    "\n",
    "    # apply stopword removal\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "\n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "\n",
    "    # Create a new list and insert each element from the original list\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    list_text = source_ds['stemmed'].tolist()\n",
    "    new_list = []\n",
    "    for i in tqdm(range( len(list_text) )):\n",
    "        res = pipe(list_text[i])\n",
    "        #print(res)\n",
    "        new_list.append(res)\n",
    "\n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "    # create a list of our conditions\n",
    "    for i in range(0, len(new_list)):\n",
    "        # print(new_list[i][0]['label'])\n",
    "        if new_list[i][0]['label'] == 'Positive':\n",
    "            new_list[i][0]['Value'] = 1\n",
    "        elif new_list[i][0]['label'] == 'Negative':\n",
    "            new_list[i][0]['Value'] = -1\n",
    "        elif new_list[i][0]['label'] == 'Neutral':\n",
    "            new_list[i][0]['Value'] = 0\n",
    "\n",
    "    for index, row in source_ds.iterrows():\n",
    "        source_ds.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "\n",
    "    for index, row in source_ds.iterrows():\n",
    "        source_ds.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "        source_ds.at[index,'Score'] = new_list[index][0]['score']\n",
    "        #source_ds.at[index,'Score'] = new_list[index][0]['Value']\n",
    "        \n",
    "    return source_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3853b4df-3823-4ce4-8b18-bfe24581c81a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T14:18:08.699416Z",
     "iopub.status.busy": "2024-06-19T14:18:08.699036Z",
     "iopub.status.idle": "2024-06-19T14:18:08.708082Z",
     "shell.execute_reply": "2024-06-19T14:18:08.707426Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_by_df_col(source_ds, col_name):\n",
    "    from tqdm import tqdm\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    \n",
    "    # trim to 2500 character\n",
    "    source_ds[col_name] = source_ds[col_name].str.slice(0,2000)\n",
    "\n",
    "    # skip stemming\n",
    "    source_ds['stemmed'] = source_ds[col_name].apply(clean_up_tag)\n",
    "    source_ds['stemmed'] = source_ds['stemmed'].apply(casefolding)\n",
    "    source_ds['stemmed'] = source_ds['stemmed'].apply(text_preproc)\n",
    "\n",
    "    sw = stopwords.words('indonesian')\n",
    "    #tokenized\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: nltk.word_tokenize(row['stemmed']), axis=1)\n",
    "\n",
    "    # apply stopword removal\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "    sw = stopwords.words('indonesian')\n",
    "\n",
    "    #tokenized\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: nltk.word_tokenize(row['stemmed']), axis=1)\n",
    "\n",
    "    # apply stopword removal\n",
    "    source_ds['tokenized_tweet'] = source_ds.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "\n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "\n",
    "    # Create a new list and insert each element from the original list\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    list_text = source_ds['stemmed'].tolist()\n",
    "    new_list = []\n",
    "    for i in tqdm(range( len(list_text) )):\n",
    "        # print(len(list_text[i]))\n",
    "        try:\n",
    "            if len(list_text[i]) > 0:\n",
    "                res = pipe( list_text[i] )\n",
    "                new_list.append(res)\n",
    "            else:\n",
    "                res = pipe( 'kalimat netral' )\n",
    "                new_list.append(res)\n",
    "                zip\n",
    "        except Exception as e:\n",
    "            # Handle the exception (error) here\n",
    "            print(f\"Error processing value at index {i}: {list_text[i]}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            res = pipe( 'kalimat netral' )\n",
    "            new_list.append(res)      \n",
    "            \n",
    "            continue\n",
    "\n",
    "    # Notes on return sentiment values\n",
    "    # 1 >> positif >> stay\n",
    "    # 2 >> negatif >> convert to -1\n",
    "    # 0 >> netral >> stay\n",
    "    # create a list of our conditions\n",
    "    for i in range(0, len(new_list)):\n",
    "        # print(new_list[i][0]['label'])\n",
    "        if new_list[i][0]['label'] == 'Positive':\n",
    "            new_list[i][0]['Value'] = 1\n",
    "        elif new_list[i][0]['label'] == 'Negative':\n",
    "            new_list[i][0]['Value'] = -1\n",
    "        elif new_list[i][0]['label'] == 'Neutral':\n",
    "            new_list[i][0]['Value'] = 0\n",
    "\n",
    "    for index, row in source_ds.iterrows():\n",
    "        source_ds.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "\n",
    "    for index, row in source_ds.iterrows():\n",
    "        source_ds.at[index,'Prediction'] = new_list[index][0]['label']\n",
    "        source_ds.at[index,'Score'] = new_list[index][0]['score']\n",
    "        #source_ds.at[index,'Score'] = new_list[index][0]['Value']\n",
    "        \n",
    "    return source_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325b5528-6ca4-411d-9b57-069e3a6d3a25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T14:18:08.710864Z",
     "iopub.status.busy": "2024-06-19T14:18:08.710535Z",
     "iopub.status.idle": "2024-06-19T14:18:08.831360Z",
     "shell.execute_reply": "2024-06-19T14:18:08.830755Z"
    }
   },
   "outputs": [],
   "source": [
    "# query 400 rows of data query data from table\n",
    "select_query = \"select * \\\n",
    "from news_tbn_medol gr \\\n",
    "where id not in (select id_tbn_news from news_tbn_sentiment) \\\n",
    "order by id desc limit 1000\"\n",
    "\n",
    "source_ds = execute_query(select_query)\n",
    "\n",
    "if len(source_ds) != 0:\n",
    "    # Processing title sentiment analysis\n",
    "    content_ds = source_ds\n",
    "    res_ds = process_by_df(source_ds)\n",
    "    # Processing content sentiment analysis\n",
    "    res_content_ds = process_by_df_col(content_ds,'content')\n",
    "\n",
    "    # record result\n",
    "    sentiment_class = 0\n",
    "    \n",
    "    for index, row in res_ds.iterrows():\n",
    "        if row['Prediction'] == 'Positive':\n",
    "            sentiment_class = 1\n",
    "        elif row['Prediction'] == 'Negative':\n",
    "            sentiment_class = -1\n",
    "        else:\n",
    "            sentiment_class = 0\n",
    "            \n",
    "        # sql = \"INSERT INTO cekmedsos_database.ret_sentiment_result (job_id, tweet_id, sentiment_class) VALUES(%s, %s , %s);\" % (str(const_job_id), row['id_str'], str(sentiment_class))\n",
    "        sql = \"INSERT INTO public.news_tbn_sentiment (id_tbn_news, sentiment_title, sentiment_title_value) VALUES( %s, %s, %s);\" % (row['id'], sentiment_class, row['Score'],) \n",
    "        #print(sql)\n",
    "        execute_query(sql)\n",
    "\n",
    "    # record result\n",
    "    sentiment_class = 0\n",
    "    \n",
    "    for index, row in res_ds.iterrows():\n",
    "        if row['Prediction'] == 'Positive':\n",
    "            sentiment_class = 1\n",
    "        elif row['Prediction'] == 'Negative':\n",
    "            sentiment_class = -1\n",
    "        else:\n",
    "            sentiment_class = 0\n",
    "            \n",
    "        sql = \"UPDATE public.news_tbn_sentiment set sentiment_content = %s, sentiment_content_value = %s where id_tbn_news = %s;\" % (sentiment_class, row['Score'], row['id'])\n",
    "        \n",
    "        execute_query(sql)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f378c4c8-f8d3-4729-9cc8-5454d0bb7d4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T14:18:08.834076Z",
     "iopub.status.busy": "2024-06-19T14:18:08.833759Z",
     "iopub.status.idle": "2024-06-19T14:18:08.968858Z",
     "shell.execute_reply": "2024-06-19T14:18:08.968158Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate instagram sentiments\n",
    "select_query = \"\"\"\n",
    "select * from instagram_post \n",
    "where id not in \n",
    "(select instagram_post_id from instagram_post_sentiment)\n",
    "limit 1000\n",
    "\"\"\"\n",
    "\n",
    "source_ds_ig = execute_query(select_query)\n",
    "if len(source_ds_ig) != 0:\n",
    "    # Processing content sentiment analysis\n",
    "    res_content_ds_ig = process_by_df_col(source_ds_ig,'content')\n",
    "\n",
    "    # record result\n",
    "    sentiment_class = 0\n",
    "    \n",
    "    for index, row in res_content_ds_ig.iterrows():\n",
    "        if row['Prediction'] == 'Positive':\n",
    "            sentiment_class = 1\n",
    "        elif row['Prediction'] == 'Negative':\n",
    "            sentiment_class = -1\n",
    "        else:\n",
    "            sentiment_class = 0\n",
    "            \n",
    "        sql = \"INSERT INTO public.instagram_post_sentiment (instagram_post_id, sentiment_category, score) VALUES( '%s', %s, %s);\" % (row['id'], sentiment_class, row['Score'],) \n",
    "        # print(sql)\n",
    "        execute_query(sql)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
