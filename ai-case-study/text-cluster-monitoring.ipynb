{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c7120d-e7e7-4b2b-96cb-12b79e7617eb",
   "metadata": {},
   "source": [
    "## Notebook for Text Cluster Analysis\n",
    "Queue from screen-analisis-ai where :\n",
    "- status = 1\n",
    "- jenis-analisa = 10\n",
    "- order by created desc, limit 1\n",
    "\n",
    "### Input as proces started\n",
    "Record header and parameter information\n",
    "\n",
    " * Kesepakatan status di kolom screen_analisis_ai.status\n",
    " * 1 --> baru diinput\n",
    " * 2 --> lagi dikerjakan\n",
    " * 3 --> proses berhasil\n",
    " * 4 --> proses gagal\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8865c8f-d9c3-4d40-a9c1-7833e9578b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from sqlalchemy import create_engine, Column, Integer, String, MetaData, Table\n",
    "from sqlalchemy.sql import text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pandas as pd\n",
    "\n",
    "# import custom data connector\n",
    "import data_connector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2575d46-fa9a-4df0-92fb-dc2a4fe155c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "3935\n",
      "eb784a56-6fcb-4252-b0b8-21402b8c286f\n",
      "115fd4ed-8608-4137-8170-e8d49238dece\n",
      "3935\n",
      "update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = 3935\n",
      "update 1 rows\n"
     ]
    }
   ],
   "source": [
    "strSQL = \"\"\"\n",
    "select \ta.id as jobsid,\n",
    "        a.*,\n",
    "\t\td.id as key_monitoring_media_social, \n",
    "\t\te.id as key_monitoring_media_online,\n",
    "\t\tc.*\n",
    "from screen_analisis_ai a inner join monitoring b\n",
    "\ton a.monitoring_id = cast(b.id as varchar)\n",
    "\tinner join monitoring_search c \n",
    "\t\ton b.id = c.monitoring_id \n",
    "\tinner join monitoring_media_social d\n",
    "\t\ton d.monitoring_id = c.monitoring_id\n",
    "\tleft outer join monitoring_media_online e\n",
    "\t\ton e.monitoring_id = c.monitoring_id \n",
    "where a.jenis_analisa = '10'\n",
    "and a.status = 1\n",
    "order by a.created desc \n",
    "limit 1\n",
    "\"\"\"\n",
    "\n",
    "df_job = data_connector.execute_query_psql(strSQL)\n",
    "if len(df_job) == 0:\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "    \n",
    "similarity_treshold = 0.9\n",
    "i_process_id = df_job['jobsid'][0]\n",
    "screen_name = ''\n",
    "database_keyword_id = df_job['key_monitoring_media_social'][0]\n",
    "social_media_monitoring_id = df_job['key_monitoring_media_social'][0]\n",
    "media_online_monitoring_id = df_job['key_monitoring_media_online'][0]\n",
    "\n",
    "# print(database_keyword_id)\n",
    "print(similarity_treshold)\n",
    "print(i_process_id)\n",
    "print(social_media_monitoring_id)\n",
    "print(media_online_monitoring_id)\n",
    "\n",
    "\n",
    "# Prepare SQL Statement\n",
    "print(i_process_id)\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = %s\"\n",
    "sql = sql.replace('%s', str(i_process_id))\n",
    "\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "print('update ' + str(row_count) + ' rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c141bcf4-1d32-419d-a7fb-f77c0bcb4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3935\n",
      "update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = 3935\n",
      "update 1 rows\n",
      "insert into ret_analysis_header (job_id, datetime_start, user_id) values (3935, now(), '1')\n",
      "\n",
      "insert into ret_analysis_parameter \n",
      "(job_id, param_id, param_name, param_value) \n",
      "values (3935, 1, 'Similarity Treshold', 0.9)\n",
      "\n",
      "\n",
      "insert into ret_analysis_parameter \n",
      "(job_id, param_id, param_name, param_value) \n",
      "values (3935, 1, 'DB_ID', 'eb784a56-6fcb-4252-b0b8-21402b8c286f')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# marking jobs\n",
    "# Prepare SQL Statement\n",
    "print(i_process_id)\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = %s\"\n",
    "sql = sql.replace('%s', str(i_process_id))\n",
    "\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "print('update ' + str(row_count) + ' rows')\n",
    "\n",
    "#\n",
    "# Create Header Record\n",
    "sql = \"insert into ret_analysis_header (job_id, datetime_start, user_id) values (\" + str(i_process_id) + \", now(), '1')\"\n",
    "# Execute the query\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "#\n",
    "# Create Parameter Record\n",
    "sql = \"\"\"\n",
    "insert into ret_analysis_parameter \n",
    "(job_id, param_id, param_name, param_value) \n",
    "values (%s, %s, %s, %s)\n",
    "\"\"\" % (str(i_process_id), '1', \"'Similarity Treshold'\", str(similarity_treshold))\n",
    "# Execute the query\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "\n",
    "sql = \"\"\"\n",
    "insert into ret_analysis_parameter \n",
    "(job_id, param_id, param_name, param_value) \n",
    "values (%s, %s, %s, '%s')\n",
    "\"\"\" % (str(i_process_id), '1', \"'DB_ID'\", str(database_keyword_id))\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dec7569-472c-4574-aaaa-23587e641f5b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import array as arr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    \n",
    "    return strOut\n",
    "# end Text Preprocessing\n",
    "\n",
    "def calculate_similarity(X_set, Y_set):\n",
    "# Program to measure the similarity between  \n",
    "# two sentences using cosine similarity. \n",
    "\n",
    "    l1 =[];l2 =[]\n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector\n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i]\n",
    "    try:\n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    except:\n",
    "        # print('zero div on X_set')\n",
    "        return 0;\n",
    "        \n",
    "    return cosine\n",
    "\n",
    "def largest_in_col(arr,nCol):\n",
    "    # \n",
    "    # Find largest value of col nCol on 2D arr\n",
    "    #\n",
    "    \n",
    "    # init value\n",
    "    max_val = arr[0][nCol]\n",
    "    # also, remember index\n",
    "    row_index = 0\n",
    "    \n",
    "    for x in range(0, len(arr)):\n",
    "        if arr[x][nCol] > max_val:\n",
    "            max_val = arr[x][nCol]\n",
    "            row_index = x\n",
    "        \n",
    "    return max_val,row_index\n",
    "\n",
    "def smallest_in_col(arr,nCol):\n",
    "    # \n",
    "    # Find smallest value of col nCol on 2D arr\n",
    "    #\n",
    "    \n",
    "    # init value\n",
    "    min_val = arr[0][nCol]\n",
    "    # also, remember index\n",
    "    row_index = 0\n",
    "    \n",
    "    for x in range(0, len(arr)):\n",
    "        if arr[x][nCol] < min_val:\n",
    "            min_val = arr[x][nCol]\n",
    "            row_index = x\n",
    "        \n",
    "    return min_val,row_index\n",
    "\n",
    "def process_text_content(df, platform_id, i_process_id):\n",
    "    # Apply Text Preproc\n",
    "    df['text_content'] = df['text_content'].apply(text_preproc)\n",
    "    \n",
    "    # Apply to data frame\n",
    "    df['text_content'] = df['text_content'].apply(text_preproc)\n",
    "    df.head()\n",
    "    \n",
    "    from nltk.corpus import stopwords \n",
    "    \n",
    "    # Load stopwords\n",
    "    sw = stopwords.words('indonesian')\n",
    "    \n",
    "    # apply tokenize\n",
    "    df['tokenized_tweet'] = df.apply(lambda row: nltk.word_tokenize(row['text_content']), axis=1)\n",
    "    \n",
    "    # apply stopword removal\n",
    "    df['sw'] = df.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "    \n",
    "    df.head()\n",
    "    \n",
    "    \n",
    "    st = similarity_treshold\n",
    "    cluster_no = 1\n",
    "    s_score = 0\n",
    "    s_score_current = 0\n",
    "    i_current_cluster = 0\n",
    "    \n",
    "    #create zero element array\n",
    "    #col 0 => index base tweet\n",
    "    #col 1 => cluster number\n",
    "    #col 2 => similarity score\n",
    "    base_tweet = []\n",
    "    \n",
    "    #proceed to compare to all tweet\n",
    "    cosine_matrix = np.zeros(( len(df), len(df) ), dtype=np.dtype('f4'))\n",
    "    \n",
    "    # flag the mt\n",
    "    with tqdm(total=( (len(df)*len(df)/2))-(len(df)/2))  as pbar:\n",
    "        for j in range(0, len(df)):\n",
    "            tweet_to_compare = df['sw'][j]\n",
    "    \n",
    "            #check, is this second tweet?\n",
    "            if(j == 0):\n",
    "                #first tweet, add as cluster no #1\n",
    "                base_tweet.append([j,1,1.0])\n",
    "                i_current_cluster = base_tweet[0][1]\n",
    "    \n",
    "            elif(j == 1):\n",
    "                #compare to prev tweet\n",
    "                s_score = calculate_similarity(tweet_to_compare, df['sw'][ base_tweet[0][0] ])\n",
    "    \n",
    "                if(s_score < st):\n",
    "                    #not similar\n",
    "                    base_tweet.append([j,2,1])\n",
    "                    i_current_cluster = base_tweet[j][1]\n",
    "    \n",
    "            else:\n",
    "                #other else tweet\n",
    "                for x in range(0,len(base_tweet)):\n",
    "                    #compare every element\n",
    "                    s_score = calculate_similarity(tweet_to_compare, df['sw'][ base_tweet[x][0] ])\n",
    "                    base_tweet[x][2] = s_score\n",
    "    \n",
    "                if(largest_in_col(base_tweet,2)[0] < st):\n",
    "                    #no similar, add as one new cluster\n",
    "                    i_current_cluster = i_current_cluster + 1\n",
    "                    base_tweet.append([j,i_current_cluster,largest_in_col(base_tweet,2)[0]])\n",
    "                else:\n",
    "                    #determine cluster# from biggest similarity\n",
    "                    i_current_cluster = base_tweet[(largest_in_col(base_tweet,2)[1])][1]\n",
    "    \n",
    "            #proceed to compare to all tweet\n",
    "            for i in range(0,len(df)):\n",
    "                # update progress\n",
    "                if (j<i):\n",
    "                    pbar.update(1)\n",
    "                    s_score = calculate_similarity(tweet_to_compare, df['sw'][i])\n",
    "                    if (s_score >= st):\n",
    "                        cosine_matrix[i,j] = i_current_cluster\n",
    "    \n",
    "    \n",
    "    pbar.close()\n",
    "    # print(cosine_matrix)\n",
    "    #\n",
    "    # Create Tweet Cluster Record\n",
    "    sql = \"insert into ret_cluster_result_monitor (ref_id, cluster_no, platform_id, job_id) values ('%s', %s, %s, %s)\"\n",
    "    \n",
    "    #print(len(cosine_matrix))\n",
    "    #print(i_process_id)\n",
    "    \n",
    "    # initiate cluster number\n",
    "    i_cluster_no_save = 0\n",
    "    temp_val = 0\n",
    "                     \n",
    "    ## inserting tweet cluster\n",
    "    for i in range(0, len(cosine_matrix)):\n",
    "        \n",
    "        # find value in this particular row\n",
    "        for j in range(0,len(cosine_matrix[i])):\n",
    "            \n",
    "            # print(cosine_matrix[i][j])\n",
    "            temp_val = cosine_matrix[j][i]\n",
    "            \n",
    "            if(temp_val != 0):\n",
    "                i_cluster_no_save = temp_val\n",
    "    \n",
    "        # Execute the query\n",
    "        data_connector.execute_query_psql(sql % (df['ref_id'][i], i_cluster_no_save, platform_id, i_process_id,))\n",
    "        # print(sql % (df['ref_id'][i], i_cluster_no_save, platform_id, i_process_id))\n",
    "    \n",
    "    print('finished inserting cluster data')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52502a1-e541-44e6-962e-a62f8840f7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12246/12246.0 [00:00<00:00, 42454.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished inserting cluster data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 142311/142311.0 [00:02<00:00, 56191.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished inserting cluster data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3523185/3523185.0 [03:10<00:00, 18523.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished inserting cluster data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 175528/175528.0 [00:06<00:00, 25223.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished inserting cluster data\n",
      "Total Processed Data: 3939\n"
     ]
    }
   ],
   "source": [
    "# 10 = tiktok\n",
    "# 20 = youtube\n",
    "# 30 = instagram_post\n",
    "# 40 = facebook_post\n",
    "# 50 = google_result\n",
    "\n",
    "# Processing jobs for each platform\n",
    "iRowCount = 0\n",
    "\n",
    "if df_job['is_tiktok'][0]:\n",
    "    sql = '''\n",
    "    select id as ref_id, \"desc\" as text_content \n",
    "    from tiktok \n",
    "    where monitoring_id = '%s' -- 10\n",
    "    '''\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    process_text_content(df,10, i_process_id)\n",
    "    iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_youtube'][0]:\n",
    "    sql = '''\n",
    "    select \tid as ref_id, title as text_content\n",
    "    from youtube \n",
    "    where monitoring_id = '%s' -- 20\n",
    "    '''\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    process_text_content(df,20, i_process_id)\n",
    "    iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_instagram'][0]:\n",
    "    sql = '''\n",
    "    select id as ref_id, content as text_content\n",
    "    from instagram_post \n",
    "    where monitoring_id = '%s' -- 30\n",
    "    '''\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    process_text_content(df,30, i_process_id)\n",
    "    iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_facebook'][0]:\n",
    "    sql = '''\n",
    "    select id as ref_id, \"description\" as text_content\n",
    "    from facebook_post \n",
    "    where monitoring_id = '%s' -- 40\n",
    "    and length(trim(\"description\")) > 0\n",
    "    '''\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    process_text_content(df,40, i_process_id)\n",
    "    iRowCount = iRowCount + len(df)\n",
    "\n",
    "# if row_count['is_google'][0]:\n",
    "#     sql = '''\n",
    "#     select \tid as ref_id, \"description\" as text_content\n",
    "#     from google_result \n",
    "#     where monitoring_id = '%s' -- 50\n",
    "#     '''\n",
    "#     df_google = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "#     process_text_content(df_google,50, i_process_id)\n",
    "\n",
    "\n",
    "print(\"Total Processed Data: \" + str(iRowCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052ca3f-2ed8-45bd-ae46-0277ea00e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finishing Jobs\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "data_connector.execute_query_psql(sql % (i_process_id, 1, \"'#Content Processed'\",iRowCount))\n",
    "\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"update ret_analysis_header set datetime_finish = NOW() where job_id = %s\"\n",
    "# Executing query\n",
    "data_connector.execute_query_psql(sql % (i_process_id) )\n",
    "\n",
    "sql = \"update screen_analisis_ai set status = 3, duration = EXTRACT(EPOCH FROM (now() - start_process)), end_process = NOW() where id = %s\"\n",
    "data_connector.execute_query_psql(sql % (i_process_id))\n",
    "\n",
    "print('inserting result finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a3cff-f511-40db-a274-f2854a03805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait 10 sec before release\n",
    "time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
