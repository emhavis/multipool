{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77be3745-de0f-4f27-a7ce-f0b3839d647f",
   "metadata": {},
   "source": [
    "1. get sample image\n",
    "2. download image\n",
    "3. perform imnage grouping\n",
    "4. inspect result\n",
    "\n",
    "Next items to do, prepare and package as jobs\n",
    "1. create auto-query\n",
    "2. create sql to store result\n",
    "3. export to executable py\n",
    "\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse)\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');\n",
    "\n",
    "# prepare data for saving to rdbms\n",
    "# print(clus_new)\n",
    "# i = 0 \n",
    "result_cluster = []\n",
    "\n",
    "for item in clus_new:\n",
    "    # print(clus_new[item])\n",
    "    print(len(clus_new[item]))\n",
    "    for file in clus_new[item]:\n",
    "        # print(file, item)\n",
    "        result_cluster.append(file)\n",
    "    # i = i + 1\n",
    "\n",
    "# view_cluster_2(0)\n",
    "len(clus_new)\n",
    "len(result_cluster)\n",
    "\n",
    "\n",
    " * Kesepakatan status di kolom screen_analisis_ai.status\n",
    " * 1 --> baru diinput\n",
    " * 2 --> lagi dikerjakan\n",
    " * 3 --> proses berhasil\n",
    " * 4 --> proses gagal\n",
    " *\n",
    " \n",
    " * Kesepakatan jenis analisa AI\n",
    " * 1 --> Analisa Cluster\n",
    " * 2 --> Analisa image clustering\n",
    " * 3 --> Analisa sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0951b4-bdc4-4c7a-9846-016d02539805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 10:55:25.582552: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-14 10:55:25.690234: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-14 10:55:25.691054: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-14 10:55:28.645062: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# for loading/processing the images  \n",
    "from keras.preprocessing.image import load_img \n",
    "from keras.preprocessing.image import img_to_array \n",
    "from keras.applications.vgg16 import preprocess_input \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# models \n",
    "from keras.applications.vgg16 import VGG16 \n",
    "from keras.models import Model\n",
    "\n",
    "# clustering and dimension reduction\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# for everything else\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51bdb754-c427-4732-8940-d82b64678866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, folder_path):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Extract the filename from the URL\n",
    "            filename = os.path.join(folder_path, os.path.basename(url))\n",
    "\n",
    "            # Save the image to the specified folder location\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "            # print(f\"Image downloaded and saved as {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download image. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "## query functions\n",
    "def execute_mysql_query(query):\n",
    "    try:\n",
    "        # Define the connection parameters inside the function\n",
    "        host = \"202.157.185.40\"\n",
    "        port = 3306  # Replace with your MySQL port number\n",
    "        database = \"cekmedsos_database\"\n",
    "        user = \"cekmedsos_db\"\n",
    "        password = \"282E~f0si\"\n",
    "\n",
    "        # Create a SQLAlchemy engine using the provided connection parameters\n",
    "        connection_url = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{database}\"\n",
    "        engine = create_engine(connection_url)\n",
    "\n",
    "        # Establish a connection\n",
    "        connection = engine.connect()\n",
    "\n",
    "        # Execute the MySQL query and fetch the results into a DataFrame\n",
    "        result_df = pd.read_sql(query, connection)\n",
    "\n",
    "        # Close the database connection\n",
    "        connection.close()\n",
    "\n",
    "        print(\"Query executed successfully.\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def clear_image():\n",
    "    folder_path = '/home/qudoco/jupyter/ai-project/ai-case-study/img'\n",
    "    # Create a Path object for the folder\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    # Iterate through the files in the folder and delete them\n",
    "    for file in folder.iterdir():\n",
    "        try:\n",
    "            if file.is_file():\n",
    "                file.unlink()\n",
    "                # print(f\"Deleted {file}\")\n",
    "            else:\n",
    "                print(f\"{file} is not a file.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {str(e)}\")\n",
    "\n",
    "# extracting feature from image files\n",
    "def extract_features(file, model):\n",
    "    # load the image as a 224x224 array\n",
    "    img = load_img(file, target_size=(224,224))\n",
    "    # convert from 'PIL.Image.Image' to numpy array\n",
    "    img = np.array(img) \n",
    "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n",
    "    reshaped_img = img.reshape(1,224,224,3) \n",
    "    # prepare image for model\n",
    "    imgx = preprocess_input(reshaped_img)\n",
    "    # get the feature vector\n",
    "    features = model.predict(imgx, use_multiprocessing=True)\n",
    "    return features\n",
    "\n",
    "# function that lets you view a cluster (based on identifier)        \n",
    "def view_cluster(cluster):\n",
    "    plt.figure(figsize = (25,25));\n",
    "    # gets the list of filenames for a cluster\n",
    "    files = groups[cluster]\n",
    "    # only allow up to 30 images to be shown at a time\n",
    "    if len(files) > 30:\n",
    "        print(f\"Clipping cluster size from {len(files)} to 30\")\n",
    "        files = files[:29]\n",
    "    # plot each image in the cluster\n",
    "    for index, file in enumerate(files):\n",
    "        plt.subplot(10,10,index+1);\n",
    "        img = load_img(file)\n",
    "        img = np.array(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e7431f-bef6-42e7-a49e-27ebcf66c2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully.\n",
      "iJobID: 488\n",
      "const_job_id: 2678\n",
      "const_parameter: None\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Get jobsid from queue table\n",
    "# \n",
    "iJobID = 0\n",
    "sql = \"select id, hastag, `parameter` \\\n",
    "from screen_analisis_ai \\\n",
    "where active = 1 \\\n",
    "and status = 1 \\\n",
    "and jenis_analisa = 2 \\\n",
    "order by created asc, id asc limit 1\"\n",
    "# sql = \"select * from screen_analisis_ai saa where id = 1913\"\n",
    "\n",
    "df_res = execute_mysql_query(sql)\n",
    "\n",
    "# check job availability\n",
    "if(len(df_res)) == 0:\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "\n",
    "df_res.head()\n",
    "iJobID = df_res['hastag'][0]\n",
    "const_job_id = df_res['id'][0]\n",
    "const_parameter = df_res['parameter'][0]\n",
    "\n",
    "print('iJobID: ' + str(iJobID))\n",
    "print('const_job_id: ' + str(const_job_id))\n",
    "print('const_parameter: ' + str(const_parameter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad97233a-c5eb-404b-9e92-f883fe3f4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "def execute_sqlalchemy_transaction(transaction_query):\n",
    "    # Database connection parameters\n",
    "    db_url = 'mysql://cekmedsos_db:282E~f0si@202.157.185.40/cekmedsos_database'\n",
    "        # Define the connection parameters inside the function\n",
    "    \n",
    "    try:\n",
    "        # Create a SQLAlchemy engine and session\n",
    "        engine = create_engine(db_url)\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "\n",
    "        # Begin a transaction\n",
    "        session.begin()\n",
    "\n",
    "        try:\n",
    "            # Execute the transaction query\n",
    "            query_text = text(transaction_query)\n",
    "            session.execute(query_text)\n",
    "\n",
    "            # Commit the transaction if the query succeeded\n",
    "            session.commit()\n",
    "            #print(\"Transaction completed successfully.\")\n",
    "        except SQLAlchemyError as e:\n",
    "            # Rollback the transaction on error\n",
    "            session.rollback()\n",
    "            print(f\"Transaction error: {str(e)}\")\n",
    "        finally:\n",
    "            # Close the session\n",
    "            session.close()\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error connecting to the database: {str(e)}\")\n",
    "\n",
    "# # Example usage:\n",
    "# transaction_query = \"\"\"\n",
    "#     UPDATE screen_analisis_ai\n",
    "#     SET status = 2,\n",
    "#         last_status_update = now(),\n",
    "#         start_process = now()\n",
    "#     WHERE id = '1841';\n",
    "# \"\"\"\n",
    "\n",
    "# execute_sqlalchemy_transaction(transaction_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14cc5e6-bd16-4a40-b585-478eeea0e8c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully.\n"
     ]
    }
   ],
   "source": [
    "## get images from server\n",
    "# Example usage:\n",
    "query = \"select a.id, a.db_id, c.tweet_id, c.filename \\\n",
    "from ret_available_db a inner join ret_tweet b on a.db_id = b.db_id \\\n",
    "inner join media_files c on a.db_id = c.db_id and b.id = c.tweet_id \\\n",
    "where \ta.db_id = \" + str(iJobID)\n",
    "\n",
    "result_df = execute_mysql_query(query)\n",
    "\n",
    "## downloading image set\n",
    "img_prefix_http = \"https://cekmedsos.com/uploads/twimg/\"\n",
    "folder_path = \"/home/qudoco/jupyter/ai-project/ai-case-study/img\"    # Replace with the desired folder path\n",
    "\n",
    "# download_image(url, folder_path)\n",
    "# clear up folder first\n",
    "clear_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5db1b-ff77-41c2-88c1-f0dc1e6eae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cekmedsos.com/uploads/twimg/20230808/EupzLUqVoAIs9Js.jpg\n",
      "https://cekmedsos.com/uploads/twimg/20230808/EupzLjdU4AA0vAg.jpg\n",
      "https://cekmedsos.com/uploads/twimg/20230808/EupzL0aVEAEWfGT.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def download_image(url, folder_path):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Extract the filename from the URL\n",
    "            filename = os.path.join(folder_path, os.path.basename(url))\n",
    "\n",
    "            # Save the image to the specified folder location\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            file_size_bytes = os.path.getsize(filename)\n",
    "            file_size_kb = file_size_bytes / 1024\n",
    "\n",
    "            if file_size_kb >= 200:\n",
    "                print(\"Resizing...\")\n",
    "                resize_and_upload_image(filename,url)\n",
    "            # print(f\"Image downloaded and saved as {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download image. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "def resize_and_upload_image(file_path, upload_url, target_size_kb=200):\n",
    "    try:\n",
    "        # Open the image from the file\n",
    "        with Image.open(file_path) as image:\n",
    "            # Calculate the target size in bytes\n",
    "            target_size_bytes = target_size_kb * 1024\n",
    "\n",
    "            # Initialize the quality variable\n",
    "            quality = 95\n",
    "\n",
    "            while os.path.getsize(file_path) > target_size_bytes:\n",
    "                # Resize the image while keeping the quality constant\n",
    "                width, height = image.size\n",
    "                new_width = int(width * 0.9)\n",
    "                new_height = int(height * 0.9)\n",
    "\n",
    "                # Ensure the dimensions are at least 1\n",
    "                new_width = max(1, new_width)\n",
    "                new_height = max(1, new_height)\n",
    "\n",
    "                image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "                # Save the resized image to the same file\n",
    "                image.save(file_path, \"JPEG\", quality=quality)\n",
    "\n",
    "            if os.path.getsize(file_path) <= target_size_bytes:\n",
    "                print(f\"Image resized and overwritten at: {file_path}\")\n",
    "\n",
    "                # Upload the resized image to the given URL\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    response = requests.post(upload_url, files={'file': (os.path.basename(file_path), file)})\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"Image uploaded to {upload_url}\")\n",
    "                else:\n",
    "                    print(f\"Upload failed with status code {response.status_code}\")\n",
    "            else:\n",
    "                print(f\"Image not resized, not uploaded.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "path = \"/home/qudoco/jupyter/ai-project/ai-case-study/img\"\n",
    "\n",
    "i = 0\n",
    "for index, row in result_df.iterrows():\n",
    "    # print(result_df.at[index,'filename'])\n",
    "    url = img_prefix_http + result_df.at[index,'filename']\n",
    "    print(url)\n",
    "    download_image(url, folder_path)\n",
    "    i = i +1\n",
    "\n",
    "print(\"finished downloading \" + str(i) + \" image set\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09264a4d-a914-4308-b384-160dcb5b3610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12d0bc-5858-4b57-b7b8-b47cf21c7838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8056e-731a-4a32-8ce6-a00d44932c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f420ef6-2705-4f24-a59f-1cf70795679c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c4e8c-9403-4fc5-b1c1-562e1f0cf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"/home/qudoco/jupyter/ai-project/ai-case-study/img\"\n",
    "# change the working directory to the path where the images are located\n",
    "os.chdir(path)\n",
    "\n",
    "# this list holds all the image filename\n",
    "flowers = []\n",
    "\n",
    "# creates a ScandirIterator aliased as files\n",
    "with os.scandir(path) as files:\n",
    "  # loops through each file in the directory\n",
    "    for file in files:\n",
    "        if file.name.endswith('.jpg'):\n",
    "          # adds only the image files to the flowers list\n",
    "            flowers.append(file.name)\n",
    "        if file.name.endswith('.png'):\n",
    "            # adds only the image files to the flowers list\n",
    "            flowers.append(file.name)\n",
    "            \n",
    "model = VGG16()\n",
    "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
    "   \n",
    "data = {}\n",
    "p = \"/home/qudoco/jupyter/ai-project/ai-case-study/img/fvec\"\n",
    "\n",
    "# lop through each image in the dataset\n",
    "for flower in flowers:\n",
    "    # try to extract the features and update the dictionary\n",
    "    try:\n",
    "        feat = extract_features(flower,model)\n",
    "        data[flower] = feat\n",
    "    # if something fails, save the extracted features as a pickle file (optional)\n",
    "    except:\n",
    "        with open(p,'wb') as file:\n",
    "            pickle.dump(data,file)\n",
    "          \n",
    "# get a list of the filenames\n",
    "filenames = np.array(list(data.keys()))\n",
    "\n",
    "# get a list of just the features\n",
    "feat = np.array(list(data.values()))\n",
    "\n",
    "# reshape so that there are 210 samples of 4096 vectors\n",
    "feat = feat.reshape(-1,4096)\n",
    "\n",
    "# get the unique labels (from the flower_labels.csv)\n",
    "df = pd.read_csv('/home/qudoco/jupyter/ai-project/ai-case-study/flower_labels.csv')\n",
    "label = df['label'].tolist()\n",
    "unique_labels = list(set(label))\n",
    "\n",
    "# reduce the amount of dimensions in the feature vector\n",
    "pca = PCA(n_components=100, random_state=22)\n",
    "pca.fit(feat)\n",
    "x = pca.transform(feat)\n",
    "\n",
    "# cluster feature vectors\n",
    "kmeans = KMeans(n_clusters=len(unique_labels), random_state=22)\n",
    "kmeans.fit(x)\n",
    "\n",
    "# holds the cluster id and the images { id: [images] }\n",
    "groups = {}\n",
    "for file, cluster in zip(filenames,kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(file)\n",
    "    else:\n",
    "        groups[cluster].append(file)\n",
    "\n",
    "# this is just incase you want to see which value for k might be the best \n",
    "sse = []\n",
    "list_k = list(range(3, 50))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, random_state=22)\n",
    "    km.fit(x)\n",
    "    \n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# function to calculate clusters\n",
    "def cluster(filePaths, features, threshold=0.9):\n",
    "    features = features.reshape(-1,4096)\n",
    "    simMatrix = cosine_similarity(features)\n",
    "    clusters = {}\n",
    "    for i in range(len(features)):\n",
    "        dupIdx = list(np.where(simMatrix[i] > threshold)[0])\n",
    "        # The similarity matrix will include comparisons of items with themselves, which will always \n",
    "        # result in a similarity of 1.0 (100%) and is redundant, so we ignore those\n",
    "        if len(dupIdx) > 1:\n",
    "            curCluster, clusterMatch = list(dupIdx), None\n",
    "            # The first time an image is found to be in any given cluster, we log the entire cluster, \n",
    "            # so subsequent checks of other images from the same cluster would result in duplicated clusters.\n",
    "            # Check for that here\n",
    "            for cIdx in clusters:\n",
    "                if curCluster[0] in clusters[cIdx]:\n",
    "                    clusterMatch = cIdx\n",
    "                    break\n",
    "            # If the current cluster didn't match any existing ones, create/log it\n",
    "            if clusterMatch == None: clusters[len(clusters)] = curCluster\n",
    "    # Resolve file indices back to file paths\n",
    "    for cIdx in clusters: clusters[cIdx] = [filePaths[x] for x in clusters[cIdx]]\n",
    "    return clusters\n",
    "\n",
    "# another method of clustering based on CSI\n",
    "clus_new = cluster(filenames, feat, threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431c067-bb02-4785-9f0a-511ceacbb4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for saving to rdbms\n",
    "save_df = result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19662fd0-5dd8-466f-a505-a81568f2a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df[[\"file_folder\",\"filename_actual\"]] = save_df['filename'].str.split('/',n=1, expand=True)\n",
    "save_df['cluster_number'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81fc242-b9f9-4d7e-a4e2-7e4605826aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_cluster = []\n",
    "\n",
    "for item in clus_new:\n",
    "    # print(clus_new[item])\n",
    "    # print(len(clus_new[item]))\n",
    "    for file in clus_new[item]:\n",
    "        # print(item)\n",
    "        print(file)\n",
    "        filter_condition = save_df['filename_actual'] == file\n",
    "        save_df.loc[filter_condition,'cluster_number'] = item\n",
    "        result = save_df[filter_condition]\n",
    "        # result['cluster_number'] = item\n",
    "        # print(save_df.loc[filter_condition,'cluster_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d585c9-ce51-461d-86c7-c9bc2c45fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving result to table\n",
    "# jobid, tweet_id, cluster_no\n",
    "\n",
    "s_cluster_number = \"\"\n",
    "\n",
    "for index, row in save_df.iterrows():\n",
    "    if row['cluster_number'] == '':\n",
    "        s_cluster_number = \"NULL\"\n",
    "    else:\n",
    "        s_cluster_number = row['cluster_number']\n",
    "        \n",
    "    sql = \"INSERT into ret_cluster_result (job_id, tweet_id, cluster_no) values (%s, %s, %s)\" % (str(const_job_id), row['tweet_id'], s_cluster_number)\n",
    "    print(sql)\n",
    "    execute_sqlalchemy_transaction(sql)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f02c84-7b1a-44c5-9b1f-40a952c24729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing .... report back job status into rdbms\n",
    "sql = \"update screen_analisis_ai set end_process = now(), status = 3, processby_id = 1 where id = %s\" % (str(const_job_id))\n",
    "execute_sqlalchemy_transaction(sql)\n",
    "\n",
    "sql = \"update screen_analisis_ai set duration = TIMEDIFF(end_process, start_process) where id = \" + str(const_job_id)\n",
    "execute_sqlalchemy_transaction(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e4f75-0cb1-403d-bce5-355edb5f9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait 10 seconds before finished\n",
    "import time\n",
    "time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
