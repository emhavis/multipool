{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75099d4-aaab-49a4-9c27-0fa5e5a8ba70",
   "metadata": {},
   "source": [
    "# Main function definition\n",
    "Queue from screen-analisis-ai where :\n",
    "- status = 1\n",
    "- jenis-analisa = 10\n",
    "- order by created desc, limit 1\n",
    "\n",
    "### Input as proces started\n",
    "Record header and parameter information\n",
    "\n",
    " * Kesepakatan status di kolom screen_analisis_ai.status\n",
    " * 1 --> baru diinput\n",
    " * 2 --> lagi dikerjakan\n",
    " * 3 --> proses berhasil\n",
    " * 4 --> proses gagal\n",
    "\n",
    "### Steps Explanation:\n",
    "1. Data Loading and Cleaning: Load the JSON data and clean the text data.\n",
    "2. Feature Extraction: Convert the cleaned text into numerical features using TF-IDF.\n",
    "3. Optimal Cluster Determination: Use the Elbow Method and Silhouette Score to determine the optimal number of clusters. The optimal number of clusters is determined based on the highest silhouette score.\n",
    "4. Clustering: Perform clustering using K-Means with the optimal number of clusters.\n",
    "5. Print Cluster Content: Print the content of each cluster, including the number of members in each cluster.\n",
    "6. PCA Scatter Plot: Visualize the clusters using a 2D scatter plot with PCA.\n",
    "\n",
    "This script will dynamically determine the optimal number of clusters, perform clustering, and print the content of each cluster along with the number of members in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82f8f0ec-9ce0-46e2-93e8-58697d0e07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import json\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import zipfile\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Internal Data Connector\n",
    "import data_connector\n",
    "\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation and numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "def cluster_and_return(df, max_clusters=15, batch_size=10000):\n",
    "    # Clean the text data\n",
    "    df['cleaned_text'] = df['text_content'].apply(clean_text)\n",
    "    \n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    \n",
    "    # Fit the vectorizer on the entire dataset to establish the vocabulary\n",
    "    vectorizer.fit(df['cleaned_text'])\n",
    "    \n",
    "    # Transform the data in batches\n",
    "    n_batches = int(np.ceil(len(df) / batch_size))\n",
    "    X = csr_matrix((0, len(vectorizer.vocabulary_)), dtype=np.float64)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        batch_texts = df['cleaned_text'][i * batch_size:(i + 1) * batch_size]\n",
    "        X_batch = vectorizer.transform(batch_texts)\n",
    "        X = vstack([X, X_batch])\n",
    "    \n",
    "    # Determine the optimal number of clusters using Elbow Method and Silhouette Score\n",
    "    def determine_optimal_clusters(X, max_clusters):\n",
    "        wcss = []\n",
    "        silhouette_scores = []\n",
    "        for i in range(2, max_clusters):  # start from 2 clusters\n",
    "            kmeans = MiniBatchKMeans(n_clusters=i, random_state=42, batch_size=batch_size)\n",
    "            kmeans.fit(X)\n",
    "            wcss.append(kmeans.inertia_)\n",
    "            if i > 1:\n",
    "                silhouette_avg = silhouette_score(X, kmeans.labels_)\n",
    "                silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "        # Determine optimal number of clusters based on highest silhouette score\n",
    "        optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because silhouette_scores starts from 2 clusters\n",
    "        return optimal_clusters\n",
    "\n",
    "    optimal_clusters = determine_optimal_clusters(X, max_clusters)\n",
    "    print(f'Optimal number of clusters: {optimal_clusters}')\n",
    "\n",
    "    # Perform clustering with the optimal number of clusters\n",
    "    kmeans = MiniBatchKMeans(n_clusters=optimal_clusters, random_state=42, batch_size=batch_size)\n",
    "    kmeans.fit(X)\n",
    "    df['cluster_no'] = kmeans.labels_\n",
    "\n",
    "    # Calculate average similarity score for each cluster using sparse matrix operations\n",
    "    cluster_scores = []\n",
    "    cluster_avg_similarities = []\n",
    "\n",
    "    for cluster in range(optimal_clusters):\n",
    "        indices = np.where(kmeans.labels_ == cluster)[0]\n",
    "        if len(indices) > 1:\n",
    "            cluster_sim = cosine_similarity(X[indices])\n",
    "            avg_sim = cluster_sim[np.triu_indices(len(indices), k=1)].mean()\n",
    "        else:\n",
    "            avg_sim = 1.0  # If there's only one member in the cluster, similarity is 1\n",
    "        cluster_scores.extend([avg_sim] * len(indices))\n",
    "        cluster_avg_similarities.append(avg_sim)\n",
    "\n",
    "    df['cluster_score'] = cluster_scores\n",
    "\n",
    "    # calculate the PCA metric\n",
    "    X = TfidfVectorizer(max_features=1000).fit_transform(df['text_content'])\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    pca_components = pca.fit_transform(X.toarray())\n",
    "    df['pca1'] = pca_components[:, 0]\n",
    "    df['pca2'] = pca_components[:, 1]\n",
    "\n",
    "    # Drop the temporary cleaned_text column\n",
    "    df = df.drop(columns=['cleaned_text'])\n",
    "\n",
    "    # Return the updated DataFrame and cluster average similarities\n",
    "    return df, cluster_avg_similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002d3bd-9ef9-4eb0-b9a2-e2b52e67eafb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Grab Jobs Queue\n",
    "ID Jobs ada di screen-analisis-ai, dengan monitoring-id relais ke monitoring_search.id\n",
    "Diambil untuk jenis_analisa = '10' untuk text clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bcfc2a2-297f-4cf0-92fc-f8b05556336c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "5079\n",
      "30fe3eee-55b4-489b-95df-21487eb68a05\n",
      "None\n",
      "5079\n",
      "update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = 5079\n",
      "update 1 rows\n"
     ]
    }
   ],
   "source": [
    "strSQL = \"\"\"\n",
    "select \ta.id as jobsid,\n",
    "        a.*,\n",
    "\t\td.id as key_monitoring_media_social, \n",
    "\t\te.id as key_monitoring_media_online,\n",
    "\t\tc.*\n",
    "from screen_analisis_ai a \n",
    "\tinner join monitoring_search c \n",
    "\t\ton cast(c.id as varchar) = cast(a.monitoring_id as varchar) \n",
    "\tinner join monitoring_media_social d\n",
    "\t\ton d.monitoring_id = c.monitoring_id\n",
    "\tleft outer join monitoring_media_online e\n",
    "\t\ton e.monitoring_id = c.monitoring_id \n",
    "where a.jenis_analisa = '10'\n",
    "and a.status = 1\n",
    "order by a.created desc \n",
    "limit 1\n",
    "\"\"\"\n",
    "\n",
    "df_job = data_connector.execute_query_psql(strSQL)\n",
    "if len(df_job) == 0:\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "    \n",
    "similarity_treshold = 0.9\n",
    "i_process_id = df_job['jobsid'][0]\n",
    "screen_name = ''\n",
    "database_keyword_id = df_job['key_monitoring_media_social'][0]\n",
    "social_media_monitoring_id = df_job['key_monitoring_media_social'][0]\n",
    "media_online_monitoring_id = df_job['key_monitoring_media_online'][0]\n",
    "\n",
    "# print(database_keyword_id)\n",
    "print(similarity_treshold)\n",
    "print(i_process_id)\n",
    "print(social_media_monitoring_id)\n",
    "print(media_online_monitoring_id)\n",
    "\n",
    "\n",
    "# Prepare SQL Statement\n",
    "print(i_process_id)\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = %s\"\n",
    "sql = sql.replace('%s', str(i_process_id))\n",
    "\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "print('update ' + str(row_count) + ' rows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a115370-9a54-41fa-abfc-00a57e40c917",
   "metadata": {},
   "source": [
    "## Processing jobs from queue line\n",
    "- Check for each social media or media online platform,\n",
    "- Get the data accordingly\n",
    "- process to cluster-and-return function\n",
    "- recording result to database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41d277-6190-4131-abcd-c4a18a1e8b2e",
   "metadata": {},
   "source": [
    "### Recording Result to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd5c196-9848-42fc-a4d8-dde792e2adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare function to record result\n",
    "def record_result(result_df, cluster_avg_similarities, i_platform_id):\n",
    "    # result_df, cluster_avg_similarities\n",
    "    for i in range(0, len(result_df)):\n",
    "        ssql = \"\"\"\n",
    "        insert into ret_cluster_result_monitor (ref_id, cluster_no, platform_id,\n",
    "        job_id, cluster_score,pca_1, pca_2) \n",
    "        values ('%s',%s, %s, %s, %s, '%s', '%s')\n",
    "        \"\"\"\n",
    "\n",
    "        # builds str \n",
    "        ssql = ssql % (str(result_df['ref_id'][i]), str(result_df['cluster_no'][i]), str(i_platform_id), str(i_process_id), str(cluster_avg_similarities[result_df['cluster_no'][i]]), str(result_df['pca1'][i]), str(result_df['pca2'][i]))\n",
    "        # print(ssql)\n",
    "        data_connector.execute_query_psql(ssql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "257405ad-78a0-40d5-99da-6ec3b8345412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of clusters: 14\n",
      "Total Processed Data: 210\n"
     ]
    }
   ],
   "source": [
    "# Processing jobs for each platform\n",
    "# 10 = tiktok\n",
    "# 20 = youtube\n",
    "# 30 = instagram_post\n",
    "# 40 = facebook_post\n",
    "# 50 = google_result\n",
    "\n",
    "iRowCount = 0\n",
    "\n",
    "if df_job['is_tiktok'][0]:\n",
    "    sql = '''\n",
    "    select id as ref_id, \"desc\" as text_content \n",
    "    from tiktok \n",
    "    where monitoring_id = '%s' -- 10\n",
    "    '''\n",
    "    i_platform_id = 10\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "        record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_youtube'][0]:\n",
    "    sql = '''\n",
    "    select \tid as ref_id, title as text_content\n",
    "    from youtube \n",
    "    where monitoring_id = '%s' -- 20\n",
    "    '''\n",
    "    i_platform_id = 20\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "        record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_instagram'][0]:\n",
    "    sql = '''\n",
    "    select id as ref_id, content as text_content\n",
    "    from instagram_post \n",
    "    where monitoring_id = '%s' -- 30\n",
    "    '''\n",
    "    i_platform_id = 30\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "        record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_facebook'][0]:\n",
    "    sql = '''\n",
    "    select id as ref_id, \"description\" as text_content\n",
    "    from facebook_post \n",
    "    where monitoring_id = '%s' -- 40\n",
    "    and length(trim(\"description\")) > 0\n",
    "    '''\n",
    "    i_platform_id = 40\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "        record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_google'][0]:\n",
    "    sql = '''\n",
    "    select \tid as ref_id, \"description\" as text_content\n",
    "    from google_result \n",
    "    where monitoring_id = '%s' -- 50\n",
    "    '''\n",
    "    i_platform_id = 50\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "        record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "if df_job['is_twitter'][0]:\n",
    "    sql = '''\n",
    "    select id as ref_id, tweet as text_content\n",
    "    from twitter_tweets \n",
    "    where monitoring_id = '%s' -- 60\n",
    "    '''\n",
    "    i_platform_id = 60\n",
    "    df = data_connector.execute_query_psql(sql % (social_media_monitoring_id))\n",
    "    if len(df) != 0:\n",
    "        result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "        record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "        iRowCount = iRowCount + len(df)\n",
    "\n",
    "print(\"Total Processed Data: \" + str(iRowCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2ba6d-2469-44cc-aabe-0f300b977256",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Mark Jobs queue as done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f092b22f-3e5b-46dd-b764-f6cf719e62d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting result finished\n"
     ]
    }
   ],
   "source": [
    "# Finishing Jobs\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "data_connector.execute_query_psql(sql % (i_process_id, 1, \"'#Content Processed'\",iRowCount))\n",
    "\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"update ret_analysis_header set datetime_finish = NOW() where job_id = %s\"\n",
    "# Executing query\n",
    "data_connector.execute_query_psql(sql % (i_process_id) )\n",
    "\n",
    "sql = \"update screen_analisis_ai set status = 3, duration = EXTRACT(EPOCH FROM (now() - start_process)), end_process = NOW() where id = %s\"\n",
    "data_connector.execute_query_psql(sql % (i_process_id))\n",
    "\n",
    "print('inserting result finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f33696a-05b3-454b-8769-dbfe4df97eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait 10 seconds before finished\n",
    "import time\n",
    "time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
