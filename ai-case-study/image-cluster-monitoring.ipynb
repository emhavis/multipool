{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77be3745-de0f-4f27-a7ce-f0b3839d647f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "1. get sample image\n",
    "2. download image\n",
    "3. perform imnage grouping\n",
    "4. inspect result\n",
    "\n",
    "Next items to do, prepare and package as jobs\n",
    "1. create auto-query\n",
    "2. create sql to store result\n",
    "3. export to executable py\n",
    "\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse)\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');\n",
    "\n",
    "# prepare data for saving to rdbms\n",
    "# print(clus_new)\n",
    "# i = 0 \n",
    "result_cluster = []\n",
    "\n",
    "for item in clus_new:\n",
    "    # print(clus_new[item])\n",
    "    print(len(clus_new[item]))\n",
    "    for file in clus_new[item]:\n",
    "        # print(file, item)\n",
    "        result_cluster.append(file)\n",
    "    # i = i + 1\n",
    "\n",
    "# view_cluster_2(0)\n",
    "len(clus_new)\n",
    "len(result_cluster)\n",
    "\n",
    "\n",
    " * Kesepakatan status di kolom screen_analisis_ai.status\n",
    " * 1 --> baru diinput\n",
    " * 2 --> lagi dikerjakan\n",
    " * 3 --> proses berhasil\n",
    " * 4 --> proses gagal\n",
    " *\n",
    " \n",
    " * Kesepakatan jenis analisa AI\n",
    " * 1 --> Analisa Cluster\n",
    " * 2 --> Analisa image clustering\n",
    " * 3 --> Analisa sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0951b4-bdc4-4c7a-9846-016d02539805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading/processing the images  \n",
    "from keras.preprocessing.image import load_img \n",
    "from keras.preprocessing.image import img_to_array \n",
    "from keras.applications.vgg16 import preprocess_input \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# models \n",
    "from keras.applications.vgg16 import VGG16 \n",
    "from keras.models import Model\n",
    "\n",
    "# clustering and dimension reduction\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# for everything else\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bdb754-c427-4732-8940-d82b64678866",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize_and_upload_image(url, save_folder):\n",
    "    try:\n",
    "        # Fetch the image from the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Open the image from the response content\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "\n",
    "        # Check the image size in bytes\n",
    "        image_size = len(response.content)\n",
    "\n",
    "        # Set the maximum allowed size in bytes (200KB)\n",
    "        max_size = 200 * 1024\n",
    "\n",
    "        # Extract the base path from the URL for saving the images\n",
    "        parsed_url = urlparse(url)\n",
    "        base_path = os.path.basename(parsed_url.path)\n",
    "\n",
    "        # Determine the paths for saving the images\n",
    "        original_image_path = os.path.join(save_folder, 'original_' + base_path)\n",
    "        resized_image_path = os.path.join(save_folder, 'resized_' + base_path)\n",
    "\n",
    "        # If the image size exceeds the maximum allowed size, resize it\n",
    "        if image_size > max_size:\n",
    "            print('more than 200kb')\n",
    "            print(original_image_path)\n",
    "            print(resized_image_path)\n",
    "            \n",
    "            # Calculate the scaling factor to reduce the image size\n",
    "            scale_factor = (max_size / image_size) ** 0.5\n",
    "            new_width = int(image.width * scale_factor)\n",
    "            new_height = int(image.height * scale_factor)\n",
    "\n",
    "            # Resize the image\n",
    "            resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "\n",
    "            # Save the resized image to the local folder\n",
    "            resized_image.save(resized_image_path, format=\"JPEG\")\n",
    "\n",
    "            # Reupload the resized image to the original location\n",
    "            with open(resized_image_path, 'rb') as resized_file:\n",
    "                reupload_url = url  # Use the original URL for reupload\n",
    "                print(url)\n",
    "                files = {'file': ('resized_' + base_path, resized_file)}\n",
    "                reupload_response = requests.post(reupload_url, files=files)\n",
    "                print(reupload_response)\n",
    "\n",
    "            # Delete the original image in the local folder\n",
    "            os.remove(original_image_path)\n",
    "\n",
    "        else:\n",
    "            # Image is smaller than 200KB, no need to resize or reupload\n",
    "            resized_image_path = original_image_path\n",
    "\n",
    "        return original_image_path, resized_image_path\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors such as invalid URLs or image processing issues\n",
    "        return str(e)\n",
    "\n",
    "# Example usage:\n",
    "# imageURL = \"https://example.com/path/to/your/image.jpg\"\n",
    "# save_folder = \"path/to/your/local/folder\"\n",
    "# original_image_path, resized_image_path = resize_and_upload_image(imageURL, save_folder)\n",
    "# Both original_image_path and resized_image_path will contain the file paths in the local folder.\n",
    "\n",
    "def download_image(url, folder_path):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Extract the filename from the URL\n",
    "            filename = os.path.join(folder_path, os.path.basename(url))\n",
    "\n",
    "            # Save the image to the specified folder location\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            file_size_bytes = os.path.getsize(filename)\n",
    "            file_size_kb = file_size_bytes / 1024\n",
    "\n",
    "            if file_size_kb >= 200:\n",
    "                print(\"Resizing...\")\n",
    "                resize_and_upload_image(filename,url)\n",
    "            # print(f\"Image downloaded and saved as {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download image. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "def resize_and_upload_image(file_path, upload_url, target_size_kb=200):\n",
    "    try:\n",
    "        # Open the image from the file\n",
    "        with Image.open(file_path) as image:\n",
    "            # Calculate the target size in bytes\n",
    "            target_size_bytes = target_size_kb * 1024\n",
    "\n",
    "            # Initialize the quality variable\n",
    "            quality = 95\n",
    "\n",
    "            while os.path.getsize(file_path) > target_size_bytes:\n",
    "                # Resize the image while keeping the quality constant\n",
    "                width, height = image.size\n",
    "                new_width = int(width * 0.9)\n",
    "                new_height = int(height * 0.9)\n",
    "\n",
    "                # Ensure the dimensions are at least 1\n",
    "                new_width = max(1, new_width)\n",
    "                new_height = max(1, new_height)\n",
    "\n",
    "                image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "                # Save the resized image to the same file\n",
    "                image.save(file_path, \"JPEG\", quality=quality)\n",
    "\n",
    "            if os.path.getsize(file_path) <= target_size_bytes:\n",
    "                print(f\"Image resized and overwritten at: {file_path}\")\n",
    "\n",
    "                # Upload the resized image to the given URL\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    response = requests.post(upload_url, files={'file': (os.path.basename(file_path), file)})\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"Image uploaded to {upload_url}\")\n",
    "                else:\n",
    "                    print(f\"Upload failed with status code {response.status_code}\")\n",
    "            else:\n",
    "                print(f\"Image not resized, not uploaded.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "## query functions\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, MetaData, Table\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy.util import deprecations\n",
    "from sqlalchemy.sql import text\n",
    "deprecations.SILENCE_UBER_WARNING = True\n",
    "\n",
    "def execute_query_psql(query, params=None):\n",
    "    # Set your PostgreSQL connection parameters\n",
    "    db_params = {\n",
    "        'host': '98.98.117.105',\n",
    "        'port': '5432',\n",
    "        'database': 'medols',\n",
    "        'user': 'postgres',\n",
    "        'password': 'FEWcTB3JIX5gK4T06c1MdkM9N2S8w9pb',\n",
    "    }\n",
    "\n",
    "    # Create a SQLAlchemy engine\n",
    "    engine = create_engine(f\"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}\")\n",
    "\n",
    "    # Create a metadata object\n",
    "    metadata = MetaData()\n",
    "\n",
    "    # Create a session\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    try:\n",
    "        # Execute the query with optional parameters\n",
    "        result = session.execute(text(query), params)\n",
    "\n",
    "        # Check if the query is a SELECT query\n",
    "        is_select_query = result.returns_rows\n",
    "\n",
    "        if is_select_query:\n",
    "            # Fetch the data and return as a Pandas DataFrame\n",
    "            columns = result.keys()\n",
    "            fetched_data = result.fetchall()\n",
    "            df = pd.DataFrame(fetched_data, columns=columns)\n",
    "            # print(\"Fetched Data as DataFrame:\")\n",
    "            # print(df)\n",
    "            return df\n",
    "        else:\n",
    "            # Get the number of rows affected for non-SELECT queries\n",
    "            rows_affected = result.rowcount\n",
    "\n",
    "            # Commit the changes to the database for non-SELECT queries\n",
    "            session.commit()\n",
    "\n",
    "            print(f\"Query executed successfully. {rows_affected} rows affected.\")\n",
    "            return rows_affected\n",
    "    except Exception as e:\n",
    "        # Rollback changes if there's an error\n",
    "        session.rollback()\n",
    "        print(f\"Error executing query: {e}\")\n",
    "    finally:\n",
    "        # Close the session\n",
    "        session.close()\n",
    "\n",
    "def clear_image():\n",
    "    folder_path = '/home/jup_user/multipool/ai-case-study/img'\n",
    "    # Create a Path object for the folder\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    # Iterate through the files in the folder and delete them\n",
    "    for file in folder.iterdir():\n",
    "        try:\n",
    "            if file.is_file():\n",
    "                file.unlink()\n",
    "                # print(f\"Deleted {file}\")\n",
    "            else:\n",
    "                print(f\"{file} is not a file.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {str(e)}\")\n",
    "\n",
    "# extracting feature from image files\n",
    "def extract_features(file, model):\n",
    "    # load the image as a 224x224 array\n",
    "    img = load_img(file, target_size=(224,224))\n",
    "    # convert from 'PIL.Image.Image' to numpy array\n",
    "    img = np.array(img) \n",
    "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n",
    "    reshaped_img = img.reshape(1,224,224,3) \n",
    "    # prepare image for model\n",
    "    imgx = preprocess_input(reshaped_img)\n",
    "    # get the feature vector\n",
    "    features = model.predict(imgx, use_multiprocessing=True)\n",
    "    return features\n",
    "\n",
    "# function that lets you view a cluster (based on identifier)        \n",
    "def view_cluster(cluster):\n",
    "    plt.figure(figsize = (25,25));\n",
    "    # gets the list of filenames for a cluster\n",
    "    files = groups[cluster]\n",
    "    # only allow up to 30 images to be shown at a time\n",
    "    if len(files) > 30:\n",
    "        print(f\"Clipping cluster size from {len(files)} to 30\")\n",
    "        files = files[:29]\n",
    "    # plot each image in the cluster\n",
    "    for index, file in enumerate(files):\n",
    "        plt.subplot(10,10,index+1);\n",
    "        img = load_img(file)\n",
    "        img = np.array(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7431f-bef6-42e7-a49e-27ebcf66c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "strSQL = \"\"\"\n",
    "select \ta.id as jobsid,\n",
    "        a.*,\n",
    "\t\td.id as key_monitoring_media_social, \n",
    "\t\te.id as key_monitoring_media_online,\n",
    "\t\tc.*\n",
    "from screen_analisis_ai a inner join monitoring b\n",
    "\ton a.monitoring_id = cast(b.id as varchar)\n",
    "\tinner join monitoring_search c \n",
    "\t\ton b.id = c.monitoring_id \n",
    "\tinner join monitoring_media_social d\n",
    "\t\ton d.monitoring_id = c.monitoring_id\n",
    "\tleft outer join monitoring_media_online e\n",
    "\t\ton e.monitoring_id = c.monitoring_id \n",
    "where a.jenis_analisa = '20'\n",
    "and a.status = 1\n",
    "order by a.created desc \n",
    "limit 1\n",
    "\"\"\"\n",
    "\n",
    "df_job = data_connector.execute_query_psql(strSQL)\n",
    "if len(df_job) == 0:\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "    \n",
    "similarity_treshold = 0.9\n",
    "i_process_id = df_job['jobsid'][0]\n",
    "screen_name = ''\n",
    "database_keyword_id = df_job['key_monitoring_media_social'][0]\n",
    "social_media_monitoring_id = df_job['key_monitoring_media_social'][0]\n",
    "media_online_monitoring_id = df_job['key_monitoring_media_online'][0]\n",
    "\n",
    "# print(database_keyword_id)\n",
    "print(similarity_treshold)\n",
    "print(i_process_id)\n",
    "print(social_media_monitoring_id)\n",
    "print(media_online_monitoring_id)\n",
    "\n",
    "\n",
    "# Prepare SQL Statement\n",
    "print(i_process_id)\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = %s\"\n",
    "sql = sql.replace('%s', str(i_process_id))\n",
    "\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "print('update ' + str(row_count) + ' rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f216f10-c9cf-45c4-8a41-3ab3245d8c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## report to dbms that we are working on this row\n",
    "sql = \"\"\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = '\" + str(const_job_id) + \"';\"\n",
    "print(sql)\n",
    "# execute\n",
    "# execute_sqlalchemy_transaction(sql)\n",
    "execute_query_psql(sql)\n",
    "#\n",
    "# Create Header Record\n",
    "sql = \"insert into ret_analysis_header (job_id, datetime_start, user_id) values (%s, now(), %s)\" % (const_job_id,\"1\")\n",
    "# Execute the query\n",
    "execute_query_psql(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14cc5e6-bd16-4a40-b585-478eeea0e8c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get images from server\n",
    "# Example usage:\n",
    "query = \"\"\"\n",
    "select a.id, a.db_id, c.tweet_id, c.filename\n",
    "from ret_available_db a inner join ret_tweet b on a.db_id = b.db_id\n",
    "inner join media_files c on b.id = c.tweet_id\n",
    "where \ta.db_id = %s\n",
    "\"\"\"\n",
    "query = query % str(iJobID)\n",
    "print(query)\n",
    "\n",
    "result_df = execute_query_psql(query)\n",
    "\n",
    "## downloading image set\n",
    "img_prefix_http = \"http://98.98.117.121:8082/api/media/photo/download/twitter/\"\n",
    "folder_path = \"/home/jup_user/multipool/ai-case-study/img\"    # Replace with the desired folder path\n",
    "\n",
    "# download_image(url, folder_path)\n",
    "# clear up folder first\n",
    "clear_image()\n",
    "\n",
    "i = 0\n",
    "for index, row in result_df.iterrows():\n",
    "    # print(result_df.at[index,'filename'])\n",
    "    url = img_prefix_http + result_df.at[index,'filename']\n",
    "    print(url)\n",
    "    download_image(url, folder_path)\n",
    "    i = i +1\n",
    "\n",
    "print(\"finished downloading \" + str(i) + \" image set\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c4e8c-9403-4fc5-b1c1-562e1f0cf97a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = folder_path\n",
    "# change the working directory to the path where the images are located\n",
    "os.chdir(path)\n",
    "\n",
    "# this list holds all the image filename\n",
    "flowers = []\n",
    "\n",
    "# creates a ScandirIterator aliased as files\n",
    "with os.scandir(path) as files:\n",
    "  # loops through each file in the directory\n",
    "    for file in files:\n",
    "        if file.name.endswith('.jpg'):\n",
    "          # adds only the image files to the flowers list\n",
    "            flowers.append(file.name)\n",
    "        if file.name.endswith('.png'):\n",
    "            # adds only the image files to the flowers list\n",
    "            flowers.append(file.name)\n",
    "            \n",
    "model = VGG16()\n",
    "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
    "   \n",
    "data = {}\n",
    "p = \"/home/jup_user/multipool/ai-case-study/img\"\n",
    "\n",
    "# lop through each image in the dataset\n",
    "for flower in flowers:\n",
    "    # try to extract the features and update the dictionary\n",
    "    try:\n",
    "        feat = extract_features(flower,model)\n",
    "        data[flower] = feat\n",
    "    # if something fails, save the extracted features as a pickle file (optional)\n",
    "    except:\n",
    "        with open(p,'wb') as file:\n",
    "            pickle.dump(data,file)\n",
    "          \n",
    "# get a list of the filenames\n",
    "filenames = np.array(list(data.keys()))\n",
    "\n",
    "# get a list of just the features\n",
    "feat = np.array(list(data.values()))\n",
    "\n",
    "# reshape so that there are 210 samples of 4096 vectors\n",
    "feat = feat.reshape(-1,4096)\n",
    "\n",
    "# get the unique labels (from the flower_labels.csv)\n",
    "df = pd.read_csv('/home/jup_user/multipool/ai-case-study/flower_labels.csv')\n",
    "label = df['label'].tolist()\n",
    "unique_labels = list(set(label))\n",
    "\n",
    "# reduce the amount of dimensions in the feature vector\n",
    "pca = PCA(n_components=100, random_state=22)\n",
    "pca.fit(feat)\n",
    "x = pca.transform(feat)\n",
    "\n",
    "# cluster feature vectors\n",
    "kmeans = KMeans(n_clusters=len(unique_labels), random_state=22)\n",
    "kmeans.fit(x)\n",
    "\n",
    "# holds the cluster id and the images { id: [images] }\n",
    "groups = {}\n",
    "for file, cluster in zip(filenames,kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(file)\n",
    "    else:\n",
    "        groups[cluster].append(file)\n",
    "\n",
    "# this is just incase you want to see which value for k might be the best \n",
    "sse = []\n",
    "list_k = list(range(3, 50))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, random_state=22)\n",
    "    km.fit(x)\n",
    "    \n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# function to calculate clusters\n",
    "def cluster(filePaths, features, threshold=0.9):\n",
    "    features = features.reshape(-1,4096)\n",
    "    simMatrix = cosine_similarity(features)\n",
    "    clusters = {}\n",
    "    for i in range(len(features)):\n",
    "        dupIdx = list(np.where(simMatrix[i] > threshold)[0])\n",
    "        # The similarity matrix will include comparisons of items with themselves, which will always \n",
    "        # result in a similarity of 1.0 (100%) and is redundant, so we ignore those\n",
    "        if len(dupIdx) > 1:\n",
    "            curCluster, clusterMatch = list(dupIdx), None\n",
    "            # The first time an image is found to be in any given cluster, we log the entire cluster, \n",
    "            # so subsequent checks of other images from the same cluster would result in duplicated clusters.\n",
    "            # Check for that here\n",
    "            for cIdx in clusters:\n",
    "                if curCluster[0] in clusters[cIdx]:\n",
    "                    clusterMatch = cIdx\n",
    "                    break\n",
    "            # If the current cluster didn't match any existing ones, create/log it\n",
    "            if clusterMatch == None: clusters[len(clusters)] = curCluster\n",
    "    # Resolve file indices back to file paths\n",
    "    for cIdx in clusters: clusters[cIdx] = [filePaths[x] for x in clusters[cIdx]]\n",
    "    return clusters\n",
    "\n",
    "# another method of clustering based on CSI\n",
    "clus_new = cluster(filenames, feat, threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431c067-bb02-4785-9f0a-511ceacbb4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for saving to rdbms\n",
    "save_df = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19662fd0-5dd8-466f-a505-a81568f2a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_df[[\"file_folder\",\"filename_actual\"]] = save_df['filename'].str.split('/',n=1, expand=True)\n",
    "save_df['cluster_number'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81fc242-b9f9-4d7e-a4e2-7e4605826aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in clus_new:\n",
    "    # print(clus_new[item])\n",
    "    # print(len(clus_new[item]))\n",
    "    for file in clus_new[item]:\n",
    "        # print(item)\n",
    "        print(file)\n",
    "        filter_condition = save_df['filename'] == file\n",
    "        save_df.loc[filter_condition,'cluster_number'] = item\n",
    "        result = save_df[filter_condition]\n",
    "        # result['cluster_number'] = item\n",
    "        # print(save_df.loc[filter_condition,'cluster_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d585c9-ce51-461d-86c7-c9bc2c45fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving result to table\n",
    "# jobid, tweet_id, cluster_no\n",
    "\n",
    "s_cluster_number = \"\"\n",
    "\n",
    "for index, row in save_df.iterrows():\n",
    "    if row['cluster_number'] == '':\n",
    "        s_cluster_number = \"NULL\"\n",
    "    else:\n",
    "        s_cluster_number = row['cluster_number']\n",
    "        \n",
    "    sql = \"INSERT into ret_cluster_result (job_id, tweet_id, cluster_no) values (%s, %s, %s)\" % (str(const_job_id), row['tweet_id'], s_cluster_number)\n",
    "    print(sql)\n",
    "    execute_query_psql(sql)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f02c84-7b1a-44c5-9b1f-40a952c24729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing .... report back job status into rdbms\n",
    "sql = \"update screen_analisis_ai set end_process = now(), status = 3, processby_id = 1 where id = %s\" % (str(const_job_id))\n",
    "execute_query_psql(sql)\n",
    "\n",
    "sql = \"update screen_analisis_ai set duration = EXTRACT(EPOCH FROM (end_process - start_process)) where id = \" + str(const_job_id)\n",
    "execute_query_psql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e4f75-0cb1-403d-bce5-355edb5f9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait 10 seconds before finished\n",
    "import time\n",
    "time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
