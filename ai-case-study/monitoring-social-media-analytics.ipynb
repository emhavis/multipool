{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f168469-6f08-405c-a8ac-68241439edcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T12:46:57.877219Z",
     "iopub.status.busy": "2024-08-19T12:46:57.876907Z",
     "iopub.status.idle": "2024-08-19T12:46:58.994036Z",
     "shell.execute_reply": "2024-08-19T12:46:58.993493Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# code for text clustering process\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import json\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import zipfile\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Internal Data Connector\n",
    "import data_connector\n",
    "\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation and numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "def cluster_and_return(df, max_clusters=15, batch_size=10000):\n",
    "    # Clean the text data\n",
    "    df['cleaned_text'] = df['text_content'].apply(clean_text)\n",
    "    \n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    \n",
    "    # Fit the vectorizer on the entire dataset to establish the vocabulary\n",
    "    vectorizer.fit(df['cleaned_text'])\n",
    "    \n",
    "    # Transform the data in batches\n",
    "    n_batches = int(np.ceil(len(df) / batch_size))\n",
    "    X = csr_matrix((0, len(vectorizer.vocabulary_)), dtype=np.float64)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        batch_texts = df['cleaned_text'][i * batch_size:(i + 1) * batch_size]\n",
    "        X_batch = vectorizer.transform(batch_texts)\n",
    "        X = vstack([X, X_batch])\n",
    "    \n",
    "    # Determine the optimal number of clusters using Elbow Method and Silhouette Score\n",
    "    def determine_optimal_clusters(X, max_clusters):\n",
    "        wcss = []\n",
    "        silhouette_scores = []\n",
    "        for i in range(2, max_clusters):  # start from 2 clusters\n",
    "            kmeans = MiniBatchKMeans(n_clusters=i, random_state=42, batch_size=batch_size)\n",
    "            kmeans.fit(X)\n",
    "            wcss.append(kmeans.inertia_)\n",
    "            if i > 1:\n",
    "                silhouette_avg = silhouette_score(X, kmeans.labels_)\n",
    "                silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "        # Determine optimal number of clusters based on highest silhouette score\n",
    "        optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because silhouette_scores starts from 2 clusters\n",
    "        return optimal_clusters\n",
    "\n",
    "    optimal_clusters = determine_optimal_clusters(X, max_clusters)\n",
    "    print(f'Optimal number of clusters: {optimal_clusters}')\n",
    "\n",
    "    # Perform clustering with the optimal number of clusters\n",
    "    kmeans = MiniBatchKMeans(n_clusters=optimal_clusters, random_state=42, batch_size=batch_size)\n",
    "    kmeans.fit(X)\n",
    "    df['cluster_no'] = kmeans.labels_\n",
    "\n",
    "    # Calculate average similarity score for each cluster using sparse matrix operations\n",
    "    cluster_scores = []\n",
    "    cluster_avg_similarities = []\n",
    "\n",
    "    for cluster in range(optimal_clusters):\n",
    "        indices = np.where(kmeans.labels_ == cluster)[0]\n",
    "        if len(indices) > 1:\n",
    "            cluster_sim = cosine_similarity(X[indices])\n",
    "            avg_sim = cluster_sim[np.triu_indices(len(indices), k=1)].mean()\n",
    "        else:\n",
    "            avg_sim = 1.0  # If there's only one member in the cluster, similarity is 1\n",
    "        cluster_scores.extend([avg_sim] * len(indices))\n",
    "        cluster_avg_similarities.append(avg_sim)\n",
    "\n",
    "    df['cluster_score'] = cluster_scores\n",
    "\n",
    "    # calculate the PCA metric\n",
    "    X = TfidfVectorizer(max_features=1000).fit_transform(df['text_content'])\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    pca_components = pca.fit_transform(X.toarray())\n",
    "    df['pca1'] = pca_components[:, 0]\n",
    "    df['pca2'] = pca_components[:, 1]\n",
    "\n",
    "    # Drop the temporary cleaned_text column\n",
    "    df = df.drop(columns=['cleaned_text'])\n",
    "\n",
    "    # Return the updated DataFrame and cluster average similarities\n",
    "    return df, cluster_avg_similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b5ad56-17b0-4e69-8d1e-bb09ad43b14c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T12:46:58.996956Z",
     "iopub.status.busy": "2024-08-19T12:46:58.996494Z",
     "iopub.status.idle": "2024-08-19T12:46:59.215187Z",
     "shell.execute_reply": "2024-08-19T12:46:59.214552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "7317\n",
      "79668bb9-74cb-466e-85dc-0e17c51196a0\n",
      "None\n",
      "7317\n",
      "update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = 7317\n",
      "update 1 rows\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# preload the jobs\n",
    "#\n",
    "strSQL = \"\"\"\n",
    "insert \tinto screen_analisis_ai (jenis_analisa, tanggal, created, \n",
    "createdby_id, active, status, monitoring_id )\n",
    "select \t'100',now(), now(), 1, 1, 1, a.id \n",
    "from \tmonitoring a\n",
    "where \tstatus = 3 \n",
    "\t\tand cast(a.id as varchar) \n",
    "        not in (select monitoring_id from \n",
    "\t\t\t    screen_analisis_ai where jenis_analisa = '100')\n",
    "order by updated_date desc \n",
    "limit 1\n",
    "\"\"\"\n",
    "\n",
    "data_connector.execute_query_psql(strSQL)\n",
    "\n",
    "strSQL = \"\"\"\n",
    "select \ta.id as jobsid,\n",
    "        a.*,\n",
    "\t\td.id as key_monitoring_media_social, \n",
    "\t\te.id as key_monitoring_media_online,\n",
    "\t\tc.*\n",
    "from screen_analisis_ai a \n",
    "\tinner join monitoring_search c \n",
    "\t\ton cast(c.monitoring_id as varchar) = cast(a.monitoring_id as varchar) \n",
    "\tinner join monitoring_media_social d\n",
    "\t\ton d.monitoring_id = c.monitoring_id\n",
    "\tleft outer join monitoring_media_online e\n",
    "\t\ton e.monitoring_id = c.monitoring_id \n",
    "where a.jenis_analisa = '100'\n",
    "and a.status = 1\n",
    "order by a.created desc \n",
    "limit 1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df_job = data_connector.execute_query_psql(strSQL)\n",
    "if len(df_job) == 0:\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "    \n",
    "similarity_treshold = 0.9\n",
    "i_process_id = df_job['jobsid'][0]\n",
    "screen_name = ''\n",
    "database_keyword_id = df_job['key_monitoring_media_social'][0]\n",
    "social_media_monitoring_id = df_job['key_monitoring_media_social'][0]\n",
    "media_online_monitoring_id = df_job['key_monitoring_media_online'][0]\n",
    "\n",
    "# print(database_keyword_id)\n",
    "print(similarity_treshold)\n",
    "print(i_process_id)\n",
    "print(social_media_monitoring_id)\n",
    "print(media_online_monitoring_id)\n",
    "\n",
    "\n",
    "# Prepare SQL Statement\n",
    "print(i_process_id)\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = %s\"\n",
    "sql = sql.replace('%s', str(i_process_id))\n",
    "\n",
    "print(sql)\n",
    "row_count = data_connector.execute_query_psql(sql)\n",
    "print('update ' + str(row_count) + ' rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8cc6ee8-8028-41c7-b0e5-8f7a47820d8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T12:46:59.217979Z",
     "iopub.status.busy": "2024-08-19T12:46:59.217466Z",
     "iopub.status.idle": "2024-08-19T12:46:59.366196Z",
     "shell.execute_reply": "2024-08-19T12:46:59.365665Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 12:46:59,307 - ERROR - Error processing Twitter data: dataset population less than 100 (0), stopping jobs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Processed Data: 0\n"
     ]
    }
   ],
   "source": [
    "# prepare function to record result\n",
    "def record_result(result_df, cluster_avg_similarities, i_platform_id):\n",
    "    # result_df, cluster_avg_similarities\n",
    "    for i in range(0, len(result_df)):\n",
    "        ssql = \"\"\"\n",
    "        insert into ret_cluster_result_monitor (ref_id, cluster_no, platform_id,\n",
    "        job_id, cluster_score,pca_1, pca_2) \n",
    "        values ('%s',%s, %s, %s, %s, '%s', '%s')\n",
    "        \"\"\"\n",
    "\n",
    "        # builds str \n",
    "        ssql = ssql % (str(result_df['ref_id'][i]), str(result_df['cluster_no'][i]), str(i_platform_id), str(i_process_id), str(cluster_avg_similarities[result_df['cluster_no'][i]]), str(result_df['pca1'][i]), str(result_df['pca2'][i]))\n",
    "        # print(ssql)\n",
    "        data_connector.execute_query_psql(ssql)\n",
    "\n",
    "# Processing jobs for each platform\n",
    "import logging\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define a function to log errors to PostgreSQL\n",
    "def log_error_to_postgresql(message):\n",
    "    ssql = \"\"\"\n",
    "    insert into ret_analysis_parameter (job_id, param_id, param_name ,param_value)\n",
    "    values (%s, %s, '%s', '%s')\n",
    "    \"\"\" % (i_process_id, 99, 'ERR', message)\n",
    "    \n",
    "    # print(ssql)\n",
    "    data_connector.execute_query_psql(ssql)\n",
    "\n",
    "# Function to check the DataFrame size and throw an error if it contains less than 100 rows\n",
    "def check_dataset_size(df):\n",
    "    if len(df) < 100:\n",
    "        raise ValueError(\"dataset population less than 100 (%s), stopping jobs\" % str(len(df)))\n",
    "\n",
    "# Processing jobs for each platform\n",
    "# 10 = tiktok\n",
    "# 20 = youtube\n",
    "# 30 = instagram_post\n",
    "# 40 = facebook_post\n",
    "# 50 = google_result\n",
    "# 60 = twitter_tweets\n",
    "\n",
    "iRowCount = 0\n",
    "\n",
    "try:\n",
    "    if df_job['is_tiktok'][0]:\n",
    "        sql_query = '''\n",
    "        select id as ref_id, \"desc\" as text_content \n",
    "        from tiktok \n",
    "        where monitoring_id = '%s' -- 10\n",
    "        '''\n",
    "        i_platform_id = 10\n",
    "        df = data_connector.execute_query_psql(sql_query % (social_media_monitoring_id))\n",
    "        check_dataset_size(df)\n",
    "        if len(df) != 0:\n",
    "            result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "            record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "            iRowCount = iRowCount + len(df)\n",
    "except Exception as e:\n",
    "    error_message = f\"Error processing TikTok data: {str(e)}\"\n",
    "    logging.error(error_message)\n",
    "    log_error_to_postgresql(error_message)\n",
    "\n",
    "try:\n",
    "    if df_job['is_youtube'][0]:\n",
    "        sql_query = '''\n",
    "        select id as ref_id, title as text_content\n",
    "        from youtube \n",
    "        where monitoring_id = '%s' -- 20\n",
    "        '''\n",
    "        i_platform_id = 20\n",
    "        df = data_connector.execute_query_psql(sql_query % (social_media_monitoring_id))\n",
    "        check_dataset_size(df)\n",
    "        if len(df) != 0:\n",
    "            result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "            record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "            iRowCount = iRowCount + len(df)\n",
    "except Exception as e:\n",
    "    error_message = f\"Error processing YouTube data: {str(e)}\"\n",
    "    logging.error(error_message)\n",
    "    log_error_to_postgresql(error_message)\n",
    "\n",
    "try:\n",
    "    if df_job['is_instagram'][0]:\n",
    "        sql_query = '''\n",
    "        select id as ref_id, content as text_content\n",
    "        from instagram_post \n",
    "        where monitoring_id = '%s' -- 30\n",
    "        '''\n",
    "        i_platform_id = 30\n",
    "        df = data_connector.execute_query_psql(sql_query % (social_media_monitoring_id))\n",
    "        check_dataset_size(df)\n",
    "        if len(df) != 0:\n",
    "            result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "            record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "            iRowCount = iRowCount + len(df)\n",
    "except Exception as e:\n",
    "    error_message = f\"Error processing Instagram data: {str(e)}\"\n",
    "    logging.error(error_message)\n",
    "    log_error_to_postgresql(error_message)\n",
    "\n",
    "try:\n",
    "    if df_job['is_facebook'][0]:\n",
    "        sql_query = '''\n",
    "        select id as ref_id, \"description\" as text_content\n",
    "        from facebook_post \n",
    "        where monitoring_id = '%s' -- 40\n",
    "        and length(trim(\"description\")) > 0\n",
    "        '''\n",
    "        i_platform_id = 40\n",
    "        df = data_connector.execute_query_psql(sql_query % (social_media_monitoring_id))\n",
    "        check_dataset_size(df)\n",
    "        if len(df) != 0:\n",
    "            result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "            record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "            iRowCount = iRowCount + len(df)\n",
    "except Exception as e:\n",
    "    error_message = f\"Error processing Facebook data: {str(e)}\"\n",
    "    logging.error(error_message)\n",
    "    log_error_to_postgresql(error_message)\n",
    "\n",
    "try:\n",
    "    if df_job['is_google'][0]:\n",
    "        sql_query = '''\n",
    "            select id as ref_id, \"description\" as text_content\n",
    "            from google_result \n",
    "            where monitoring_id = '%s' -- 50\n",
    "        '''\n",
    "        i_platform_id = 50\n",
    "        df = data_connector.execute_query_psql(sql_query % (media_online_monitoring_id))\n",
    "        check_dataset_size(df)\n",
    "        if len(df) != 0:\n",
    "            result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "            record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "            iRowCount = iRowCount + len(df)\n",
    "except Exception as e:\n",
    "    error_message = f\"Error processing Google data: {str(e)}\"\n",
    "    logging.error(error_message)\n",
    "    log_error_to_postgresql(error_message)\n",
    "\n",
    "try:\n",
    "    if df_job['is_twitter'][0]:\n",
    "        sql_query = '''\n",
    "        select id as ref_id, tweet as text_content\n",
    "        from twitter_tweets \n",
    "        where monitoring_id = '%s' -- 60\n",
    "        '''\n",
    "        i_platform_id = 60\n",
    "        df = data_connector.execute_query_psql(sql_query % (social_media_monitoring_id))\n",
    "        check_dataset_size(df)\n",
    "        if len(df) != 0:\n",
    "            result_df, cluster_avg_similarities = cluster_and_return(df)\n",
    "            record_result(result_df, cluster_avg_similarities, i_platform_id)\n",
    "            iRowCount = iRowCount + len(df)\n",
    "except Exception as e:\n",
    "    error_message = f\"Error processing Twitter data: {str(e)}\"\n",
    "    logging.error(error_message)\n",
    "    log_error_to_postgresql(error_message)\n",
    "\n",
    "print(\"Total Processed Data: \" + str(iRowCount))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a1e46f-f730-412b-b2e0-d066cbe5d6df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T12:46:59.368684Z",
     "iopub.status.busy": "2024-08-19T12:46:59.368512Z",
     "iopub.status.idle": "2024-08-19T12:46:59.420614Z",
     "shell.execute_reply": "2024-08-19T12:46:59.420058Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# code for image clustering\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from urllib.parse import urlsplit, urlunsplit\n",
    "import data_connector\n",
    "\n",
    "# Step 1: Data Loading and Cleaning\n",
    "def load_image(url, url_prefix, size=(512, 512)):\n",
    "    # Remove the filename extension\n",
    "    parts = list(urlsplit(url))\n",
    "    path = parts[2]\n",
    "    path = '/'.join(part.rsplit('.', 1)[0] for part in path.split('/'))\n",
    "    parts[2] = path\n",
    "    url = urlunsplit(parts)\n",
    "\n",
    "    # Add the URL prefix\n",
    "    full_url = url_prefix + url\n",
    "    print(full_url)\n",
    "\n",
    "    response = requests.get(full_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.resize(size)\n",
    "    img = np.array(img)\n",
    "    if img.shape[2] == 4:\n",
    "        img = img[:, :, :3]  # Remove alpha channel if present\n",
    "    img = img / 255.0  # Normalize\n",
    "    return img\n",
    "\n",
    "# Step 2: Feature Extraction\n",
    "def extract_features(img):\n",
    "    h, w, c = img.shape\n",
    "    return img.reshape(1, h * w * c)\n",
    "\n",
    "# Main function to execute the steps\n",
    "def main(df, url_column, url_prefix):\n",
    "    all_features = []\n",
    "    for url in df[url_column]:\n",
    "        img = load_image(url, url_prefix)\n",
    "        features = extract_features(img)\n",
    "        all_features.append(features)\n",
    "    all_features = np.vstack(all_features)\n",
    "\n",
    "    # Determine optimal clusters\n",
    "    optimal_clusters, inertia, silhouette_scores = determine_optimal_clusters(all_features)\n",
    "    if optimal_clusters is None:\n",
    "        print(\"Could not determine optimal clusters due to insufficient unique labels.\")\n",
    "        return []\n",
    "    \n",
    "    cluster_labels, kmeans = perform_clustering(all_features, optimal_clusters)\n",
    "    print_cluster_content(cluster_labels)\n",
    "    pca_result = visualize_clusters(all_features, cluster_labels)\n",
    "    output = prepare_output(cluster_labels, pca_result, all_features)\n",
    "    outputlist = prepare_output_final(output)\n",
    "    \n",
    "    # Calculate average distance from centroid for each cluster\n",
    "    average_distances = average_distance_from_centroid(outputlist)\n",
    "    score = normalize_distances(average_distances)\n",
    "\n",
    "    # output_final = prepare_output_final(output,score)\n",
    "\n",
    "    return output, optimal_clusters, inertia, silhouette_scores, outputlist, score\n",
    "\n",
    "# Step 3: Optimal Cluster Determination\n",
    "def determine_optimal_clusters(features):\n",
    "    inertia = []\n",
    "    silhouette_scores = []\n",
    "    range_n_clusters = list(range(2, min(10, len(features))))\n",
    "    for n_clusters in range_n_clusters:\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        cluster_labels = kmeans.fit_predict(features)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        if len(np.unique(cluster_labels)) > 1:\n",
    "            silhouette_scores.append(silhouette_score(features, cluster_labels))\n",
    "        else:\n",
    "            silhouette_scores.append(-1)\n",
    "    \n",
    "    if all(score == -1 for score in silhouette_scores):\n",
    "        return None, inertia, silhouette_scores\n",
    "    \n",
    "    optimal_clusters = range_n_clusters[np.argmax(silhouette_scores)]\n",
    "    return optimal_clusters, inertia, silhouette_scores\n",
    "\n",
    "# Step 4: Clustering\n",
    "def perform_clustering(features, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = kmeans.fit_predict(features)\n",
    "    return cluster_labels, kmeans\n",
    "\n",
    "# Step 5: Print Cluster Content\n",
    "def print_cluster_content(cluster_labels):\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    for label in unique_labels:\n",
    "        print(f\"Cluster {label}: {np.sum(cluster_labels == label)} members\")\n",
    "\n",
    "# Step 6: PCA Scatter Plot\n",
    "def visualize_clusters(features, cluster_labels):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(features)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=cluster_labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title('Clusters Visualization with PCA')\n",
    "    # plt.show()\n",
    "    return pca_result\n",
    "\n",
    "# Step 7: Prepare Output\n",
    "def prepare_output(cluster_labels, pca_result, features):\n",
    "    output = []\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    for label in unique_labels:\n",
    "        members = np.where(cluster_labels == label)[0].tolist()\n",
    "        cluster_pca1 = pca_result[cluster_labels == label, 0].tolist()\n",
    "        cluster_pca2 = pca_result[cluster_labels == label, 1].tolist()\n",
    "        cluster_features = features[cluster_labels == label]\n",
    "        # if len(np.unique(cluster_labels)) > 1:\n",
    "        #     similarity_score = silhouette_score(cluster_features, [label] * len(cluster_features))\n",
    "        # else:\n",
    "        #     similarity_score = -1\n",
    "        similarity_score = 0\n",
    "        output.append({\n",
    "            'cluster': label,\n",
    "            'members': members,\n",
    "            'pca1': cluster_pca1,\n",
    "            'pca2': cluster_pca2,\n",
    "            'similarity_score': similarity_score\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_output = pd.DataFrame(output)\n",
    "    \n",
    "    # Convert DataFrame to list\n",
    "    list_output = df_output.to_numpy().tolist()\n",
    "    \n",
    "    return list_output\n",
    "\n",
    "# Function to compute the centroid of a cluster\n",
    "def compute_centroid(points):\n",
    "    points = np.array(points)\n",
    "    return np.mean(points, axis=0)\n",
    "\n",
    "# Function to calculate the Euclidean distance\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "\n",
    "# Function to compute the average distance from the centroid for each cluster\n",
    "def average_distance_from_centroid(data):\n",
    "    clusters = defaultdict(list)\n",
    "    \n",
    "    # Grouping points by cluster number\n",
    "    for entry in data:\n",
    "        _, _, cluster_number, PCA1, PCA2 = entry\n",
    "        clusters[cluster_number].append((PCA1, PCA2))\n",
    "    \n",
    "    average_distances = {}\n",
    "    \n",
    "    for cluster_number, points in clusters.items():\n",
    "        centroid = compute_centroid(points)\n",
    "        total_distance = sum(euclidean_distance(point, centroid) for point in points)\n",
    "        average_distance = total_distance / len(points)\n",
    "        average_distances[cluster_number] = average_distance\n",
    "    \n",
    "    return average_distances\n",
    "\n",
    "# Function to normalize the distances to a score between 0 and 1\n",
    "def normalize_distances(average_distances):\n",
    "    min_distance = min(average_distances.values())\n",
    "    max_distance = max(average_distances.values())\n",
    "    \n",
    "    normalized_scores = {}\n",
    "    for cluster_number, avg_distance in average_distances.items():\n",
    "        if max_distance == min_distance:  # Avoid division by zero\n",
    "            normalized_scores[cluster_number] = 0.0\n",
    "            normalized_scores[cluster_number] = 1 - normalized_scores[cluster_number]\n",
    "        else:\n",
    "            normalized_scores[cluster_number] = (avg_distance - min_distance) / (max_distance - min_distance)\n",
    "            normalized_scores[cluster_number] = 1 - normalized_scores[cluster_number]\n",
    "    \n",
    "    return normalized_scores\n",
    "\n",
    "def prepare_output_final(output):\n",
    "    # create list of:\n",
    "    memberIdx = 0\n",
    "    outputlist = []\n",
    "    \n",
    "    for i in range(len(output)):\n",
    "        # 0: cluster number\n",
    "        # 1: member\n",
    "        # 2: PCA1\n",
    "        # 3: PCA2\n",
    "        # 4: Sim Score ->> from score list\n",
    "        \n",
    "        # iterate thru member\n",
    "        for j in range(len(output[i][1])):\n",
    "            memberIndex = output[i][1][j]\n",
    "            PCA1 = output[i][2][j]\n",
    "            PCA2 = output[i][3][j]\n",
    "            clusterId = output[i][0]\n",
    "            key_id = result_df['key_id'][memberIndex]\n",
    "            \n",
    "            # print(key_id, memberIndex, clusterId, PCA1, PCA2)\n",
    "            outputlist.append([key_id, memberIndex, clusterId, PCA1, PCA2])\n",
    "    \n",
    "    return outputlist\n",
    "\n",
    "def recording_output(output_final, score, platform_id):\n",
    "# id, ref_id, cluster_no, platform_id, job_id\n",
    "    query_sql = \"\"\"\n",
    "    insert into ret_cluster_image_result_monitor \n",
    "    (ref_id, cluster_no, platform_id, job_id, pca1, pca2, score)\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(len(output_final)):\n",
    "        sim_score = score[output_final[i][2]]\n",
    "        valuesql = \"values ( '\" + str(output_final[i][0]) + \"','\" + str(output_final[i][2])  + \"','\" + str(platform_id) + \"','\" + str(i_process_id) + \"','\"+ str(output_final[i][3]) +\"','\" + str(output_final[i][4]) + \"','\" + str(sim_score) + \"' )\"\n",
    "        ssql = query_sql + valuesql\n",
    "        print(ssql)\n",
    "        data_connector.execute_query_psql(ssql)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a4a785-0f32-42c3-9a10-82b41a7992da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T12:46:59.423285Z",
     "iopub.status.busy": "2024-08-19T12:46:59.422905Z",
     "iopub.status.idle": "2024-08-19T12:47:00.220274Z",
     "shell.execute_reply": "2024-08-19T12:47:00.219749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    select \ta.id as key_id,\n",
      "            b.filename as filename\n",
      "    from \ttwitter_tweets a inner join media_files b \n",
      "            on a.id = b.id \n",
      "    where a.monitoring_id = '79668bb9-74cb-466e-85dc-0e17c51196a0'\n",
      "    \n",
      "https://disestages.com/api/media/photo/download/twitter/\n"
     ]
    }
   ],
   "source": [
    "img_prefix_http = \"https://disestages.com/api/media/photo/download/%s/\"\n",
    "\n",
    "# Processing jobs for each platform\n",
    "# 10 = tiktok\n",
    "# 20 = youtube\n",
    "# 30 = instagram_post\n",
    "# 40 = facebook_post\n",
    "# 50 = google_result\n",
    "# 60 = twitter_tweets\n",
    "\n",
    "iRowCount = 0\n",
    "\n",
    "if (df_job['is_twitter'][0]):\n",
    "    ssql = \"\"\"\n",
    "    select \ta.id as key_id,\n",
    "            b.filename as filename\n",
    "    from \ttwitter_tweets a inner join media_files b \n",
    "            on a.id = b.id \n",
    "    where a.monitoring_id = '%s'\n",
    "    \"\"\"\n",
    "    ssql = ssql % (social_media_monitoring_id)\n",
    "    print(ssql)\n",
    "    img_prefix_http = \"https://disestages.com/api/media/photo/download/%s/\"\n",
    "    img_prefix_http = img_prefix_http % \"twitter\"\n",
    "    print(img_prefix_http)\n",
    "    \n",
    "    result_df = data_connector.execute_query_psql(ssql)\n",
    "    if (len(result_df)) > 0:\n",
    "        # processing\n",
    "        output, optimal_clusters, inertia, silhouette_scores, output_final, score = main(result_df, 'filename', img_prefix_http)\n",
    "        # Recording Result\n",
    "        recording_output(output_final, score, 60)\n",
    "        iRowCount = iRowCount + len(output_final)\n",
    "\n",
    "if (df_job['is_tiktok'][0]):\n",
    "    ssql = \"\"\"\n",
    "    select \ta.id as key_id,\n",
    "            b.filename as filename\n",
    "    from \ttiktok a inner join media_files_tiktok b \n",
    "            on a.id = b.tiktok_id \n",
    "    where a.monitoring_id = '%s'\n",
    "    \"\"\"\n",
    "    ssql = ssql % (social_media_monitoring_id)\n",
    "    print(ssql)\n",
    "    img_prefix_http = \"https://disestages.com/api/media/photo/download/%s/\"\n",
    "    img_prefix_http = img_prefix_http % \"tiktok\"\n",
    "    print(img_prefix_http)\n",
    "    \n",
    "    result_df = data_connector.execute_query_psql(ssql)\n",
    "    if (len(result_df)) > 0:\n",
    "        # processing\n",
    "        output, optimal_clusters, inertia, silhouette_scores, output_final, score = main(result_df, 'filename', img_prefix_http)\n",
    "    \n",
    "        # Recording Result\n",
    "        recording_output(output_final, score, 10)\n",
    "        iRowCount = iRowCount + len(output_final)\n",
    "    \n",
    "elif (df_job['is_facebook'][0]):\n",
    "    ssql = \"\"\"\n",
    "    select \ta.id as key_id,\n",
    "            b.filename as filename\n",
    "    from \tfacebook_post a inner join media_files_facebook b\n",
    "    \t\ton a.id = b.facebook_id \n",
    "    where a.monitoring_id = '%s'\n",
    "    \"\"\"\n",
    "    ssql = ssql % (social_media_monitoring_id)\n",
    "    print(ssql)\n",
    "\n",
    "    img_prefix_http = \"https://disestages.com/api/media/photo/download/%s/\"\n",
    "    img_prefix_http = img_prefix_http % \"facebook\"\n",
    "    print(img_prefix_http)\n",
    "    \n",
    "    result_df = data_connector.execute_query_psql(ssql)\n",
    "    \n",
    "    if (len(result_df)) > 0:\n",
    "        # processing\n",
    "        output, optimal_clusters, inertia, silhouette_scores, output_final, score = main(result_df, 'filename', img_prefix_http)\n",
    "    \n",
    "        # Recording Result\n",
    "        recording_output(output_final, score, 40)\n",
    "        iRowCount = iRowCount + len(output_final)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458c2647-22c0-4ef5-9333-8b4b208c5d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T12:47:00.222851Z",
     "iopub.status.busy": "2024-08-19T12:47:00.222359Z",
     "iopub.status.idle": "2024-08-19T12:47:00.390620Z",
     "shell.execute_reply": "2024-08-19T12:47:00.390079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting result finished\n"
     ]
    }
   ],
   "source": [
    "# Finishing Jobs\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "data_connector.execute_query_psql(sql % (i_process_id, 1, \"'#Content Processed'\",iRowCount))\n",
    "\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"update ret_analysis_header set datetime_finish = NOW() where job_id = %s\"\n",
    "# Executing query\n",
    "data_connector.execute_query_psql(sql % (i_process_id) )\n",
    "\n",
    "sql = \"update screen_analisis_ai set status = 3, duration = EXTRACT(EPOCH FROM (now() - start_process)), end_process = NOW() where id = %s\"\n",
    "data_connector.execute_query_psql(sql % (i_process_id))\n",
    "\n",
    "print('inserting result finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
