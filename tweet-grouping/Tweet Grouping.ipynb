{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liked-washington",
   "metadata": {},
   "source": [
    "# Hashtag Grouping on Twitter\n",
    "\n",
    "Input: \n",
    "    Hastag scrap result from cekmedsos_database.vw_ttidata\n",
    "\n",
    "Output: \n",
    "    Mapping tables to group each hashtag entry with similarity\n",
    "    \n",
    "\n",
    "What we need to do....\n",
    "1. Load the table entry from mySQL into python\n",
    "2. Read the entry\n",
    "\n",
    "Pre-requisite\n",
    "pip install wheel\n",
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-hayes",
   "metadata": {},
   "source": [
    "### 1. Create Connection to mySQL\n",
    "\n",
    "ref to this page:\n",
    "    [How to Use Python with mySQL in Jupyter](https://medium.com/@tattwei46/how-to-use-python-with-mysql-79304bee8753)\n",
    "    \n",
    "first, we need to install mysql connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db6b8d0-5727-45b7-97ce-d8af57b17764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting click (from nltk)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m188.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m300.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m452.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m545.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1 regex-2023.8.8 tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd50544-8a0c-4c09-883b-2363c755d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe790fa2-dd3b-4837-a9d6-d28b988e4258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /home/qudoco/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-ticket",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# This value is pass from API\n",
    "# --------------------------------------------------------------------------------\n",
    "# i_process_id = 1\n",
    "# print(i_process_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-spiritual",
   "metadata": {},
   "source": [
    "### Get parameter from API Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /GET /cluster_analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-palestinian",
   "metadata": {},
   "source": [
    "### Input as proces started\n",
    "Record header and parameter information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fresh-video",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(i_process_id)\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host='202.157.176.225',\n",
    "                             user='cekmedsos_db',\n",
    "                             password='kuku838485*#',\n",
    "                             database='cekmedsos_database',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Query parameter of the jobs\n",
    "sql = \"select hastag, parameter from screen_analisis_ai where id = %s\"\n",
    "cursor.execute(sql, i_process_id)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "database_keyword_id = result[0]['hastag']\n",
    "similarity_treshold = result[0]['parameter']\n",
    "screen_name = ''\n",
    "\n",
    "#\n",
    "# Create Header Record\n",
    "sql = \"insert into ret_analysis_header (job_id, datetime_start, user_id) values (%s, %s, %s)\"\n",
    "# Execute the query\n",
    "print(i_process_id)\n",
    "cursor.execute(sql, (str(i_process_id), datetime.now(), 1 ))\n",
    "\n",
    "#\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "cursor.execute(sql, (i_process_id, 1, 'Similarity Treshold', similarity_treshold))\n",
    "cursor.execute(sql, (i_process_id, 1, 'DB_ID', database_keyword_id))\n",
    "\n",
    "# Commit Record\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-lighting",
   "metadata": {},
   "source": [
    "#### Starting process, \n",
    "Run query against RDBMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weekly-jonathan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select id, tweet, tweet_date_time from ret_tweet where db_id = \"2907\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1359291070861856768</td>\n",
       "      <td>Orang-orang yang beriman dan beramal saleh, ba...</td>\n",
       "      <td>2021-02-10 07:00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1359291114608500737</td>\n",
       "      <td>Cuma islam yang serius membangun peradaban mas...</td>\n",
       "      <td>2021-02-10 07:00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1359291133088518146</td>\n",
       "      <td>An Ottoman Officer, 1898  Bir Osmanlı Subayı, ...</td>\n",
       "      <td>2021-02-10 07:00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1359291332703920128</td>\n",
       "      <td>Barangsiapa membaca satu huruf dari Al Qur'an ...</td>\n",
       "      <td>2021-02-10 07:01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359291390425853953</td>\n",
       "      <td>Barangsiapa akhir ucapannya \"Laa ilaaha illall...</td>\n",
       "      <td>2021-02-10 07:01:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1359291070861856768  Orang-orang yang beriman dan beramal saleh, ba...   \n",
       "1  1359291114608500737  Cuma islam yang serius membangun peradaban mas...   \n",
       "2  1359291133088518146  An Ottoman Officer, 1898  Bir Osmanlı Subayı, ...   \n",
       "3  1359291332703920128  Barangsiapa membaca satu huruf dari Al Qur'an ...   \n",
       "4  1359291390425853953  Barangsiapa akhir ucapannya \"Laa ilaaha illall...   \n",
       "\n",
       "      tweet_date_time  \n",
       "0 2021-02-10 07:00:20  \n",
       "1 2021-02-10 07:00:30  \n",
       "2 2021-02-10 07:00:35  \n",
       "3 2021-02-10 07:01:22  \n",
       "4 2021-02-10 07:01:36  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_query_string = 'select id, tweet, tweet_date_time from ret_tweet where '\n",
    "\n",
    "if (screen_name != ''):\n",
    "    # print('use screen name')\n",
    "    s_query_string = s_query_string + 'screen_name = \"' + screen_name + '\" and db_id = \"' + str(database_keyword_id) + '\"'\n",
    "else:\n",
    "    # print('no use')\n",
    "    s_query_string = s_query_string + 'db_id = \"' + str(database_keyword_id) + '\"'\n",
    "    \n",
    "print(s_query_string)\n",
    "df = pd.read_sql(s_query_string, con=connection)\n",
    "\n",
    "# Close Connection\n",
    "connection.close()\n",
    "\n",
    "# see result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-waste",
   "metadata": {},
   "source": [
    "### 2. Try to pre-process all the text\n",
    "\n",
    "target-> tokenizing into another dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chemical-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1359291070861856768</td>\n",
       "      <td>orangorang yang beriman dan beramal saleh bagi...</td>\n",
       "      <td>2021-02-10 07:00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1359291114608500737</td>\n",
       "      <td>cuma islam yang serius membangun peradaban mas...</td>\n",
       "      <td>2021-02-10 07:00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1359291133088518146</td>\n",
       "      <td>an ottoman officer bir osmanlı subayı islamsel...</td>\n",
       "      <td>2021-02-10 07:00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1359291332703920128</td>\n",
       "      <td>barangsiapa membaca satu huruf dari al quran m...</td>\n",
       "      <td>2021-02-10 07:01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359291390425853953</td>\n",
       "      <td>barangsiapa akhir ucapannya laa ilaaha illalla...</td>\n",
       "      <td>2021-02-10 07:01:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1359291070861856768  orangorang yang beriman dan beramal saleh bagi...   \n",
       "1  1359291114608500737  cuma islam yang serius membangun peradaban mas...   \n",
       "2  1359291133088518146  an ottoman officer bir osmanlı subayı islamsel...   \n",
       "3  1359291332703920128  barangsiapa membaca satu huruf dari al quran m...   \n",
       "4  1359291390425853953  barangsiapa akhir ucapannya laa ilaaha illalla...   \n",
       "\n",
       "      tweet_date_time  \n",
       "0 2021-02-10 07:00:20  \n",
       "1 2021-02-10 07:00:30  \n",
       "2 2021-02-10 07:00:35  \n",
       "3 2021-02-10 07:01:22  \n",
       "4 2021-02-10 07:01:36  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    \n",
    "    return strOut\n",
    "# end Text Preprocessing\n",
    "\n",
    "\n",
    "# Apply to data frame\n",
    "df['tweet'] = df['tweet'].apply(text_preproc)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-power",
   "metadata": {},
   "source": [
    "### Finish preprocessing, Tokenized\n",
    "Next step, is to output to new column for tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "classical-position",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Load stopwords\n",
    "sw = stopwords.words('indonesian')\n",
    "\n",
    "# apply tokenize\n",
    "df['tokenized_tweet'] = df.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
    "# apply stopword removal\n",
    "df['sw'] = df.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "\n",
    "# print((df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-commerce",
   "metadata": {},
   "source": [
    "### Define function to calculate similarity\n",
    "Function return similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "close-sessions",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(X_set, Y_set):\n",
    "# Program to measure the similarity between  \n",
    "# two sentences using cosine similarity. \n",
    "\n",
    "    l1 =[];l2 =[]\n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector\n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    \n",
    "    return cosine\n",
    "\n",
    "def largest_in_col(arr,nCol):\n",
    "    # \n",
    "    # Find largest value of col nCol on 2D arr\n",
    "    #\n",
    "    \n",
    "    # init value\n",
    "    max_val = arr[0][nCol]\n",
    "    # also, remember index\n",
    "    row_index = 0\n",
    "    \n",
    "    for x in range(0, len(arr)):\n",
    "        if arr[x][nCol] > max_val:\n",
    "            max_val = arr[x][nCol]\n",
    "            row_index = x\n",
    "        \n",
    "    return max_val,row_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-appearance",
   "metadata": {},
   "source": [
    "### Try to using function\n",
    "using some of array cells, create n matrices\n",
    "\n",
    "1. take one tweet, compare to all data set\n",
    "2. flag 1 if similar\n",
    "3. take next tweet, if similar from prev tweet, skip\n",
    "4. if not similar, add counter, then proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unavailable-clearing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarity_treshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-08a8e0846c3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# multitasking.set_max_threads(multitasking.config[\"CPU_CORES\"] * 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_treshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcluster_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0ms_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'similarity_treshold' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import array as arr\n",
    "import multitasking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set to max CPU Cores\n",
    "# multitasking.set_max_threads(multitasking.config[\"CPU_CORES\"] * 5)\n",
    "\n",
    "st = similarity_treshold\n",
    "cluster_no = 1\n",
    "s_score = 0\n",
    "s_score_current = 0\n",
    "i_current_cluster = 0\n",
    "\n",
    "#create zero element array\n",
    "#col 0 => index base tweet\n",
    "#col 1 => cluster number\n",
    "#col 2 => similarity score\n",
    "base_tweet = []\n",
    "\n",
    "#proceed to compare to all tweet\n",
    "cosine_matrix = np.zeros(( len(df), len(df) ))\n",
    "\n",
    "# flag the mt\n",
    "with tqdm(total=( (len(df)*len(df)/2))-(len(df)/2))  as pbar:\n",
    "    for j in range(0, len(df)):\n",
    "        tweet_to_compare = df['sw'][j]\n",
    "\n",
    "        #check, is this second tweet?\n",
    "        if(j == 0):\n",
    "            #first tweet, add as cluster no #1\n",
    "            base_tweet.append([j,1,1.0])\n",
    "            i_current_cluster = base_tweet[0][1]\n",
    "\n",
    "        elif(j == 1):\n",
    "            #compare to prev tweet\n",
    "            s_score = calculate_similarity(tweet_to_compare, df['sw'][ base_tweet[0][0] ])\n",
    "\n",
    "            if(s_score < st):\n",
    "                #not similar\n",
    "                base_tweet.append([j,2,1])\n",
    "                i_current_cluster = base_tweet[j][1]\n",
    "\n",
    "        else:\n",
    "            #other else tweet\n",
    "            for x in range(0,len(base_tweet)):\n",
    "                #compare every element\n",
    "                s_score = calculate_similarity(tweet_to_compare, df['sw'][ base_tweet[x][0] ])\n",
    "                base_tweet[x][2] = s_score\n",
    "\n",
    "            if(largest_in_col(base_tweet,2)[0] < st):\n",
    "                #no similar, add as one new cluster\n",
    "                i_current_cluster = i_current_cluster + 1\n",
    "                base_tweet.append([j,i_current_cluster,largest_in_col(base_tweet,2)[0]])\n",
    "            else:\n",
    "                #determine cluster# from biggest similarity\n",
    "                i_current_cluster = base_tweet[(largest_in_col(base_tweet,2)[1])][1]\n",
    "\n",
    "        #proceed to compare to all tweet\n",
    "        for i in range(0,len(df)):\n",
    "            # update progress\n",
    "            if (j<i):\n",
    "                pbar.update(1)\n",
    "                s_score = calculate_similarity(tweet_to_compare, df['sw'][i])\n",
    "                if (s_score >= st):\n",
    "                    cosine_matrix[i,j] = i_current_cluster\n",
    "\n",
    "        \n",
    "pbar.close()\n",
    "print(cosine_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nearby-spokesman",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cosine_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6f60367555fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# sns.heatmap(cosine_matrix)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cosine_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# sns.heatmap(cosine_matrix)\n",
    "\n",
    "print(len(cosine_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-transparency",
   "metadata": {},
   "source": [
    "### Writing result to file\n",
    "Using NPZ format for efficiency, and try to load them after save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "valid-mobile",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "Save data complete .... \n"
     ]
    }
   ],
   "source": [
    "# save numpy array as npz file\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "\n",
    "# save to npy file\n",
    "savez_compressed('./output/df_380.npz',df)\n",
    "savez_compressed('./output/data_380.npz', cosine_matrix)\n",
    "savez_compressed('./output/data_380_base.npz', base_tweet)\n",
    "\n",
    "print(len(cosine_matrix))\n",
    "print('Save data complete .... ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-tennessee",
   "metadata": {},
   "source": [
    "### How to save to rdbms?\n",
    "save the header data, meta data of the process\n",
    "[ret_analysis_header]\n",
    "- JobID\n",
    "- user initiated\n",
    "- Time Started\n",
    "- Time End\n",
    "\n",
    "Process Parameter\n",
    "[ret_analysis_parameter]\n",
    "- JobID\n",
    "- Param Name\n",
    "- Param Value\n",
    "    - ST Value\n",
    "    - DB ID\n",
    "    - Screen Name\n",
    "    - How Many Tweet analyzed\n",
    "\n",
    "save detail cluster information\n",
    "[ret_base_tweet]\n",
    "- JobID\n",
    "- Tweet Base# --> tweet ID\n",
    "- Cluster#\n",
    "\n",
    "save detail calculation result, structure\n",
    "[ret_cluster_result]\n",
    "- JobID\n",
    "- TweetID\n",
    "- Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tribal-intent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished inserting base tweet record\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host='202.157.176.225',\n",
    "                             user='cekmedsos_db',\n",
    "                             password='kuku838485*#',\n",
    "                             database='cekmedsos_database',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Base Tweet Record\n",
    "sql = \"insert into ret_base_tweet (job_id, tweet_id, cluster_id) values (%s, %s, %s)\"\n",
    "\n",
    "## inserting base tweet\n",
    "for i in range(0,len(base_tweet)):\n",
    "    # Execute the query\n",
    "    cursor.execute(sql, (i_process_id, df['id'][i], base_tweet[i][1]))\n",
    "    \n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('finished inserting base tweet record')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-kennedy",
   "metadata": {},
   "source": [
    "### Record cluster result\n",
    "into table ret_cluster_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "painted-nylon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "1\n",
      "finished inserting cluster data\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host='202.157.176.225',\n",
    "                             user='cekmedsos_db',\n",
    "                             password='kuku838485*#',\n",
    "                             database='cekmedsos_database',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"insert into ret_cluster_result (job_id, tweet_id, cluster_no) values (%s, %s, %s)\"\n",
    "\n",
    "print(len(cosine_matrix))\n",
    "print(i_process_id)\n",
    "\n",
    "# initiate cluster number\n",
    "i_cluster_no_save = 0\n",
    "temp_val = 0\n",
    "                 \n",
    "## inserting tweet cluster\n",
    "for i in range(0, len(cosine_matrix)):\n",
    "    \n",
    "    # find value in this particular row\n",
    "    for j in range(0,len(cosine_matrix[i])):\n",
    "        \n",
    "        # print(cosine_matrix[i][j])\n",
    "        temp_val = cosine_matrix[i][j]\n",
    "        \n",
    "        if(temp_val != 0):\n",
    "            i_cluster_no_save = temp_val\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(sql, (i_process_id, df['id'][i], i_cluster_no_save))\n",
    "    \n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('finished inserting cluster data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-foundation",
   "metadata": {},
   "source": [
    "### Record finish time\n",
    "update table ret_analysis_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mighty-florence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "job finished\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host='202.157.176.225',\n",
    "                             user='cekmedsos_db',\n",
    "                             password='kuku838485*#',\n",
    "                             database='cekmedsos_database',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "cursor.execute(sql, (i_process_id, 1, '#Tweet Processed',len(df)))\n",
    "\n",
    "#\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"update ret_analysis_header set datetime_finish = %s where job_id = %s\"\n",
    "# Executing query\n",
    "cursor.execute(sql, (datetime.now(),i_process_id) )\n",
    "\n",
    "print(i_process_id)\n",
    "\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('job finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-tooth",
   "metadata": {},
   "source": [
    "### Next thing todo list ...\n",
    "1. Create data structure to record below things\n",
    "    Job Header\n",
    "    - JobID\n",
    "    - Similarity Treshold\n",
    "    - Dataset Parameter\n",
    "        - By User\n",
    "        - By Keyword\n",
    "    - Running time start\n",
    "    - Running time end\n",
    "    \n",
    "    Dataset Result\n",
    "        - base_tweet\n",
    "        - cosine_matrix\n",
    "        \n",
    "2. Push the result data into RDBMS Server, in this case is mySQL\n",
    "    - upload csv file to mySQL\n",
    "    - import to database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
