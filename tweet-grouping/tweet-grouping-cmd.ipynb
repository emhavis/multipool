{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liked-washington",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Hashtag Grouping on Twitter\n",
    "\n",
    "Input: \n",
    "    Hastag scrap result from cekmedsos_database.vw_ttidata\n",
    "\n",
    "Output: \n",
    "    Mapping tables to group each hashtag entry with similarity\n",
    "    \n",
    "\n",
    "What we need to do....\n",
    "1. Load the table entry from mySQL into python\n",
    "2. Read the entry\n",
    "\n",
    "Pre-requisite\n",
    "pip install wheel\n",
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-hayes",
   "metadata": {},
   "source": [
    "### 1. Create Connection to mySQL\n",
    "\n",
    "ref to this page:\n",
    "    [How to Use Python with mySQL in Jupyter](https://medium.com/@tattwei46/how-to-use-python-with-mysql-79304bee8753)\n",
    "    \n",
    "first, we need to install mysql connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "danish-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "\n",
    "# db parameters\n",
    "_conn_host='202.157.185.40'\n",
    "_conn_user='cekmedsos_db'\n",
    "_conn_password='282E~f0si'\n",
    "_conn_database='cekmedsos_database'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-palestinian",
   "metadata": {},
   "source": [
    "### Input as proces started\n",
    "Record header and parameter information\n",
    "\n",
    " * Kesepakatan status di kolom screen_analisis_ai.status\n",
    " * 1 --> baru diinput\n",
    " * 2 --> lagi dikerjakan\n",
    " * 3 --> proses berhasil\n",
    " * 4 --> proses gagal\n",
    " *\n",
    " \n",
    " * Kesepakatan jenis analisa AI\n",
    " * 1 --> Analisa Cluster\n",
    " * 2 --> Analisa image clustering\n",
    " * 3 --> Analisa sentiment\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fresh-video",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87259\n",
      "0.7\n",
      "2905\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# get available jobs from database server, first come first serve\n",
    "sql = \"select id, hastag, `parameter` \\\n",
    "from screen_analisis_ai \\\n",
    "where active = 1 \\\n",
    "and status = 1 \\\n",
    "and jenis_analisa = 1 \\\n",
    "order by created asc, id asc limit 1\"\n",
    "\n",
    "row_count = cursor.execute(sql)\n",
    "\n",
    "if(row_count == 0):\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "database_keyword_id = result[0]['hastag']\n",
    "similarity_treshold = result[0]['parameter']\n",
    "i_process_id = result[0]['id']\n",
    "screen_name = ''\n",
    "\n",
    "print(database_keyword_id)\n",
    "print(similarity_treshold)\n",
    "print(i_process_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-microwave",
   "metadata": {},
   "source": [
    "### Marking this process as running\n",
    "so that another process will not take this one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chief-resource",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2905\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Prepare SQL Statement\n",
    "print(i_process_id)\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = %s\"\n",
    "\n",
    "# execute\n",
    "cursor.execute(sql, i_process_id)\n",
    "\n",
    "# commit record\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-louisiana",
   "metadata": {},
   "source": [
    "### Runnning Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spectacular-manner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2905\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Header Record\n",
    "sql = \"insert into ret_analysis_header (job_id, datetime_start, user_id) values (%s, %s, %s)\"\n",
    "# Execute the query\n",
    "print(i_process_id)\n",
    "cursor.execute(sql, (str(i_process_id), datetime.now(), 1 ))\n",
    "\n",
    "#\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "cursor.execute(sql, (i_process_id, 1, 'Similarity Treshold', similarity_treshold))\n",
    "cursor.execute(sql, (i_process_id, 1, 'DB_ID', database_keyword_id))\n",
    "\n",
    "# Commit Record\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-lighting",
   "metadata": {},
   "source": [
    "#### Starting process, \n",
    "Run query against RDBMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18eb618-f28a-42c6-92d6-c721943afdbc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "def execute_mysql_query(query):\n",
    "    try:\n",
    "        # Define the connection parameters inside the function\n",
    "        host = \"202.157.185.40\"\n",
    "        port = 3306  # Replace with your MySQL port number\n",
    "        database = \"cekmedsos_database\"\n",
    "        user = \"cekmedsos_db\"\n",
    "        password = \"282E~f0si\"\n",
    "\n",
    "        # Create a SQLAlchemy engine using the provided connection parameters\n",
    "        connection_url = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{database}\"\n",
    "        engine = create_engine(connection_url)\n",
    "\n",
    "        # Establish a connection\n",
    "        connection = engine.connect()\n",
    "\n",
    "        # Execute the MySQL query and fetch the results into a DataFrame\n",
    "        result_df = pd.read_sql(query, connection)\n",
    "\n",
    "        # Close the database connection\n",
    "        connection.close()\n",
    "\n",
    "        print(\"Query executed successfully.\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weekly-jonathan",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select id, tweet, tweet_date_time from ret_tweet where db_id = \"87259\"\n",
      "Query executed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1135435194872188928</td>\n",
       "      <td>Just finished binge watching #GoodOmens on @Pr...</td>\n",
       "      <td>2019-06-03 13:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1670808660132638720</td>\n",
       "      <td>The only real-time generative AI app for video...</td>\n",
       "      <td>2023-06-19 22:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1677329120433422336</td>\n",
       "      <td>This indie game has won over 30 awards!\\n?????...</td>\n",
       "      <td>2023-07-07 21:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1702957556183007617</td>\n",
       "      <td>⛔️ Gantungan Kunci Unik ⛔️\\n\\nA thread https:/...</td>\n",
       "      <td>2023-09-16 15:08:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1710185613000048748</td>\n",
       "      <td>PKS,   terima kasih karena selalu menjadi suar...</td>\n",
       "      <td>2023-10-06 13:49:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1135435194872188928  Just finished binge watching #GoodOmens on @Pr...   \n",
       "1  1670808660132638720  The only real-time generative AI app for video...   \n",
       "2  1677329120433422336  This indie game has won over 30 awards!\\n?????...   \n",
       "3  1702957556183007617  ⛔️ Gantungan Kunci Unik ⛔️\\n\\nA thread https:/...   \n",
       "4  1710185613000048748  PKS,   terima kasih karena selalu menjadi suar...   \n",
       "\n",
       "      tweet_date_time  \n",
       "0 2019-06-03 13:37:00  \n",
       "1 2023-06-19 22:00:01  \n",
       "2 2023-07-07 21:50:00  \n",
       "3 2023-09-16 15:08:15  \n",
       "4 2023-10-06 13:49:58  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Query to get tweet data, apply analitics to this dataset\n",
    "#\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "s_query_string = 'select id, tweet, tweet_date_time from ret_tweet where '\n",
    "# s_query_string = 'select id, tweet, tweet_date_time from vw_ret_tweet_clean where '\n",
    "\n",
    "if (screen_name != ''):\n",
    "    # print('use screen name')\n",
    "    s_query_string = s_query_string + 'screen_name = \"' + screen_name + '\" and db_id = \"' + str(database_keyword_id) + '\"'\n",
    "else:\n",
    "    # print('no use')\n",
    "    s_query_string = s_query_string + 'db_id = \"' + database_keyword_id.replace('\"','') + '\"'\n",
    "    \n",
    "print(s_query_string)\n",
    "df = execute_mysql_query(s_query_string)\n",
    "\n",
    "# Close Connection\n",
    "connection.close()\n",
    "\n",
    "# see result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-waste",
   "metadata": {},
   "source": [
    "### 2. Try to pre-process all the text\n",
    "\n",
    "target-> tokenizing into another dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chemical-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1135435194872188928</td>\n",
       "      <td>just finished binge watching goodomens on prim...</td>\n",
       "      <td>2019-06-03 13:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1670808660132638720</td>\n",
       "      <td>the only realtime generative ai app for video ...</td>\n",
       "      <td>2023-06-19 22:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1677329120433422336</td>\n",
       "      <td>this indie game has won over awards free demo ...</td>\n",
       "      <td>2023-07-07 21:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1702957556183007617</td>\n",
       "      <td>⛔️ gantungan kunci unik ⛔️ a thread httpstcozn...</td>\n",
       "      <td>2023-09-16 15:08:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1710185613000048748</td>\n",
       "      <td>pks terima kasih karena selalu menjadi suara r...</td>\n",
       "      <td>2023-10-06 13:49:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1135435194872188928  just finished binge watching goodomens on prim...   \n",
       "1  1670808660132638720  the only realtime generative ai app for video ...   \n",
       "2  1677329120433422336  this indie game has won over awards free demo ...   \n",
       "3  1702957556183007617  ⛔️ gantungan kunci unik ⛔️ a thread httpstcozn...   \n",
       "4  1710185613000048748  pks terima kasih karena selalu menjadi suara r...   \n",
       "\n",
       "      tweet_date_time  \n",
       "0 2019-06-03 13:37:00  \n",
       "1 2023-06-19 22:00:01  \n",
       "2 2023-07-07 21:50:00  \n",
       "3 2023-09-16 15:08:15  \n",
       "4 2023-10-06 13:49:58  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    \n",
    "    return strOut\n",
    "# end Text Preprocessing\n",
    "\n",
    "\n",
    "# Apply to data frame\n",
    "df['tweet'] = df['tweet'].apply(text_preproc)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-power",
   "metadata": {},
   "source": [
    "### Finish preprocessing, Tokenized\n",
    "Next step, is to output to new column for tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "classical-position",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1135435194872188928</td>\n",
       "      <td>just finished binge watching goodomens on prim...</td>\n",
       "      <td>2019-06-03 13:37:00</td>\n",
       "      <td>[just, finished, binge, watching, goodomens, o...</td>\n",
       "      <td>{clever, life, performances, great, with, davi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1670808660132638720</td>\n",
       "      <td>the only realtime generative ai app for video ...</td>\n",
       "      <td>2023-06-19 22:00:01</td>\n",
       "      <td>[the, only, realtime, generative, ai, app, for...</td>\n",
       "      <td>{the, app, video, creation, ai, live, streamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1677329120433422336</td>\n",
       "      <td>this indie game has won over awards free demo ...</td>\n",
       "      <td>2023-07-07 21:50:00</td>\n",
       "      <td>[this, indie, game, has, won, over, awards, fr...</td>\n",
       "      <td>{won, has, over, indie, steam, on, game, award...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1702957556183007617</td>\n",
       "      <td>⛔️ gantungan kunci unik ⛔️ a thread httpstcozn...</td>\n",
       "      <td>2023-09-16 15:08:15</td>\n",
       "      <td>[⛔️, gantungan, kunci, unik, ⛔️, a, thread, ht...</td>\n",
       "      <td>{unik, httpstcoznpkvbbxii, gantungan, thread, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1710185613000048748</td>\n",
       "      <td>pks terima kasih karena selalu menjadi suara r...</td>\n",
       "      <td>2023-10-06 13:49:58</td>\n",
       "      <td>[pks, terima, kasih, karena, selalu, menjadi, ...</td>\n",
       "      <td>{suara, kasih, terdengar, letsgopks, pks, raky...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1135435194872188928  just finished binge watching goodomens on prim...   \n",
       "1  1670808660132638720  the only realtime generative ai app for video ...   \n",
       "2  1677329120433422336  this indie game has won over awards free demo ...   \n",
       "3  1702957556183007617  ⛔️ gantungan kunci unik ⛔️ a thread httpstcozn...   \n",
       "4  1710185613000048748  pks terima kasih karena selalu menjadi suara r...   \n",
       "\n",
       "      tweet_date_time                                    tokenized_tweet  \\\n",
       "0 2019-06-03 13:37:00  [just, finished, binge, watching, goodomens, o...   \n",
       "1 2023-06-19 22:00:01  [the, only, realtime, generative, ai, app, for...   \n",
       "2 2023-07-07 21:50:00  [this, indie, game, has, won, over, awards, fr...   \n",
       "3 2023-09-16 15:08:15  [⛔️, gantungan, kunci, unik, ⛔️, a, thread, ht...   \n",
       "4 2023-10-06 13:49:58  [pks, terima, kasih, karena, selalu, menjadi, ...   \n",
       "\n",
       "                                                  sw  \n",
       "0  {clever, life, performances, great, with, davi...  \n",
       "1  {the, app, video, creation, ai, live, streamin...  \n",
       "2  {won, has, over, indie, steam, on, game, award...  \n",
       "3  {unik, httpstcoznpkvbbxii, gantungan, thread, ...  \n",
       "4  {suara, kasih, terdengar, letsgopks, pks, raky...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Load stopwords\n",
    "sw = stopwords.words('indonesian')\n",
    "\n",
    "# apply tokenize\n",
    "df['tokenized_tweet'] = df.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
    "\n",
    "# apply stopword removal\n",
    "df['sw'] = df.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-commerce",
   "metadata": {},
   "source": [
    "### Define function to calculate similarity\n",
    "Function return similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "close-sessions",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(X_set, Y_set):\n",
    "# Program to measure the similarity between  \n",
    "# two sentences using cosine similarity. \n",
    "\n",
    "    l1 =[];l2 =[]\n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector\n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i]\n",
    "    try:\n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    except:\n",
    "        # print('zero div on X_set')\n",
    "        return 0;\n",
    "        \n",
    "    return cosine\n",
    "\n",
    "def largest_in_col(arr,nCol):\n",
    "    # \n",
    "    # Find largest value of col nCol on 2D arr\n",
    "    #\n",
    "    \n",
    "    # init value\n",
    "    max_val = arr[0][nCol]\n",
    "    # also, remember index\n",
    "    row_index = 0\n",
    "    \n",
    "    for x in range(0, len(arr)):\n",
    "        if arr[x][nCol] > max_val:\n",
    "            max_val = arr[x][nCol]\n",
    "            row_index = x\n",
    "        \n",
    "    return max_val,row_index\n",
    "\n",
    "def smallest_in_col(arr,nCol):\n",
    "    # \n",
    "    # Find smallest value of col nCol on 2D arr\n",
    "    #\n",
    "    \n",
    "    # init value\n",
    "    min_val = arr[0][nCol]\n",
    "    # also, remember index\n",
    "    row_index = 0\n",
    "    \n",
    "    for x in range(0, len(arr)):\n",
    "        if arr[x][nCol] < min_val:\n",
    "            min_val = arr[x][nCol]\n",
    "            row_index = x\n",
    "        \n",
    "    return min_val,row_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-appearance",
   "metadata": {},
   "source": [
    "### Try to using function\n",
    "using some of array cells, create n matrices\n",
    "\n",
    "1. take one tweet, compare to all data set\n",
    "2. flag 1 if similar\n",
    "3. take next tweet, if similar from prev tweet, skip\n",
    "4. if not similar, add counter, then proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unavailable-clearing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 603351/603351.0 [00:10<00:00, 57258.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import array as arr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set to max CPU Cores\n",
    "# multitasking.set_max_threads(multitasking.config[\"CPU_CORES\"] * 5)\n",
    "\n",
    "st = similarity_treshold\n",
    "cluster_no = 1\n",
    "s_score = 0\n",
    "s_score_current = 0\n",
    "i_current_cluster = 0\n",
    "\n",
    "#create zero element array\n",
    "#col 0 => index base tweet\n",
    "#col 1 => cluster number\n",
    "#col 2 => similarity score\n",
    "base_tweet = []\n",
    "\n",
    "#proceed to compare to all tweet\n",
    "cosine_matrix = np.zeros(( len(df), len(df) ), dtype=np.dtype('f4'))\n",
    "\n",
    "# flag the mt\n",
    "with tqdm(total=( (len(df)*len(df)/2))-(len(df)/2))  as pbar:\n",
    "    for j in range(0, len(df)):\n",
    "        tweet_to_compare = df['sw'][j]\n",
    "\n",
    "        #check, is this second tweet?\n",
    "        if(j == 0):\n",
    "            #first tweet, add as cluster no #1\n",
    "            base_tweet.append([j,1,1.0])\n",
    "            i_current_cluster = base_tweet[0][1]\n",
    "\n",
    "        elif(j == 1):\n",
    "            #compare to prev tweet\n",
    "            s_score = calculate_similarity(tweet_to_compare, df['sw'][ base_tweet[0][0] ])\n",
    "\n",
    "            if(s_score < st):\n",
    "                #not similar\n",
    "                base_tweet.append([j,2,1])\n",
    "                i_current_cluster = base_tweet[j][1]\n",
    "\n",
    "        else:\n",
    "            #other else tweet\n",
    "            for x in range(0,len(base_tweet)):\n",
    "                #compare every element\n",
    "                s_score = calculate_similarity(tweet_to_compare, df['sw'][ base_tweet[x][0] ])\n",
    "                base_tweet[x][2] = s_score\n",
    "\n",
    "            if(largest_in_col(base_tweet,2)[0] < st):\n",
    "                #no similar, add as one new cluster\n",
    "                i_current_cluster = i_current_cluster + 1\n",
    "                base_tweet.append([j,i_current_cluster,largest_in_col(base_tweet,2)[0]])\n",
    "            else:\n",
    "                #determine cluster# from biggest similarity\n",
    "                i_current_cluster = base_tweet[(largest_in_col(base_tweet,2)[1])][1]\n",
    "\n",
    "        #proceed to compare to all tweet\n",
    "        for i in range(0,len(df)):\n",
    "            # update progress\n",
    "            if (j<i):\n",
    "                pbar.update(1)\n",
    "                s_score = calculate_similarity(tweet_to_compare, df['sw'][i])\n",
    "                if (s_score >= st):\n",
    "                    cosine_matrix[i,j] = i_current_cluster\n",
    "\n",
    "\n",
    "pbar.close()\n",
    "print(cosine_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c3e4e-1355-489a-a639-920b9ee60a7e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "minor-transparency",
   "metadata": {},
   "source": [
    "### Writing result to file\n",
    "Using NPZ format for efficiency, and try to load them after save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "valid-mobile",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099\n",
      "Save data complete .... \n"
     ]
    }
   ],
   "source": [
    "# save numpy array as npz file\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "\n",
    "# save to npy file\n",
    "# savez_compressed('./output/df_380.npz',df)\n",
    "# savez_compressed('./output/data_380.npz', cosine_matrix)\n",
    "# savez_compressed('./output/data_380_base.npz', base_tweet)\n",
    "\n",
    "print(len(cosine_matrix))\n",
    "print('Save data complete .... ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-tennessee",
   "metadata": {},
   "source": [
    "### How to save to rdbms?\n",
    "save the header data, meta data of the process\n",
    "[ret_analysis_header]\n",
    "- JobID\n",
    "- user initiated\n",
    "- Time Started\n",
    "- Time End\n",
    "\n",
    "Process Parameter\n",
    "[ret_analysis_parameter]\n",
    "- JobID\n",
    "- Param Name\n",
    "- Param Value\n",
    "    - ST Value\n",
    "    - DB ID\n",
    "    - Screen Name\n",
    "    - How Many Tweet analyzed\n",
    "\n",
    "save detail cluster information\n",
    "[ret_base_tweet]\n",
    "- JobID\n",
    "- Tweet Base# --> tweet ID\n",
    "- Cluster#\n",
    "\n",
    "save detail calculation result, structure\n",
    "[ret_cluster_result]\n",
    "- JobID\n",
    "- TweetID\n",
    "- Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tribal-intent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished inserting base tweet record\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Base Tweet Record\n",
    "sql = \"insert into ret_base_tweet (job_id, tweet_id, cluster_id) values (%s, %s, %s)\"\n",
    "\n",
    "## inserting base taweet\n",
    "for i in range(0,len(base_tweet)):\n",
    "    # Execute the query\n",
    "    cursor.execute(sql, (i_process_id, df['id'][i], base_tweet[i][1]))\n",
    "    \n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('finished inserting base tweet record')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-kennedy",
   "metadata": {},
   "source": [
    "### Record cluster result\n",
    "into table ret_cluster_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mineral-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "\n",
    "savetxt('data.csv', cosine_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "painted-nylon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099\n",
      "2905\n",
      "finished inserting cluster data\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"insert into ret_cluster_result (job_id, tweet_id, cluster_no) values (%s, %s, %s)\"\n",
    "\n",
    "print(len(cosine_matrix))\n",
    "print(i_process_id)\n",
    "\n",
    "# initiate cluster number\n",
    "i_cluster_no_save = 0\n",
    "temp_val = 0\n",
    "                 \n",
    "## inserting tweet cluster\n",
    "for i in range(0, len(cosine_matrix)):\n",
    "    \n",
    "    # find value in this particular row\n",
    "    for j in range(0,len(cosine_matrix[i])):\n",
    "        \n",
    "        # print(cosine_matrix[i][j])\n",
    "        temp_val = cosine_matrix[j][i]\n",
    "        \n",
    "        if(temp_val != 0):\n",
    "            i_cluster_no_save = temp_val\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(sql, (i_process_id, df['id'][i], i_cluster_no_save))\n",
    "    \n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('finished inserting cluster data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-foundation",
   "metadata": {},
   "source": [
    "### Record finish time\n",
    "update table ret_analysis_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mighty-florence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2905\n",
      "job finished\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "cursor.execute(sql, (i_process_id, 1, '#Tweet Processed',len(df)))\n",
    "\n",
    "#\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"update ret_analysis_header set datetime_finish = %s where job_id = %s\"\n",
    "# Executing query\n",
    "cursor.execute(sql, (datetime.now(),i_process_id) )\n",
    "\n",
    "print(i_process_id)\n",
    "\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('job finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-brown",
   "metadata": {},
   "source": [
    "### Marking as finished the job\n",
    "on this particular i_process_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "august-interaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting result finished\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=_conn_host,\n",
    "                             user=_conn_user,\n",
    "                             password=_conn_password,\n",
    "                             database=_conn_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# inserting jobs into table mv result\n",
    "sql = \"call spInsertResultToMV(%s);\" \n",
    "# Executing query\n",
    "cursor.execute(sql,i_process_id)\n",
    "\n",
    "sql = \"update screen_analisis_ai set status = 3, end_process = now(), duration = TIMESTAMPDIFF(second,start_process, end_process) where id = %s\"\n",
    "# Executing query\n",
    "cursor.execute(sql,i_process_id)\n",
    "\n",
    "# commit changes\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('inserting result finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-tooth",
   "metadata": {},
   "source": [
    "### Done Here ...\n",
    "1. Create data structure to record below things\n",
    "    Job Header\n",
    "    - JobID\n",
    "    - Similarity Treshold\n",
    "    - Dataset Parameter\n",
    "        - By User\n",
    "        - By Keyword\n",
    "    - Running time start\n",
    "    - Running time end\n",
    "    \n",
    "    Dataset Result\n",
    "        - base_tweet\n",
    "        - cosine_matrix\n",
    "        \n",
    "2. Push the result data into RDBMS Server, in this case is mySQL\n",
    "    - upload csv file to mySQL\n",
    "    - import to database\n",
    "  \n",
    "Entry on crontab\n",
    "* * * * * /home/haviz/ai-project/tweet-grouping/run_one.sh >> /home/haviz/ai-project/tweet_gorup.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-technique",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wait 10 sec before release\n",
    "time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
