{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southeast-pittsburgh",
   "metadata": {},
   "source": [
    "## Sentiment Analysis - Model Building\n",
    "### Built specifically for cekmedsos.com\n",
    "\n",
    "Storage for tweet data is stored on cekmedsos.com, communicating through database layer\n",
    "\n",
    "### Training Data retrieved from:\n",
    "https://github.com/ridife/dataset-idsa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "helpful-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## need to install this !!!!\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# database connection properties\n",
    "db_host='202.157.176.225'\n",
    "db_user ='cekmedsos_db'\n",
    "db_password='kuku838485*#'\n",
    "db_database='cekmedsos_database'\n",
    "\n",
    "# save model to what filename?\n",
    "model_data_filename = 'train-data-01.h5'\n",
    "\n",
    "# model parameters\n",
    "num_of_epoch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parliamentary-vienna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#pigaihinasukujawa\n",
      "0.85\n",
      "693\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=db_host,\n",
    "                             user=db_user,\n",
    "                             password=db_password,\n",
    "                             database=db_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# get available jobs from database server, first come first serve\n",
    "# sql = \"select id, hastag, `parameter` from screen_analisis_ai where active = 1 and status = 1 and jenis_analisa = 1 order by created asc, id asc limit 1\"\n",
    "sql = \"select id, hastag, `parameter` \\\n",
    "from screen_analisis_ai \\\n",
    "where active = 1 \\\n",
    "and status = 1 \\\n",
    "and jenis_analisa = 2 \\\n",
    "order by created asc, id asc limit 1\"\n",
    "\n",
    "row_count = cursor.execute(sql)\n",
    "\n",
    "if(row_count == 0):\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "database_keyword_id = result[0]['hastag']\n",
    "similarity_treshold = result[0]['parameter']\n",
    "i_process_id = result[0]['id']\n",
    "screen_name = ''\n",
    "\n",
    "print(database_keyword_id)\n",
    "print(similarity_treshold)\n",
    "print(i_process_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-suffering",
   "metadata": {},
   "source": [
    "## Query tweet data from database\n",
    "using parameter retrieve from job list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suitable-stick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select id, tweet, tweet_date_time from vw_ret_tweet_clean where keyword = \"#pigaihinasukujawa\" limit 25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1354579947801759748</td>\n",
       "      <td>Jangan mancing di air keruh...   #PigaiHinaSuk...</td>\n",
       "      <td>2021-01-28 07:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1354609891395182592</td>\n",
       "      <td>Kritik itu baik untuk pemerintah.  Kritik itu ...</td>\n",
       "      <td>2021-01-28 08:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1354610143015673856</td>\n",
       "      <td>Natalius Pigai ngajak gelut cah...   #PigaiHin...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1354610143019888644</td>\n",
       "      <td>\" Natalius Pigai telah menghina suku Jawa dan ...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1354610143032451072</td>\n",
       "      <td>Apa yang kalian tangkap dari pernyataan Pigai ...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1354579947801759748  Jangan mancing di air keruh...   #PigaiHinaSuk...   \n",
       "1  1354609891395182592  Kritik itu baik untuk pemerintah.  Kritik itu ...   \n",
       "2  1354610143015673856  Natalius Pigai ngajak gelut cah...   #PigaiHin...   \n",
       "3  1354610143019888644  \" Natalius Pigai telah menghina suku Jawa dan ...   \n",
       "4  1354610143032451072  Apa yang kalian tangkap dari pernyataan Pigai ...   \n",
       "\n",
       "      tweet_date_time  \n",
       "0 2021-01-28 07:00:01  \n",
       "1 2021-01-28 08:59:00  \n",
       "2 2021-01-28 09:00:00  \n",
       "3 2021-01-28 09:00:00  \n",
       "4 2021-01-28 09:00:00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host='202.157.176.225',\n",
    "                             user='cekmedsos_db',\n",
    "                             password='kuku838485*#',\n",
    "                             database='cekmedsos_database',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# s_query_string = 'select id, tweet, tweet_date_time from ret_tweet where '\n",
    "s_query_string = 'select id, tweet, tweet_date_time from vw_ret_tweet_clean where '\n",
    "\n",
    "if (screen_name != ''):\n",
    "    # print('use screen name')\n",
    "    s_query_string = s_query_string + 'screen_name = \"' + screen_name + '\" and db_id = \"' + str(database_keyword_id) + '\"'\n",
    "else:\n",
    "    # print('no use')\n",
    "    s_query_string = s_query_string + 'keyword = \"' + database_keyword_id.replace('\"','') + '\" limit 25'\n",
    "    \n",
    "print(s_query_string)\n",
    "df = pd.read_sql(s_query_string, con=connection)\n",
    "\n",
    "# Close Connection\n",
    "connection.close()\n",
    "\n",
    "# see result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "physical-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function stemming\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def stemming(comment):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    do = []\n",
    "    for w in comment:\n",
    "        dt = stemmer.stem(w)\n",
    "        do.append(dt)\n",
    "    d_clean = []\n",
    "    d_clean = \" \".join(do)\n",
    "    return d_clean\n",
    "    \n",
    "# function case folding\n",
    "import re\n",
    "def casefolding(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = comment.strip(\" \")\n",
    "    comment = re.sub(r'[?|$|.|!_:\")(-+,]','',comment)\n",
    "    return comment\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    return strOut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-fisher",
   "metadata": {},
   "source": [
    "## Remote stop words, stemming, and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "contained-wrist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(casefolding)\n",
    "df['tweet'] = df['tweet'].apply(text_preproc)\n",
    "\n",
    "sw = stopwords.words('indonesian')\n",
    "\n",
    "#tokenized\n",
    "df['tokenized_tweet'] = df.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
    "\n",
    "# apply stopword removal\n",
    "df['tokenized_tweet'] = df.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "passive-defendant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_date_time</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1354579947801759748</td>\n",
       "      <td>jangan mancing di air keruh pigaihinasukujawa ...</td>\n",
       "      <td>2021-01-28 07:00:01</td>\n",
       "      <td>{httpstcogooflieq, pigaihinasukujawa, air, ker...</td>\n",
       "      <td>httpstcogooflieq pigaihinasukujawa air keruh m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1354609891395182592</td>\n",
       "      <td>kritik itu baik untuk pemerintah kritik itu me...</td>\n",
       "      <td>2021-01-28 08:59:00</td>\n",
       "      <td>{kesukuan, orang, menghina, kebijakan, program...</td>\n",
       "      <td>suku orang hina bijak program perintah kritik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1354610143015673856</td>\n",
       "      <td>natalius pigai ngajak gelut cah pigaihinasukuj...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{pigai, ☝️☝️☝️☝️, httpstcowyawjzgcxt, gelut, n...</td>\n",
       "      <td>pigai  httpstcowyawjzgcxt gelut ngajak pigaihi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1354610143019888644</td>\n",
       "      <td>natalius pigai telah menghina suku jawa dan in...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{menghina, kebencian, pigai, ras, jawa, antars...</td>\n",
       "      <td>hina benci pigai ras jawa antarsuku pigaihinas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1354610143032451072</td>\n",
       "      <td>apa yang kalian tangkap dari pernyataan pigai ...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{pigai, divideo, pernyataan, pigaihinasukujawa...</td>\n",
       "      <td>pigai video nyata pigaihinasukujawa httpstcobp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1354610143040741381</td>\n",
       "      <td>anjimm dah pigai ini otak taro mana pak pigaih...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{httpstcoydcrwwvra, pigai, berasa, ya, taro, h...</td>\n",
       "      <td>httpstcoydcrwwvra pigai asa ya taro hebat otak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1354610143040839680</td>\n",
       "      <td>maunya apa ya ni orang pigaihinasukujawa https...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{orang, ya, maunya, ni, httpstcokurnyqi, pigai...</td>\n",
       "      <td>orang ya mau ni httpstcokurnyqi pigaihinasukujawa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1354610143040860164</td>\n",
       "      <td>merendahkan suatu etnis tertentu itu dengan ra...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{menghina, orang, pigai, httpstcoupgsigykh, me...</td>\n",
       "      <td>hina orang pigai httpstcoupgsigykh aku rendah ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1354610143074283520</td>\n",
       "      <td>semenjak keluar dr komnas ham kok bg pigai ini...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{pigai, komnas, ya, bg, httpstcowpxzhqdk, prov...</td>\n",
       "      <td>pigai komnas ya bg httpstcowpxzhqdk provokator...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1354610143107969025</td>\n",
       "      <td>waaah pigaihinasukujawa padahal orang jawa itu...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{orang, murah, penyabar, httpstconzbzyyqavw, j...</td>\n",
       "      <td>orang murah sabar httpstconzbzyyqavw jawa waaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1354610143124549634</td>\n",
       "      <td>betul sekali pernyataan dari bang dennysiregar...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{orang, sopan, httpstcoofdfcruu, pemaafsabar, ...</td>\n",
       "      <td>orang sopan httpstcoofdfcruu pemaafsabar jawa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1354610143145680903</td>\n",
       "      <td>pigaihinasukujawa fotonya dibikin meme pigai m...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{menghina, suku, pigai, amp, provokasi, fotony...</td>\n",
       "      <td>hina suku pigai amp provokasi foto meme adu fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1354610143200219139</td>\n",
       "      <td>siapa bilang org jawa gak mau minta maaf bg jg...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{httpstcovfwgfcxsb, jgn, maaf, gak, jawa, org,...</td>\n",
       "      <td>httpstcovfwgfcxsb jgn maaf gak jawa org bg pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1354610143233679368</td>\n",
       "      <td>yg rasis siapa coba klo kek gini pigaihinasuku...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{coba, gini, klo, yg, kek, pigaihinasukujawa, ...</td>\n",
       "      <td>coba gin klo yg kek pigaihinasukujawa ras http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1354610143279734784</td>\n",
       "      <td>tni bereaksi pak pigai didesak minta maaf pada...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{institusi, pigai, masyarakat, maaf, tni, bere...</td>\n",
       "      <td>institusi pigai masyarakat maaf tni reaksi pig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1354610143292493824</td>\n",
       "      <td>pigaihinasukujawa kenapa begini httpstcoyvmmlelxe</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{pigaihinasukujawa, httpstcoyvmmlelxe}</td>\n",
       "      <td>pigaihinasukujawa httpstcoyvmmlelxe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1354610143321841668</td>\n",
       "      <td>pigaihinasukujawa sudah over nih httpstcofiqqjhr</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{over, nih, pigaihinasukujawa, httpstcofiqqjhr}</td>\n",
       "      <td>over nih pigaihinasukujawa httpstcofiqqjhr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1354610143346888704</td>\n",
       "      <td>maksud lo apa pigaimengatakan tirani mayoritas...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{mayoritas, emang, nyata, jawa, dipakai, mu, b...</td>\n",
       "      <td>mayoritas emang nyata jawa pakai mu ucap kontr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1354610143367815174</td>\n",
       "      <td>duh pigaihinasukujawa padahal orang jawa itu l...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{orang, sopan, lemah, santun, jawa, pemaaf, pi...</td>\n",
       "      <td>orang sopan lemah santun jawa maaf pigaihinasu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1354610143380393990</td>\n",
       "      <td>pigaihinasukujawa dia mencoba ciptakan propaga...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{mencoba, pigaihinasukujawa, httpstcogpikdqsn,...</td>\n",
       "      <td>coba pigaihinasukujawa httpstcogpikdqsn cipta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1354610143418339328</td>\n",
       "      <td>parah banget dah pigai kebisaan nya itu apa si...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{basi, pigai, sih, ya, aja, jualan, kebisaan, ...</td>\n",
       "      <td>basi pigai sih ya aja jual bisa banget parah c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1354610143443431424</td>\n",
       "      <td>pigaihinasukujawa kita harus waspada dengan pr...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{bangsa, waspada, sengaja, perpecahan, anak, y...</td>\n",
       "      <td>bangsa waspada sengaja pecah anak yg provokato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1354610143569317898</td>\n",
       "      <td>orang jawa itu penyabar penuh senyum dan pemaa...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{orang, pigai, httpstcogbgelrm, sumatera, sala...</td>\n",
       "      <td>orang pigai httpstcogbgelrm sumatera salam sab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1354610143573413889</td>\n",
       "      <td>pigaihinasukujawa udah rasis provokasi pula ht...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{httpstcotqmqonbk, provokasi, udah, pigaihinas...</td>\n",
       "      <td>httpstcotqmqonbk provokasi udah pigaihinasukuj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1354610143586082817</td>\n",
       "      <td>natalius pigai telah menghina suku jawa pigaih...</td>\n",
       "      <td>2021-01-28 09:00:00</td>\n",
       "      <td>{menghina, httpstcofloamgsi, pigai, ☝️☝️☝️, ja...</td>\n",
       "      <td>hina httpstcofloamgsi pigai  jawa naik pigaihi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              tweet  \\\n",
       "0   1354579947801759748  jangan mancing di air keruh pigaihinasukujawa ...   \n",
       "1   1354609891395182592  kritik itu baik untuk pemerintah kritik itu me...   \n",
       "2   1354610143015673856  natalius pigai ngajak gelut cah pigaihinasukuj...   \n",
       "3   1354610143019888644  natalius pigai telah menghina suku jawa dan in...   \n",
       "4   1354610143032451072  apa yang kalian tangkap dari pernyataan pigai ...   \n",
       "5   1354610143040741381  anjimm dah pigai ini otak taro mana pak pigaih...   \n",
       "6   1354610143040839680  maunya apa ya ni orang pigaihinasukujawa https...   \n",
       "7   1354610143040860164  merendahkan suatu etnis tertentu itu dengan ra...   \n",
       "8   1354610143074283520  semenjak keluar dr komnas ham kok bg pigai ini...   \n",
       "9   1354610143107969025  waaah pigaihinasukujawa padahal orang jawa itu...   \n",
       "10  1354610143124549634  betul sekali pernyataan dari bang dennysiregar...   \n",
       "11  1354610143145680903  pigaihinasukujawa fotonya dibikin meme pigai m...   \n",
       "12  1354610143200219139  siapa bilang org jawa gak mau minta maaf bg jg...   \n",
       "13  1354610143233679368  yg rasis siapa coba klo kek gini pigaihinasuku...   \n",
       "14  1354610143279734784  tni bereaksi pak pigai didesak minta maaf pada...   \n",
       "15  1354610143292493824  pigaihinasukujawa kenapa begini httpstcoyvmmlelxe   \n",
       "16  1354610143321841668   pigaihinasukujawa sudah over nih httpstcofiqqjhr   \n",
       "17  1354610143346888704  maksud lo apa pigaimengatakan tirani mayoritas...   \n",
       "18  1354610143367815174  duh pigaihinasukujawa padahal orang jawa itu l...   \n",
       "19  1354610143380393990  pigaihinasukujawa dia mencoba ciptakan propaga...   \n",
       "20  1354610143418339328  parah banget dah pigai kebisaan nya itu apa si...   \n",
       "21  1354610143443431424  pigaihinasukujawa kita harus waspada dengan pr...   \n",
       "22  1354610143569317898  orang jawa itu penyabar penuh senyum dan pemaa...   \n",
       "23  1354610143573413889  pigaihinasukujawa udah rasis provokasi pula ht...   \n",
       "24  1354610143586082817  natalius pigai telah menghina suku jawa pigaih...   \n",
       "\n",
       "       tweet_date_time                                    tokenized_tweet  \\\n",
       "0  2021-01-28 07:00:01  {httpstcogooflieq, pigaihinasukujawa, air, ker...   \n",
       "1  2021-01-28 08:59:00  {kesukuan, orang, menghina, kebijakan, program...   \n",
       "2  2021-01-28 09:00:00  {pigai, ☝️☝️☝️☝️, httpstcowyawjzgcxt, gelut, n...   \n",
       "3  2021-01-28 09:00:00  {menghina, kebencian, pigai, ras, jawa, antars...   \n",
       "4  2021-01-28 09:00:00  {pigai, divideo, pernyataan, pigaihinasukujawa...   \n",
       "5  2021-01-28 09:00:00  {httpstcoydcrwwvra, pigai, berasa, ya, taro, h...   \n",
       "6  2021-01-28 09:00:00  {orang, ya, maunya, ni, httpstcokurnyqi, pigai...   \n",
       "7  2021-01-28 09:00:00  {menghina, orang, pigai, httpstcoupgsigykh, me...   \n",
       "8  2021-01-28 09:00:00  {pigai, komnas, ya, bg, httpstcowpxzhqdk, prov...   \n",
       "9  2021-01-28 09:00:00  {orang, murah, penyabar, httpstconzbzyyqavw, j...   \n",
       "10 2021-01-28 09:00:00  {orang, sopan, httpstcoofdfcruu, pemaafsabar, ...   \n",
       "11 2021-01-28 09:00:00  {menghina, suku, pigai, amp, provokasi, fotony...   \n",
       "12 2021-01-28 09:00:00  {httpstcovfwgfcxsb, jgn, maaf, gak, jawa, org,...   \n",
       "13 2021-01-28 09:00:00  {coba, gini, klo, yg, kek, pigaihinasukujawa, ...   \n",
       "14 2021-01-28 09:00:00  {institusi, pigai, masyarakat, maaf, tni, bere...   \n",
       "15 2021-01-28 09:00:00             {pigaihinasukujawa, httpstcoyvmmlelxe}   \n",
       "16 2021-01-28 09:00:00    {over, nih, pigaihinasukujawa, httpstcofiqqjhr}   \n",
       "17 2021-01-28 09:00:00  {mayoritas, emang, nyata, jawa, dipakai, mu, b...   \n",
       "18 2021-01-28 09:00:00  {orang, sopan, lemah, santun, jawa, pemaaf, pi...   \n",
       "19 2021-01-28 09:00:00  {mencoba, pigaihinasukujawa, httpstcogpikdqsn,...   \n",
       "20 2021-01-28 09:00:00  {basi, pigai, sih, ya, aja, jualan, kebisaan, ...   \n",
       "21 2021-01-28 09:00:00  {bangsa, waspada, sengaja, perpecahan, anak, y...   \n",
       "22 2021-01-28 09:00:00  {orang, pigai, httpstcogbgelrm, sumatera, sala...   \n",
       "23 2021-01-28 09:00:00  {httpstcotqmqonbk, provokasi, udah, pigaihinas...   \n",
       "24 2021-01-28 09:00:00  {menghina, httpstcofloamgsi, pigai, ☝️☝️☝️, ja...   \n",
       "\n",
       "                                              stemmed  \n",
       "0   httpstcogooflieq pigaihinasukujawa air keruh m...  \n",
       "1   suku orang hina bijak program perintah kritik ...  \n",
       "2   pigai  httpstcowyawjzgcxt gelut ngajak pigaihi...  \n",
       "3   hina benci pigai ras jawa antarsuku pigaihinas...  \n",
       "4   pigai video nyata pigaihinasukujawa httpstcobp...  \n",
       "5   httpstcoydcrwwvra pigai asa ya taro hebat otak...  \n",
       "6   orang ya mau ni httpstcokurnyqi pigaihinasukujawa  \n",
       "7   hina orang pigai httpstcoupgsigykh aku rendah ...  \n",
       "8   pigai komnas ya bg httpstcowpxzhqdk provokator...  \n",
       "9   orang murah sabar httpstconzbzyyqavw jawa waaa...  \n",
       "10  orang sopan httpstcoofdfcruu pemaafsabar jawa ...  \n",
       "11  hina suku pigai amp provokasi foto meme adu fi...  \n",
       "12  httpstcovfwgfcxsb jgn maaf gak jawa org bg pro...  \n",
       "13  coba gin klo yg kek pigaihinasukujawa ras http...  \n",
       "14  institusi pigai masyarakat maaf tni reaksi pig...  \n",
       "15                pigaihinasukujawa httpstcoyvmmlelxe  \n",
       "16         over nih pigaihinasukujawa httpstcofiqqjhr  \n",
       "17  mayoritas emang nyata jawa pakai mu ucap kontr...  \n",
       "18  orang sopan lemah santun jawa maaf pigaihinasu...  \n",
       "19  coba pigaihinasukujawa httpstcogpikdqsn cipta ...  \n",
       "20  basi pigai sih ya aja jual bisa banget parah c...  \n",
       "21  bangsa waspada sengaja pecah anak yg provokato...  \n",
       "22  orang pigai httpstcogbgelrm sumatera salam sab...  \n",
       "23  httpstcotqmqonbk provokasi udah pigaihinasukuj...  \n",
       "24  hina httpstcofloamgsi pigai  jawa naik pigaihi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform stemming on 'sw'\n",
    "df['stemmed'] = df['tokenized_tweet'].apply(stemming)\n",
    "\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-lambda",
   "metadata": {},
   "source": [
    "## Learning dataset\n",
    "X --> Variabel yang jadi input\n",
    "Y --> Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "genuine-sentence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimen</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>lagu bosan apa yang aku save ni huhuhuhuhuhuhu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>kita lanjutkan saja diam ini hingga kau dan ak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>doa rezeki tak putus inna haa zaa larizquna ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>makasih loh ntar kita bagi hasil aku 99 9 sisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>aku tak faham betul jenis orang malaysia yang ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentimen                                              Tweet\n",
       "0         2  lagu bosan apa yang aku save ni huhuhuhuhuhuhu...\n",
       "1         2  kita lanjutkan saja diam ini hingga kau dan ak...\n",
       "2         1  doa rezeki tak putus inna haa zaa larizquna ma...\n",
       "3         1  makasih loh ntar kita bagi hasil aku 99 9 sisa...\n",
       "4         2  aku tak faham betul jenis orang malaysia yang ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv('./train-data/train-data-labeled.csv',sep=\"\\t\",)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "imperial-basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10806\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    5327\n",
       "2    2887\n",
       "1    2592\n",
       "Name: sentimen, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array of text, tweet content\n",
    "text = df_train['Tweet'].tolist()\n",
    "print(len(text))\n",
    "\n",
    "# memisahkan X dan Y\n",
    "# 1 >> positif\n",
    "# 2 >> negatif\n",
    "# 0 >> netral\n",
    "y = df_train[\"sentimen\"]\n",
    "y = to_categorical(y)\n",
    "print(y)\n",
    "\n",
    "df_train['sentimen'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "taken-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(text)\n",
    "\n",
    "# token.index_word\n",
    "vocab = len(token.index_word) + 1\n",
    "\n",
    "#encoding texts\n",
    "encode_text = token.texts_to_sequences(text)\n",
    "# print(encode_text)\n",
    "\n",
    "max_kata = 100\n",
    "X = pad_sequences(encode_text, maxlen=max_kata, padding=\"post\")\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ancient-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=40, test_size=0.3, stratify=y)\n",
    "\n",
    "# Convert to array of numpy\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-static",
   "metadata": {},
   "source": [
    "### Building models\n",
    "Learn about CNN -> convolutional neural network\n",
    "    filters\n",
    "    overtippin\n",
    "    \n",
    "Build model, calculate epoch, and saves model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "enabling-maple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          6813000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 93, 64)            153664    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 46, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 46, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46, 32)            2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 46, 32)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46, 16)            528       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 6,969,323\n",
      "Trainable params: 6,969,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model definitions\n",
    "vec_size = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab, vec_size, input_length=max_kata))\n",
    "model.add(Conv1D(64, 8, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "attempted-charter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "237/237 [==============================] - 28s 114ms/step - loss: 1.0547 - accuracy: 0.4794 - val_loss: 1.0521 - val_accuracy: 0.4907\n",
      "Epoch 2/16\n",
      "237/237 [==============================] - 30s 126ms/step - loss: 1.0039 - accuracy: 0.5264 - val_loss: 0.9897 - val_accuracy: 0.5494\n",
      "Epoch 3/16\n",
      "237/237 [==============================] - 29s 123ms/step - loss: 0.8238 - accuracy: 0.6507 - val_loss: 0.9306 - val_accuracy: 0.5783\n",
      "Epoch 4/16\n",
      "237/237 [==============================] - 29s 123ms/step - loss: 0.5502 - accuracy: 0.7969 - val_loss: 0.9365 - val_accuracy: 0.5848\n",
      "Epoch 5/16\n",
      "237/237 [==============================] - 33s 140ms/step - loss: 0.3527 - accuracy: 0.8761 - val_loss: 1.0521 - val_accuracy: 0.5697\n",
      "Epoch 6/16\n",
      "237/237 [==============================] - 27s 111ms/step - loss: 0.2332 - accuracy: 0.9264 - val_loss: 1.0730 - val_accuracy: 0.5895\n",
      "Epoch 7/16\n",
      "237/237 [==============================] - 27s 116ms/step - loss: 0.1835 - accuracy: 0.9470 - val_loss: 1.1409 - val_accuracy: 0.5774\n",
      "Epoch 8/16\n",
      "237/237 [==============================] - 28s 119ms/step - loss: 0.1477 - accuracy: 0.9569 - val_loss: 1.2206 - val_accuracy: 0.5820\n",
      "Epoch 9/16\n",
      "237/237 [==============================] - 29s 120ms/step - loss: 0.1441 - accuracy: 0.9601 - val_loss: 1.1867 - val_accuracy: 0.5756\n",
      "Epoch 10/16\n",
      "237/237 [==============================] - 30s 125ms/step - loss: 0.1239 - accuracy: 0.9684 - val_loss: 1.2310 - val_accuracy: 0.5691\n",
      "Epoch 11/16\n",
      "237/237 [==============================] - 30s 127ms/step - loss: 0.1039 - accuracy: 0.9699 - val_loss: 1.2485 - val_accuracy: 0.5805\n",
      "Epoch 12/16\n",
      "237/237 [==============================] - 28s 119ms/step - loss: 0.1021 - accuracy: 0.9709 - val_loss: 1.2757 - val_accuracy: 0.5731\n",
      "Epoch 13/16\n",
      "237/237 [==============================] - 25s 107ms/step - loss: 0.0902 - accuracy: 0.9733 - val_loss: 1.3255 - val_accuracy: 0.5756\n",
      "Epoch 14/16\n",
      "237/237 [==============================] - 24s 103ms/step - loss: 0.0879 - accuracy: 0.9734 - val_loss: 1.3298 - val_accuracy: 0.5706\n",
      "Epoch 15/16\n",
      "237/237 [==============================] - 25s 107ms/step - loss: 0.0794 - accuracy: 0.9757 - val_loss: 1.4114 - val_accuracy: 0.5737\n",
      "Epoch 16/16\n",
      "237/237 [==============================] - 26s 109ms/step - loss: 0.0816 - accuracy: 0.9758 - val_loss: 1.4102 - val_accuracy: 0.5614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1486d1430>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=num_of_epoch, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scientific-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "simplified-muslim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          6813000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 93, 64)            153664    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 46, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 46, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46, 32)            2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 46, 32)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46, 16)            528       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 6,969,323\n",
      "Trainable params: 6,969,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# loading and testing models\n",
    "new_model = tf.keras.models.load_model(model_data_filename)\n",
    "\n",
    "# display summary\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "partial-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode(x):\n",
    "    x = token.texts_to_sequences(x)\n",
    "    x = pad_sequences(x, maxlen = max_kata, padding='post')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "surprised-motorcycle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadhaviz/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['saya sulit sukar senang alay yes']\n",
    "x = get_encode(x)\n",
    "\n",
    "new_model.predict_classes(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-thong",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
