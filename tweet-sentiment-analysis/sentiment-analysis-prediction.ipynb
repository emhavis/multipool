{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classified-rough",
   "metadata": {},
   "source": [
    "## Sentiment Analysis - Prediction Execution\n",
    "Based on model build on another code,\n",
    "build specifically for cekmedsos.com, communicating through database layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d622a-0876-4d62-b426-9b9401bee15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations\n",
    "!pip install pymysql\n",
    "!pip install pandas\n",
    "!pip install uuid\n",
    "!pip install time\n",
    "!pip install os\n",
    "!pip install string\n",
    "!pip install re\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "civilian-receipt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading library\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## need to install this !!!!\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow import keras\n",
    "\n",
    "# database connection properties\n",
    "db_host='202.157.185.40'\n",
    "db_user ='cekmedsos_db'\n",
    "db_password='282E~f0si'\n",
    "db_database='cekmedsos_database'\n",
    "\n",
    "# save model to what filename?\n",
    "model_data_filename = 'train-data-01.h5'\n",
    "\n",
    "# training data file\n",
    "train_data_file = './train-data/train-data-labeled.csv'\n",
    "\n",
    "# model parameters\n",
    "num_of_epoch = 128\n",
    "max_kata = 100\n",
    "\n",
    "print('Finished loading library')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-ensemble",
   "metadata": {},
   "source": [
    "### Retrieving data jobs from database\n",
    "For jenis_analisa = 2, queuing based on FIFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "detailed-sterling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero jobs, quitting now\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     quit()\n\u001b[1;32m     27\u001b[0m result \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[0;32m---> 28\u001b[0m database_keyword_id \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhastag\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m similarity_treshold \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m i_process_id \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host=db_host,\n",
    "                             user=db_user,\n",
    "                             password=db_password,\n",
    "                             database=db_database,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# get available jobs from database server, first come first serve\n",
    "# sql = \"select id, hastag, `parameter` from screen_analisis_ai where active = 1 and status = 1 and jenis_analisa = 1 order by created asc, id asc limit 1\"\n",
    "sql = \"select id, hastag, `parameter` \\\n",
    "from screen_analisis_ai \\\n",
    "where active = 1 \\\n",
    "and status = 1 \\\n",
    "and jenis_analisa = 3 \\\n",
    "order by created asc, id asc limit 1\"\n",
    "\n",
    "row_count = cursor.execute(sql)\n",
    "\n",
    "if(row_count == 0):\n",
    "    # get out, nothing to do\n",
    "    print('Zero jobs, quitting now')\n",
    "    quit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "database_keyword_id = result[0]['hastag']\n",
    "similarity_treshold = result[0]['parameter']\n",
    "i_process_id = result[0]['id']\n",
    "screen_name = ''\n",
    "\n",
    "print(database_keyword_id)\n",
    "print(similarity_treshold)\n",
    "print(i_process_id)\n",
    "print('Finished querying jobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-pitch",
   "metadata": {},
   "source": [
    "### Marking this jobs as run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host='202.157.176.225',\n",
    "                             user='cekmedsos_db',\n",
    "                             password='kuku838485*#',\n",
    "                             database='cekmedsos_database',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Prepare SQL Statement\n",
    "print(i_process_id)\n",
    "sql = \"update screen_analisis_ai set status = 2, last_status_update = now(), start_process = now() where id = %s\"\n",
    "\n",
    "# execute\n",
    "cursor.execute(sql, i_process_id)\n",
    "\n",
    "# commit record\n",
    "connection.commit()\n",
    "\n",
    "print('Job mark as run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-observer",
   "metadata": {},
   "source": [
    "### Inserting Job Header Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host='202.157.176.225',\n",
    "                             user='cekmedsos_db',\n",
    "                             password='kuku838485*#',\n",
    "                             database='cekmedsos_database',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Header Record\n",
    "sql = \"insert into ret_analysis_header (job_id, datetime_start, user_id) values (%s, %s, %s)\"\n",
    "# Execute the query\n",
    "print('ProcessID: ' + str( i_process_id))\n",
    "cursor.execute(sql, (str(i_process_id), datetime.now(), 1 ))\n",
    "\n",
    "#\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "cursor.execute(sql, (i_process_id, 1, 'Model ID', model_data_filename))\n",
    "cursor.execute(sql, (i_process_id, 2, 'DB_ID', database_keyword_id))\n",
    "\n",
    "# Commit Record\n",
    "connection.commit()\n",
    "\n",
    "print('Job Header Data Inserted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-spare",
   "metadata": {},
   "source": [
    "### Querying tweet data from database based on jobs id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Query to get tweet data, apply analitics to this dataset\n",
    "#\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(host='202.157.176.225',\n",
    "                             user='cekmedsos_db',\n",
    "                             password='kuku838485*#',\n",
    "                             database='cekmedsos_database',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "s_query_string = 'select id, tweet, tweet_date_time from ret_tweet where '\n",
    "# s_query_string = 'select id, tweet, tweet_date_time from vw_ret_tweet_clean where '\n",
    "\n",
    "if (screen_name != ''):\n",
    "    # print('use screen name')\n",
    "    s_query_string = s_query_string + 'screen_name = \"' + screen_name + '\" and db_id = \"' + str(database_keyword_id) + '\"'\n",
    "else:\n",
    "    # print('no use')\n",
    "    s_query_string = s_query_string + 'db_id = \"' + database_keyword_id.replace('\"','') + '\"'\n",
    "    \n",
    "print(s_query_string)\n",
    "df = pd.read_sql(s_query_string, con=connection)\n",
    "\n",
    "# Close Connection\n",
    "connection.close()\n",
    "\n",
    "# see result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-strain",
   "metadata": {},
   "source": [
    "### Text pre-processing functions\n",
    "1. Case Folding\n",
    "2. Stemming\n",
    "3. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function stemming\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def stemming(comment):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    do = []\n",
    "    for w in comment:\n",
    "        dt = stemmer.stem(w)\n",
    "        do.append(dt)\n",
    "    d_clean = []\n",
    "    d_clean = \" \".join(do)\n",
    "    return d_clean\n",
    "    \n",
    "# function case folding\n",
    "import re\n",
    "def casefolding(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = comment.strip(\" \")\n",
    "    comment = re.sub(r'[?|$|.|!_:\")(-+,]','',comment)\n",
    "    return comment\n",
    "\n",
    "def clean_up_tag(comment):\n",
    "    x_ret = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",comment).split())\n",
    "    return x_ret\n",
    "\n",
    "# Text Preprocessing, \n",
    "def text_preproc(strIn):\n",
    "    # case folding\n",
    "    strOut = strIn.lower()\n",
    "    \n",
    "    # remove numbers\n",
    "    strOut = re.sub(r\"\\d+\", \"\", strOut)\n",
    "    \n",
    "    # remote punctuation\n",
    "    strOut = strOut.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    \n",
    "    # remove whitspace\n",
    "    strOut = strOut.strip()\n",
    "    \n",
    "    # \n",
    "    strOut = re.sub('\\s+',' ',strOut)\n",
    "    return strOut\n",
    "\n",
    "print('functions is build')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-sociology",
   "metadata": {},
   "source": [
    "### Prepared data from database to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform stemming on 'sw'\n",
    "# df['stemmed'] = df['tokenized_tweet'].apply(stemming)\n",
    "# x = stemming(['Sistem Merit Didefinisikan Sebagai Kebijakan dan Manajemen ASN Yang Berdasarkan Pada Kualifikasi, Kompetensi dan Kinerja, Yang Diberlakukan Secara Adil dan Wajar Dengan Tanpa Diskriminasi.  #PerkuatPemulihanEkonomi  https://t.co/8al7sOKFZU'])\n",
    "from tqdm import tqdm\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "# remove first\n",
    "df['tweet'] = df['tweet'].apply(clean_up_tag)\n",
    "\n",
    "# skip stemming\n",
    "df['stemmed'] = df['tweet']\n",
    "# text = [0]\n",
    "# with tqdm(total=len(df)) as pbar:\n",
    "#     for i in range(0, len(df)):\n",
    "#         text[0] = df['tweet'][i]\n",
    "#         x = stemming(text)\n",
    "#         df['stemmed'][i] = x\n",
    "        \n",
    "#         pbar.update(1)\n",
    "\n",
    "# pbar.close()\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-accounting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['stemmed'] = df['stemmed'].apply(casefolding)\n",
    "df['stemmed'] = df['stemmed'].apply(text_preproc)\n",
    "\n",
    "sw = stopwords.words('indonesian')\n",
    "\n",
    "#tokenized\n",
    "df['tokenized_tweet'] = df.apply(lambda row: nltk.word_tokenize(row['stemmed']), axis=1)\n",
    "\n",
    "# apply stopword removal\n",
    "df['tokenized_tweet'] = df.apply(lambda row: {w for w in row['tokenized_tweet'] if not w in sw}, axis=1)\n",
    "\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-syntax",
   "metadata": {},
   "source": [
    "### Load model and prepare encoding calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv(train_data_file,sep=\"\\t\",)\n",
    "df_train.head()\n",
    "\n",
    "# array of text, tweet content\n",
    "text = df_train['Tweet'].tolist()\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(text)\n",
    "\n",
    "def get_encode(x):\n",
    "    x = token.texts_to_sequences(x)\n",
    "    x = pad_sequences(x, maxlen = max_kata, padding='post')\n",
    "    return x\n",
    "\n",
    "# loading and testing models\n",
    "new_model = tf.keras.models.load_model(model_data_filename)\n",
    "\n",
    "# display summary\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# prepare new column to hold prediction value\n",
    "df['sa'] = ''\n",
    "\n",
    "#for i in range(0, len(df)):\n",
    "\n",
    "x = ['']\n",
    "for i in range(0, len(df)):\n",
    "# for i in range(0, 10):\n",
    "    x[0] = ' '.join(df['tokenized_tweet'][i])\n",
    "    y = get_encode(x)\n",
    "    sa = np.argmax(new_model.predict(y), axis=-1)\n",
    "    \n",
    "    # convert negative values\n",
    "    if(sa[0] == 2):\n",
    "        sa[0] = -1\n",
    "        \n",
    "    df['sa'][i] = sa[0]\n",
    "\n",
    "# print('Finished predicting')\n",
    "df.head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-bowling",
   "metadata": {},
   "source": [
    "### Writeback to database\n",
    "Database structure:\n",
    "- job_id\n",
    "- sentiment_value\n",
    "- tweet_id\n",
    "- \n",
    "\n",
    "Notes on return sentiment values\n",
    "- 1 >> positif >> stay\n",
    "- 2 >> negatif >> convert to -1\n",
    "- 0 >> netral >> stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host='202.157.176.225',\n",
    "                             user='cekmedsos_db',\n",
    "                             password='kuku838485*#',\n",
    "                             database='cekmedsos_database',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"insert into ret_sentiment_result (job_id, tweet_id, sentiment_class) values (%s, %s, %s)\"\n",
    "                 \n",
    "# inserting tweet cluster\n",
    "for i in range(0, len(df)):\n",
    "    \n",
    "    # Execute the query\n",
    "    cursor.execute(sql, (i_process_id, df['id'][i], df['sa'][i]))\n",
    "    \n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('finished inserting sentiment data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-giving",
   "metadata": {},
   "source": [
    "### Record Processing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host='202.157.176.225',\n",
    "                             user='cekmedsos_db',\n",
    "                             password='kuku838485*#',\n",
    "                             database='cekmedsos_database',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#\n",
    "# Create Parameter Record\n",
    "sql = \"insert into ret_analysis_parameter (job_id, param_id, param_name, param_value) values (%s, %s, %s, %s)\"\n",
    "# Execute the query\n",
    "cursor.execute(sql, (i_process_id, 3, '#Tweet Processed',len(df)))\n",
    "\n",
    "#\n",
    "# Create Tweet Cluster Record\n",
    "sql = \"update ret_analysis_header set datetime_finish = %s where job_id = %s\"\n",
    "# Executing query\n",
    "cursor.execute(sql, (datetime.now(),i_process_id) )\n",
    "\n",
    "print(i_process_id)\n",
    "\n",
    "sql = \"update screen_analisis_ai set status = 3, end_process = now(), duration = TIMESTAMPDIFF(second,start_process, end_process) where id = %s\"\n",
    "# Executing query\n",
    "cursor.execute(sql,i_process_id)\n",
    "\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print('job finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
